{"cells":[{"cell_type":"markdown","metadata":{"id":"t6aqk80hy_1x"},"source":["# Tower Demand Curve for One Renewal Period\n","\n","## Overview\n","Find the optimal insurance tower structure — deductible (retention) and maximum limit — across\n","a range of loss ratios to formulate a demand curve for insurance.  The optimizer maximizes Sharpe ratio, subject to a hard constraint of no more than 0.25% probability of ruin over one renewal year.\n","\n","The tower is built from fixed layer breakpoints but the optimizer can \"squish\" it from both ends by choosing where to start (deductible) and where to stop (max limit).\n","\n","- **Prerequisites**: [optimization/01_optimization_overview](01_optimization_overview.ipynb)\n","- **Audience**: [Practitioner] / [Developer]"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2230,"status":"ok","timestamp":1771702309600,"user":{"displayName":"Alex Filiakov","userId":"03712643796933532488"},"user_tz":300},"id":"n1opuX5vy_12","outputId":"6a65429d-d881-420d-99c8-de3fa5489241"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","\n","Setup complete. If you see numpy/scipy import errors below,\n","restart the runtime (Runtime > Restart runtime) and re-run all cells.\n"]}],"source":["\"\"\"Google Colab setup: mount Drive and install package dependencies.\n","\n","Run this cell first. If prompted to restart the runtime, do so, then re-run all cells.\n","This cell is a no-op when running locally.\n","\"\"\"\n","import sys, os\n","if 'google.colab' in sys.modules:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","\n","    NOTEBOOK_DIR = '/content/drive/My Drive/Colab Notebooks/ei_notebooks/optimization'\n","\n","    os.chdir(NOTEBOOK_DIR)\n","    if NOTEBOOK_DIR not in sys.path:\n","        sys.path.append(NOTEBOOK_DIR)\n","\n","    !pip install ergodic-insurance -q 2>&1 | tail -3\n","    print('\\nSetup complete. If you see numpy/scipy import errors below,')\n","    print('restart the runtime (Runtime > Restart runtime) and re-run all cells.')"]},{"cell_type":"markdown","metadata":{"id":"geeprRCGy_14"},"source":["## Setup"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":713,"status":"ok","timestamp":1771702310325,"user":{"displayName":"Alex Filiakov","userId":"03712643796933532488"},"user_tz":300},"id":"wqNTpoVZy_15","outputId":"d04f6fd2-8709-42fe-823c-435fede79969"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of CPU cores: 44\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import plotly.graph_objects as go\n","from plotly.subplots import make_subplots\n","import warnings\n","import multiprocessing\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","from ergodic_insurance.pareto_frontier import (\n","    Objective, ObjectiveType, ParetoFrontier, ParetoPoint,\n",")\n","from ergodic_insurance.config import ManufacturerConfig\n","from ergodic_insurance.manufacturer import WidgetManufacturer\n","from ergodic_insurance.insurance_program import (\n","    EnhancedInsuranceLayer, InsuranceProgram,\n",")\n","from ergodic_insurance.loss_distributions import ManufacturingLossGenerator\n","\n","plt.style.use(\"seaborn-v0_8-darkgrid\")\n","SEED = 42\n","np.random.seed(SEED)\n","N_CORES = multiprocessing.cpu_count()\n","print(f\"Number of CPU cores: {N_CORES}\")  # Available parallel cores for sensitivity sweep\n","CI = False      # Set True to skip heavy computations"]},{"cell_type":"markdown","metadata":{"id":"fhE0J3eM2MIL"},"source":["## Part I: Parameter Setup"]},{"cell_type":"markdown","metadata":{"id":"52krAvZH0IP2"},"source":["### Manufacturing Company Configuration\n","\n","Baseline company parameters (the experiment varies revenue via `revenue_grid`)."]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1771702310353,"user":{"displayName":"Alex Filiakov","userId":"03712643796933532488"},"user_tz":300},"id":"JVqpuH1e0amx","outputId":"1b5c2c0d-1d10-4970-e6e2-412ceac0e191"},"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","MANUFACTURING COMPANY PROFILE\n","============================================================\n","Total Assets:          $     5,000,000\n","Annual Revenue:        $    10,000,000\n","Operating Income:      $     1,500,000\n","Operating Margin:               15.0%\n","Asset Turnover:                   2.0x\n","Revenue Volatility:               0.5\n","Tax Rate:                      25.0%\n","Retention Ratio:               70.0%\n","============================================================\n"]}],"source":["# --- Economic Parameters\n","ATR = 2.0                # Asset turnover ratio\n","OPERATING_MARGIN = 0.15  # 12% EBIT margin before Insurable Losses\n","REV_VOL = 0.50           # Revenue volatility (annualized)\n","INITIAL_ASSETS = 5_000_000\n","\n","# --- Company Configuration ---\n","mfg_config = ManufacturerConfig(\n","    initial_assets=INITIAL_ASSETS,          # $15M total assets\n","    asset_turnover_ratio=ATR,               # Revenue = Assets Ãƒâ€” turnover = $22.5M\n","    base_operating_margin=OPERATING_MARGIN, # 12% EBIT margin -> $2.7M/yr operating income\n","    tax_rate=0.25,                          # 25% corporate tax\n","    retention_ratio=0.70,                   # 70% earnings retained for growth\n",")\n","\n","# Display company profile\n","revenue = mfg_config.initial_assets * mfg_config.asset_turnover_ratio\n","ebit = revenue * mfg_config.base_operating_margin\n","print(\"=\" * 60)\n","print(\"MANUFACTURING COMPANY PROFILE\")\n","print(\"=\" * 60)\n","print(f\"Total Assets:          ${mfg_config.initial_assets:>14,.0f}\")\n","print(f\"Annual Revenue:        ${revenue:>14,.0f}\")\n","print(f\"Operating Income:      ${ebit:>14,.0f}\")\n","print(f\"Operating Margin:      {mfg_config.base_operating_margin:>14.1%}\")\n","print(f\"Asset Turnover:        {mfg_config.asset_turnover_ratio:>14.1f}x\")\n","print(f\"Revenue Volatility:    {REV_VOL:>14}\")\n","print(f\"Tax Rate:              {mfg_config.tax_rate:>13.1%}\")\n","print(f\"Retention Ratio:       {mfg_config.retention_ratio:>13.1%}\")\n","print(\"=\" * 60)"]},{"cell_type":"markdown","metadata":{"id":"5kXbr46X1SEs"},"source":["### Shared Simulation Infrastructure\n","\n","Loss model, analytical LEV-based layer pricing, CRN scenario generation, and simulation engine."]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":248,"status":"ok","timestamp":1771702310603,"user":{"displayName":"Alex Filiakov","userId":"03712643796933532488"},"user_tz":300},"id":"sFTJeZfb1UJN","outputId":"24f22719-01bf-4d87-f96e-2407919c20f9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loss model validation (10,000 one-year samples):\n","  Expected annual loss:  $     810,152\n","  Operating income:      $   1,500,000\n","  Loss / Income ratio:   54%\n","  Std dev annual loss:   $   2,232,722\n","  Max annual loss:       $  78,067,932\n","\n","Analytical layer pricing (target LR = 70%):\n","    Attachment         Limit       E[Loss]       Premium       ROL\n","  ------------  ------------  ------------  ------------  --------\n","  $     10,000  $  4,990,000  $    658,191  $    940,273   18.84%\n","  $     25,000  $  4,975,000  $    638,549  $    912,213   18.34%\n","  $     50,000  $  4,950,000  $    613,275  $    876,107   17.70%\n","  $    250,000  $  4,750,000  $    493,355  $    704,793   14.84%\n","  $  1,000,000  $  4,000,000  $    260,635  $    372,335    9.31%\n","  $  2,000,000  $  3,000,000  $    121,171  $    173,101    5.77%\n","  $  4,000,000  $  1,000,000  $     24,540  $     35,057    3.51%\n","  $  5,000,000  $ 20,000,000  $    109,088  $    155,841    0.78%\n","  $ 25,000,000  $ 25,000,000  $     20,511  $     29,301    0.12%\n","  $ 50,000,000  $ 50,000,000  $     11,164  $     15,948    0.03%\n","\n","Pre-generating CRN loss pool (500 paths x 1 years)...\n","  Done in 0.0s\n","  Shape: 500 paths x 1 years\n","  Mean annual loss: $700,309\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import matplotlib.gridspec as gridspec\n","from matplotlib.colors import LinearSegmentedColormap\n","from concurrent.futures import ProcessPoolExecutor\n","import warnings\n","import logging\n","import time\n","\n","# Suppress all warnings and verbose solver logging\n","warnings.filterwarnings(\"ignore\")\n","logging.getLogger(\"ergodic_insurance\").setLevel(logging.ERROR)\n","\n","from ergodic_insurance.hjb_solver import (\n","    StateVariable, ControlVariable, StateSpace,\n","    LogUtility, PowerUtility, ExpectedWealth,\n","    HJBProblem, HJBSolver, HJBSolverConfig,\n",")\n","from ergodic_insurance.optimal_control import (\n","    ControlSpace, StaticControl, HJBFeedbackControl,\n","    TimeVaryingControl, OptimalController,\n",")\n","from ergodic_insurance.config import ManufacturerConfig\n","from ergodic_insurance.manufacturer import WidgetManufacturer\n","from ergodic_insurance.insurance_program import (\n","    EnhancedInsuranceLayer, InsuranceProgram,\n",")\n","from ergodic_insurance.loss_distributions import (\n","    ManufacturingLossGenerator, LognormalLoss, ParetoLoss,\n",")\n","from ergodic_insurance.insurance_pricing import LayerPricer\n","\n","plt.style.use(\"seaborn-v0_8-darkgrid\")\n","SEED = 42\n","np.random.seed(SEED)\n","N_CORES = 40   # Available parallel cores for sensitivity sweep\n","CI = False      # Set True to skip heavy computations\n","\n","# =====================================================\n","# SHARED SIMULATION INFRASTRUCTURE\n","# =====================================================\n","# Used by Parts 5, 8, 9, 10, and 11.\n","\n","# --- Economic Parameters ---\n","REFERENCE_REVENUE = ATR * INITIAL_ASSETS  # Fixed reference for loss calibration\n","\n","# --- Loss Scaling ---\n","# Loss frequency (and CRN loss amounts) scale with the square root of\n","# revenue.  This keeps the loss drag proportional to the company's\n","# actual size for both insured and uninsured strategies.\n","FREQ_SCALING_EXPONENT = 0.75\n","\n","# --- Amplified Loss Model ---\n","ATTR_BASE_FREQ = 3\n","ATTR_SEV_MEAN = 10_000\n","ATTR_SEV_CV = 15\n","\n","LG_BASE_FREQ = 1.25\n","LG_SEV_MEAN = 400_000\n","LG_SEV_CV = 7\n","\n","CAT_BASE_FREQ = 0.15\n","CAT_SEV_ALPHA = 2.01\n","CAT_SEV_XM = 1_000_000\n","\n","LOSS_PARAMS = dict(\n","    attritional_params={'base_frequency': ATTR_BASE_FREQ,\n","                        'severity_mean': ATTR_SEV_MEAN,\n","                        'severity_cv': ATTR_SEV_CV,\n","                        'revenue_scaling_exponent': FREQ_SCALING_EXPONENT,\n","                        'reference_revenue': REFERENCE_REVENUE},\n","    large_params={'base_frequency': LG_BASE_FREQ,\n","                  'severity_mean': LG_SEV_MEAN,\n","                  'severity_cv': LG_SEV_CV,\n","                  'revenue_scaling_exponent': FREQ_SCALING_EXPONENT,\n","                  'reference_revenue': REFERENCE_REVENUE},\n","    catastrophic_params={'base_frequency': CAT_BASE_FREQ,\n","                         'severity_alpha': CAT_SEV_ALPHA,\n","                         'severity_xm': CAT_SEV_XM,\n","                         'revenue_scaling_exponent': FREQ_SCALING_EXPONENT,\n","                         'reference_revenue': REFERENCE_REVENUE},\n",")\n","\n","# Quick validation of the loss model\n","_val_gen = ManufacturingLossGenerator(**LOSS_PARAMS, seed=99)\n","_val_totals = []\n","SCENARIOS = 10_000\n","for _ in range(SCENARIOS):\n","    _events, _stats = _val_gen.generate_losses(duration=1.0, revenue=REFERENCE_REVENUE)\n","    _val_totals.append(_stats['total_amount'])\n","_expected_annual_loss = np.mean(_val_totals)\n","_operating_income = INITIAL_ASSETS * ATR * OPERATING_MARGIN\n","print(f\"Loss model validation ({SCENARIOS:,.0f} one-year samples):\")\n","print(f\"  Expected annual loss:  ${_expected_annual_loss:>12,.0f}\")\n","print(f\"  Operating income:      ${_operating_income:>12,.0f}\")\n","print(f\"  Loss / Income ratio:   {_expected_annual_loss / _operating_income:.0%}\")\n","print(f\"  Std dev annual loss:   ${np.std(_val_totals):>12,.0f}\")\n","print(f\"  Max annual loss:       ${np.max(_val_totals):>12,.0f}\")\n","del _val_gen, _val_totals, _events, _stats\n","\n","\n","# --- Analytical Layer Pricing via LEV ---\n","# Instead of hardcoded rate-on-line values, we compute actuarially sound\n","# premiums from the known severity distributions using limited expected\n","# values (LEVs).  For each layer (attachment a, limit l):\n","#\n","#   E[layer loss] = sum_i  freq_i * [LEV_i(a+l) - LEV_i(a)]\n","#   premium       = E[layer loss] / target_loss_ratio\n","#   rate_on_line  = premium / limit\n","#\n","# This ensures the primary-layer ROL decreases naturally as the Ded\n","# (retention) rises, producing the genuine cost-vs-variance tradeoff\n","# that the HJB solver needs.\n","#\n","# The pricers are parameterized so that the sensitivity analysis (Part 9)\n","# can adapt premiums to match the modified loss assumptions being tested.\n","\n","TARGET_LOSS_RATIO = 0.7  # Normal-market loss ratio\n","LOSS_RATIO_INFLECTION = 1  # Factor by which the max layer loss ratio differs from the base\n","\n","def make_layer_pricers(large_freq=LG_BASE_FREQ,\n","                        large_sev_mean=LG_SEV_MEAN,\n","                        cur_revenue=REFERENCE_REVENUE) -> tuple:\n","    \"\"\"Create a tuple of LayerPricers for a given loss parameterization.\n","\n","    Frequency scales as (revenue / reference)^0.5, matching the loss\n","    model's sub-linear revenue scaling.  This keeps premium and loss\n","    scaling consistent so that insured and uninsured strategies face\n","    the same proportional cost growth.\n","\n","    Args:\n","        large_freq: Large-loss annual frequency (default 1.0).\n","        large_sev_mean: Large-loss mean severity (default $1M).\n","        cur_revenue: Current revenue for frequency scaling.\n","\n","    Returns:\n","        Tuple of (attritional, large, catastrophic) LayerPricers.\n","    \"\"\"\n","    scale = (cur_revenue / REFERENCE_REVENUE) ** FREQ_SCALING_EXPONENT\n","    return (\n","        LayerPricer(LognormalLoss(mean=ATTR_SEV_MEAN, cv=ATTR_SEV_CV),\n","                    frequency=ATTR_BASE_FREQ * scale),\n","        LayerPricer(LognormalLoss(mean=large_sev_mean, cv=LG_SEV_CV),\n","                    frequency=large_freq * scale),\n","        LayerPricer(ParetoLoss(alpha=CAT_SEV_ALPHA, xm=CAT_SEV_XM),\n","                    frequency=CAT_BASE_FREQ * scale),\n","    )\n","\n","\n","# Default pricers for baseline loss model\n","DEFAULT_PRICERS = make_layer_pricers()\n","\n","MIN_LAYER_MIDPOINT = np.mean([0, 5_000_000])\n","MAX_LAYER_MIDPOINT = np.mean([450_000_000, 500_000_000])\n","\n","def analytical_layer_premium(attachment: float, limit: float,\n","                            base_loss_ratio: float,\n","                            loss_ratio_inflection: float,\n","                            pricers=None) -> float:\n","    \"\"\"Compute actuarial premium for a layer using LEV-based expected losses.\n","\n","    Premium = E[layer loss] / layer_loss_ratio, where:\n","      E[layer loss] = sum over components of freq_i * (LEV_i(a+l) - LEV_i(a))\n","\n","    Args:\n","        attachment: Layer attachment point.\n","        limit: Layer limit (width of coverage).\n","        pricers: Tuple of LayerPricers. Uses DEFAULT_PRICERS if None.\n","        base_loss_ratio: Desired loss ratio for the layer.\n","        loss_ratio_inflection: Factor by which the max layer loss ratio  differs from the base.\n","    \"\"\"\n","\n","    pricers = pricers or DEFAULT_PRICERS\n","    expected_loss = sum(p.expected_layer_loss(attachment, limit) for p in pricers)\n","    cur_layer_midpoint = np.mean([attachment, attachment + limit])\n","    layer_loss_ratio = base_loss_ratio + \\\n","                        (1.0 / loss_ratio_inflection - 1.0) * base_loss_ratio \\\n","                        * (cur_layer_midpoint - MIN_LAYER_MIDPOINT) \\\n","                        / (MAX_LAYER_MIDPOINT - MIN_LAYER_MIDPOINT)\n","    return expected_loss / layer_loss_ratio\n","\n","\n","def analytical_rate_on_line(attachment: float, limit: float,\n","                            base_loss_ratio: float,\n","                            loss_ratio_inflection: float,\n","                            pricers=None) -> float:\n","    \"\"\"Compute rate-on-line for a layer: premium / limit.\"\"\"\n","    if limit <= 0:\n","        return 0.0\n","    return analytical_layer_premium(attachment,\n","                                    limit,\n","                                    base_loss_ratio,\n","                                    loss_ratio_inflection,\n","                                    pricers) / limit\n","\n","\n","# Validate: show how ROL varies across sample attachment points\n","print(f\"\\nAnalytical layer pricing (target LR = {TARGET_LOSS_RATIO:.0%}):\")\n","print(f\"  {'Attachment':>12s}  {'Limit':>12s}  {'E[Loss]':>12s}  {'Premium':>12s}  {'ROL':>8s}\")\n","print(f\"  {'-'*12}  {'-'*12}  {'-'*12}  {'-'*12}  {'-'*8}\")\n","for _a, _l in [(10_000, 4_990_000), (25_000, 4_975_000), (50_000, 4_950_000),\n","                (250_000, 4_750_000), (1_000_000, 4_000_000),\n","                (2_000_000, 3_000_000), (4_000_000, 1_000_000),\n","                (5_000_000, 20_000_000), (25_000_000, 25_000_000), (50_000_000, 50_000_000)]:\n","    _el = analytical_layer_premium(_a, _l, TARGET_LOSS_RATIO, 1.0) * TARGET_LOSS_RATIO\n","    _p = analytical_layer_premium(_a, _l, TARGET_LOSS_RATIO, 1.0)\n","    _r = analytical_rate_on_line(_a, _l, TARGET_LOSS_RATIO, 1.0)\n","    print(f\"  ${_a:>11,.0f}  ${_l:>11,.0f}  ${_el:>11,.0f}  ${_p:>11,.0f}  {_r:>7.2%}\")\n","\n","\n","# --- Insurance Tower Factory ---\n","# Premium rates are computed analytically from the loss distribution,\n","# ensuring that the primary-layer ROL decreases with higher retention.\n","# The optional `pricers` argument lets the sensitivity analysis pass\n","# in LayerPricers built from alternative loss assumptions, so that\n","# premiums stay consistent with the loss environment being tested.\n","\n","def make_program(ded: float,\n","                base_loss_ratio: float,\n","                loss_ratio_inflection: float,\n","                max_limit: float,\n","                pricers=None) -> InsuranceProgram:\n","    \"\"\"Create 4-layer tower with analytically priced premiums.\n","\n","    Uses LEV-based layer pricing from severity distributions so that\n","    rate-on-line adjusts naturally with the retention level.\n","\n","    Args:\n","        ded: Deductible.\n","        max_limit: Maximum coverage limit (top of tower).\n","        pricers: Tuple of LayerPricers. Uses DEFAULT_PRICERS if None.\n","\n","    Returns:\n","        InsuranceProgram with actuarially sound premium loading.\n","    \"\"\"\n","    layer_defs = [\n","        # (attachment, ceiling, reinstatements)\n","        (0, 5_000_000, 0),\n","        (5_000_000, 10_000_000, 0),\n","        (10_000_000, 25_000_000, 0),\n","        (25_000_000, 50_000_000, 0),\n","        (50_000_000, 100_000_000, 0),\n","        (100_000_000, 150_000_000, 0),\n","        (150_000_000, 200_000_000, 0),\n","        (200_000_000, 250_000_000, 0),\n","        (250_000_000, 300_000_000, 0),\n","        (300_000_000, 350_000_000, 0),\n","        (350_000_000, 400_000_000, 0),\n","        (400_000_000, 450_000_000, 0),\n","        (450_000_000, 500_000_000, 0),\n","    ]\n","    layers = []\n","    for attach, ceiling, reinst in layer_defs:\n","        if ded >= ceiling:\n","            continue  # Skip layers that are fully below the deductible\n","        if max_limit is not None and ceiling > max_limit:\n","            continue  # Skip layers that exceed the max limit constraint\n","        # Now we're within the working layer\n","        # The deductible is below, the max limit is above\n","        effective_attach = max(attach, ded)\n","        limit = ceiling - effective_attach\n","        if limit <= 0:\n","            continue\n","        rol = analytical_rate_on_line(effective_attach, limit, base_loss_ratio, loss_ratio_inflection, pricers)\n","        layers.append(EnhancedInsuranceLayer(\n","            attachment_point=effective_attach,\n","            limit=limit,\n","            base_premium_rate=rol,\n","            reinstatements=reinst,\n","        ))\n","    return InsuranceProgram(\n","        layers=layers,\n","        deductible=ded,\n","        name=f\"Manufacturing Tower (Ded=${ded:,.0f})\",\n","    )\n","\n","\n","# --- CRN: Pre-generate Loss Scenarios ---\n","def generate_loss_pool(n_paths, n_years, reference_revenue=REFERENCE_REVENUE, seed=SEED, specific_loss_params=None):\n","    \"\"\"Pre-generate loss scenarios for Common Random Number comparison.\n","    All strategies will face the exact same loss events and revenue shocks.\n","    Losses are generated at a fixed reference revenue; the simulation\n","    engine then scales event amounts by (actual_revenue / reference)^0.5\n","    so that loss burden grows proportionally with the company.\n","\n","    Args:\n","        n_paths: Number of simulation paths.\n","        n_years: Number of years to simulate.\n","        reference_revenue: Reference revenue for loss calibration.\n","        seed: Base seed for random number generation.\n","        specific_loss_params: A dictionary of loss parameters to use, overriding\n","                              the global LOSS_PARAMS for this call.\n","    \"\"\"\n","    loss_params_to_use = specific_loss_params if specific_loss_params is not None else LOSS_PARAMS\n","\n","    ss = np.random.SeedSequence(seed)\n","    children = ss.spawn(n_paths + 1)\n","\n","    # Shared revenue shocks\n","    rev_rng = np.random.default_rng(children[0])\n","    revenue_shocks = rev_rng.standard_normal((n_paths, n_years))\n","\n","    # Per-path loss event sequences\n","    all_losses = []  # [path][year] -> List[LossEvent]\n","    for i in range(n_paths):\n","        gen = ManufacturingLossGenerator(\n","            **loss_params_to_use, # Use the potentially overridden loss params\n","            seed=int(children[i + 1].generate_state(1)[0] % (2**31)),\n","        )\n","        path_losses = []\n","        for t in range(n_years):\n","            events, _ = gen.generate_losses(duration=1.0, revenue=reference_revenue)\n","            path_losses.append(events)\n","        all_losses.append(path_losses)\n","\n","    return revenue_shocks, all_losses\n","\n","\n","# --- CRN Simulation Engine ---\n","def simulate_with_crn(ded,\n","                    base_loss_ratio: float,\n","                    loss_ratio_inflection: float,\n","                    max_limit: float,\n","                    revenue_shocks, loss_pool, n_years=1,\n","                    initial_assets=INITIAL_ASSETS, pricers=None):\n","    \"\"\"Simulate one static-Ded strategy across all CRN paths.\n","\n","    Uses the library's InsuranceProgram.process_claim() to correctly\n","    allocate each loss through the insurance tower.\n","\n","    Loss amounts from the CRN pool are scaled by\n","    (actual_revenue / REFERENCE_REVENUE)^FREQ_SCALING_EXPONENT so that\n","    the loss burden grows proportionally with the company.  Premium is\n","    repriced at actual revenue with the same exponent, keeping the\n","    cost-of-risk consistent between insured and uninsured strategies.\n","\n","    Args:\n","        ded: Deductible.\n","        revenue_shocks: Pre-generated revenue shocks (n_paths x n_years).\n","        loss_pool: Pre-generated loss events [path][year] -> List[LossEvent].\n","        n_years: Simulation horizon.\n","        initial_assets: Starting wealth.\n","        pricers: Tuple of LayerPricers for premium calculation.\n","            Uses DEFAULT_PRICERS if None (baseline loss assumptions).\n","\n","    Returns:\n","        paths: array of shape (n_paths, n_years + 1) with asset values.\n","    \"\"\"\n","    n_paths = len(loss_pool)\n","    paths = np.zeros((n_paths, n_years + 1))\n","    paths[:, 0] = initial_assets\n","\n","    # Build program template and get fixed annual premium\n","    if ded >= 100_000_000:\n","        # \"No insurance\" -- skip tower entirely\n","        annual_premium = 0.0\n","        use_insurance = False\n","        program_template = None\n","    else:\n","        program_template = make_program(ded,\n","                                        base_loss_ratio,\n","                                        loss_ratio_inflection,\n","                                        max_limit,\n","                                        pricers=pricers)\n","        annual_premium = program_template.calculate_premium()\n","        use_insurance = True\n","\n","    for i in range(n_paths):\n","        assets = initial_assets\n","        for t in range(n_years):\n","            # Operating income with shared revenue shock\n","            revenue = assets * ATR * np.exp(\n","                REV_VOL * revenue_shocks[i, t] - 0.5 * REV_VOL**2\n","            )\n","            operating_income = revenue * OPERATING_MARGIN\n","\n","            # Scale CRN losses to current revenue (sqrt scaling)\n","            loss_scale = (revenue / REFERENCE_REVENUE) ** FREQ_SCALING_EXPONENT\n","\n","            # Process losses through insurance tower\n","            total_retained = 0.0\n","            if use_insurance:\n","                new_pricers = make_layer_pricers(cur_revenue=revenue)\n","                program_update = make_program(ded,\n","                                                base_loss_ratio,\n","                                                loss_ratio_inflection,\n","                                                max_limit,\n","                                                pricers=new_pricers)\n","                annual_premium = program_update.calculate_premium()\n","                program = InsuranceProgram.create_fresh(program_update)\n","                for event in loss_pool[i][t]:\n","                    scaled_amount = event.amount * loss_scale\n","                    result = program.process_claim(scaled_amount)\n","                    total_retained += result.deductible_paid + result.uncovered_loss\n","            else:\n","                for event in loss_pool[i][t]:\n","                    total_retained += event.amount * loss_scale\n","\n","            # Net income and asset update\n","            assets = assets + operating_income - total_retained - annual_premium\n","            assets = max(assets, 0.0)\n","            paths[i, t + 1] = assets\n","\n","    return paths\n","\n","\n","# Pre-generate the main CRN pool\n","N_PATHS = 500\n","N_YEARS = 1\n","print(f\"\\nPre-generating CRN loss pool ({N_PATHS:,.0f} paths x {N_YEARS} years)...\")\n","t0 = time.time()\n","CRN_SHOCKS, CRN_LOSSES = generate_loss_pool(n_paths=N_PATHS, n_years=N_YEARS)\n","print(f\"  Done in {time.time() - t0:.1f}s\")\n","print(f\"  Shape: {CRN_SHOCKS.shape[0]:,} paths x {CRN_SHOCKS.shape[1]} years\")\n","\n","# Quick sanity: total losses per path-year\n","_annual_totals = [\n","    sum(e.amount for e in CRN_LOSSES[i][t])\n","    for i in range(N_PATHS) for t in range(N_YEARS)\n","]\n","print(f\"  Mean annual loss: ${np.mean(_annual_totals):,.0f}\")\n","del _annual_totals\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"sobol_is_infrastructure","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1771702310658,"user_tz":300,"elapsed":33,"user":{"displayName":"Alex Filiakov","userId":"03712643796933532488"}},"outputId":"fd98aac9-d50a-4a1d-cb57-299b756d417b"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Sobol loss pool validation (10,000 paths at reference revenue):\n","  Mean annual loss:   $     835,429\n","  Std dev:            $   2,902,205\n","  Max single event:   $ 141,541,691\n","  Paths with loss > $50M event: 4\n","  Sobol dims: 23, Max events/path: 19\n"]}],"source":["# =====================================================\n","# SOBOL QUASI-RANDOM LOSS GENERATION & IMPORTANCE SAMPLING\n","# =====================================================\n","# Quasi-random low-discrepancy sequences for variance reduction,\n","# plus rejection-based importance sampling for tail events.\n","\n","from scipy.stats.qmc import Sobol as _Sobol\n","from scipy.stats import norm as _sp_norm, poisson as _sp_poisson\n","\n","# Truncation limits for compound Poisson event counts\n","MAX_ATTR_EVENTS  = 10\n","MAX_LARGE_EVENTS = 6\n","MAX_CAT_EVENTS   = 3\n","MAX_EVENTS = MAX_ATTR_EVENTS + MAX_LARGE_EVENTS + MAX_CAT_EVENTS   # 19\n","SOBOL_DIMS = 1 + (1 + MAX_ATTR_EVENTS) + (1 + MAX_LARGE_EVENTS) + (1 + MAX_CAT_EVENTS)  # 23\n","\n","\n","def generate_sobol_loss_pool(n_paths, revenue, seed=0):\n","    \"\"\"Generate loss scenarios using 23-dim Sobol quasi-random sequences.\n","\n","    Dimension layout:\n","      0:       revenue shock  ~ N(0,1)\n","      1-11:    attritional count (Poisson) + 10 severities (Lognormal)\n","      12-18:   large count (Poisson) + 6 severities (Lognormal)\n","      19-22:   cat count (Poisson) + 3 severities (Pareto)\n","\n","    Returns:\n","        revenue_shocks:       (n_paths, 1) standard normal shocks\n","        loss_amounts:         (n_paths, MAX_EVENTS) individual losses (0 = unused slot)\n","        max_individual_loss:  (n_paths,) largest single loss per path\n","    \"\"\"\n","    scale = (revenue / REFERENCE_REVENUE) ** FREQ_SCALING_EXPONENT\n","    attr_freq = ATTR_BASE_FREQ * scale\n","    lg_freq   = LG_BASE_FREQ * scale\n","    cat_freq  = CAT_BASE_FREQ * scale\n","\n","    attr_var = np.log(1 + ATTR_SEV_CV**2)\n","    attr_mu  = np.log(ATTR_SEV_MEAN) - attr_var / 2\n","    attr_sig = np.sqrt(attr_var)\n","\n","    lg_var = np.log(1 + LG_SEV_CV**2)\n","    lg_mu  = np.log(LG_SEV_MEAN) - lg_var / 2\n","    lg_sig = np.sqrt(lg_var)\n","\n","    sampler = _Sobol(d=SOBOL_DIMS, scramble=True, seed=seed)\n","    m = int(np.ceil(np.log2(max(n_paths, 2))))\n","    U = sampler.random(2**m)[:n_paths]\n","    eps = 1e-10\n","    col = 0\n","\n","    # Revenue shock ~ N(0,1)\n","    revenue_shocks = _sp_norm.ppf(np.clip(U[:, col], eps, 1 - eps)).reshape(-1, 1)\n","    col += 1\n","\n","    # Attritional: Poisson count + Lognormal severities\n","    attr_n = _sp_poisson.ppf(np.clip(U[:, col], 0, 1 - eps), attr_freq).astype(int)\n","    attr_n = np.minimum(attr_n, MAX_ATTR_EVENTS)\n","    col += 1\n","    attr_raw = np.exp(attr_mu + attr_sig * _sp_norm.ppf(\n","        np.clip(U[:, col:col + MAX_ATTR_EVENTS], eps, 1 - eps)))\n","    col += MAX_ATTR_EVENTS\n","    attr_sevs = attr_raw * (np.arange(MAX_ATTR_EVENTS) < attr_n[:, None])\n","\n","    # Large: Poisson count + Lognormal severities\n","    lg_n = _sp_poisson.ppf(np.clip(U[:, col], 0, 1 - eps), lg_freq).astype(int)\n","    lg_n = np.minimum(lg_n, MAX_LARGE_EVENTS)\n","    col += 1\n","    lg_raw = np.exp(lg_mu + lg_sig * _sp_norm.ppf(\n","        np.clip(U[:, col:col + MAX_LARGE_EVENTS], eps, 1 - eps)))\n","    col += MAX_LARGE_EVENTS\n","    lg_sevs = lg_raw * (np.arange(MAX_LARGE_EVENTS) < lg_n[:, None])\n","\n","    # Catastrophic: Poisson count + Pareto severities\n","    cat_n = _sp_poisson.ppf(np.clip(U[:, col], 0, 1 - eps), cat_freq).astype(int)\n","    cat_n = np.minimum(cat_n, MAX_CAT_EVENTS)\n","    col += 1\n","    cat_raw = CAT_SEV_XM / (1 - np.clip(\n","        U[:, col:col + MAX_CAT_EVENTS], 0, 1 - eps)) ** (1 / CAT_SEV_ALPHA)\n","    col += MAX_CAT_EVENTS\n","    cat_sevs = cat_raw * (np.arange(MAX_CAT_EVENTS) < cat_n[:, None])\n","\n","    loss_amounts = np.hstack([attr_sevs, lg_sevs, cat_sevs])\n","    max_individual_loss = np.max(loss_amounts, axis=1)\n","\n","    return revenue_shocks, loss_amounts, max_individual_loss\n","\n","\n","def generate_is_pool_rejection(n_target, revenue, threshold=50_000_000,\n","                               seed_offset=1_000_000, batch_size=260_000,\n","                               verbose=False):\n","    \"\"\"Generate importance-sampled tail paths via rejection sampling.\n","\n","    Generates Sobol batches, keeps only paths where the max individual\n","    loss exceeds the threshold.  Continues until n_target paths collected.\n","\n","    Returns:\n","        is_shocks:  (n_target, 1) revenue shocks for tail paths\n","        is_losses:  (n_target, MAX_EVENTS) loss amounts for tail paths\n","        p_tail:     estimated tail probability (acceptance rate)\n","    \"\"\"\n","    collected_shocks, collected_losses = [], []\n","    n_generated = n_accepted = batch_num = 0\n","\n","    while n_accepted < n_target:\n","        shocks, losses, max_loss = generate_sobol_loss_pool(\n","            batch_size, revenue, seed=seed_offset + batch_num)\n","        mask = max_loss > threshold\n","        n_hit = int(mask.sum())\n","        if n_hit > 0:\n","            collected_shocks.append(shocks[mask])\n","            collected_losses.append(losses[mask])\n","            n_accepted += n_hit\n","        n_generated += len(max_loss)\n","        batch_num += 1\n","        if verbose and batch_num % 10 == 0:\n","            print(f\"  IS rejection: {n_accepted}/{n_target} from \"\n","                  f\"{n_generated:,} ({n_accepted/max(n_generated,1):.6%})\")\n","\n","    is_shocks = np.vstack(collected_shocks)[:n_target]\n","    is_losses = np.vstack(collected_losses)[:n_target]\n","    p_tail = n_accepted / n_generated\n","\n","    if verbose:\n","        print(f\"  IS complete: {n_target} tail paths from {n_generated:,} \"\n","              f\"({p_tail:.6%}, {batch_num} batches)\")\n","\n","    return is_shocks, is_losses, p_tail\n","\n","\n","def compute_stratified_stats(W_T_main, W_T_is, tail_mask_main, initial_assets):\n","    \"\"\"Combine main and IS pools via stratified estimation.\n","\n","    The tail probability is estimated from the main pool (unbiased).\n","    Tail-stratum statistics use both main-pool tail paths and IS paths\n","    for improved precision in the extreme-loss regime.\n","\n","    Returns:\n","        (growth_rate, growth_vol, ruin_prob)\n","    \"\"\"\n","    N_main = len(W_T_main)\n","    p_tail = tail_mask_main.sum() / N_main\n","\n","    log_g_main = np.log(np.maximum(W_T_main, 1.0) / initial_assets)\n","    log_g_is   = np.log(np.maximum(W_T_is, 1.0) / initial_assets)\n","\n","    normal_mask = ~tail_mask_main\n","    if normal_mask.any():\n","        lg_n = log_g_main[normal_mask]\n","        mean_n, meansq_n = lg_n.mean(), (lg_n**2).mean()\n","        ruin_n = float((W_T_main[normal_mask] <= 0).mean())\n","    else:\n","        mean_n = meansq_n = ruin_n = 0.0\n","\n","    # Tail stratum: main-pool tail paths + IS paths\n","    if tail_mask_main.any():\n","        lg_t = np.concatenate([log_g_main[tail_mask_main], log_g_is])\n","        wt_t = np.concatenate([W_T_main[tail_mask_main], W_T_is])\n","    else:\n","        lg_t, wt_t = log_g_is, W_T_is\n","\n","    mean_t, meansq_t = lg_t.mean(), (lg_t**2).mean()\n","    ruin_t = float((wt_t <= 0).mean())\n","\n","    growth_rate = (1 - p_tail) * mean_n + p_tail * mean_t\n","    E_sq = (1 - p_tail) * meansq_n + p_tail * meansq_t\n","    growth_vol  = np.sqrt(max(E_sq - growth_rate**2, 0.0))\n","    ruin_prob   = (1 - p_tail) * ruin_n + p_tail * ruin_t\n","\n","    return growth_rate, growth_vol, ruin_prob\n","\n","\n","# Quick validation of the Sobol generator\n","_val_shocks, _val_losses, _val_max = generate_sobol_loss_pool(10_000, REFERENCE_REVENUE, seed=99)\n","_val_totals = _val_losses.sum(axis=1)\n","print(f\"\\nSobol loss pool validation (10,000 paths at reference revenue):\")\n","print(f\"  Mean annual loss:   ${np.mean(_val_totals):>12,.0f}\")\n","print(f\"  Std dev:            ${np.std(_val_totals):>12,.0f}\")\n","print(f\"  Max single event:   ${np.max(_val_max):>12,.0f}\")\n","print(f\"  Paths with loss > $50M event: {(np.max(_val_losses, axis=1) > 50_000_000).sum()}\")\n","print(f\"  Sobol dims: {SOBOL_DIMS}, Max events/path: {MAX_EVENTS}\")\n","del _val_shocks, _val_losses, _val_max, _val_totals"]},{"cell_type":"markdown","metadata":{"id":"W4u_vNHU8r2w"},"source":["## Part II: Insurance Demand Curve"]},{"cell_type":"markdown","metadata":{"id":"pg-aY2jL8r2w"},"source":["For each revenue level in `revenue_grid`, we sweep all valid `(ded, hollow_bottom, hollow_top, max_limit)` tuples\n","and simulate `SENS_N_PATHS` one-year CRN paths.\n","\n","The **hollow tower** has coverage in two bands with a gap:\n","- **Lower band**: `[ded, hollow_bottom]` — primary / working layer coverage\n","- **Hollow (gap)**: `[hollow_bottom, hollow_top]` — **no coverage** (retained by the company)\n","- **Upper band**: `[hollow_top, max_limit]` — excess layer coverage\n","\n","The optimal tower at each revenue is selected by maximizing a **weighted objective**:\n","- **75% weight** on annual ergodic growth rate\n","- **25% weight** on reducing result volatility (std of log-growth)\n","\n","subject to the **hard constraint** that the probability of ruin does not exceed **0.5%**.\n","\n","Losses scale with revenue via a **0.75 exponent**, and the optimizer chooses where to place the\n","deductible (bottom), hollow section (gap boundaries), and max limit (top)."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1771702310690,"user":{"displayName":"Alex Filiakov","userId":"03712643796933532488"},"user_tz":300},"id":"rd7LR-Nhy_17","outputId":"724f3813-7e4c-4254-9e32-03d3eac5e072"},"outputs":[{"output_type":"stream","name":"stdout","text":["Deductible grid:   35 values ($10,000 -- $228,546,386)\n","Hollow grid:       30 values ($106,082 -- $554,102,033)\n","Max limit grid:    26 values ($623,551 -- $1,000,000,000)\n","Revenue grid:      3 values ($5,000,000 -- $50,000,000)\n","\n","Valid (ded, hb, ht, ml) tuples per worker: 87,507\n","Scan grid: 99 (revenue, lr, inflection) combos\n","Total evaluations: ~8,663,193\n"]}],"source":["import itertools\n","\n","# --- HOLLOW TOWER GRIDS ---\n","# 4D sweep: (ded, hollow_bottom, hollow_top, max_limit)\n","# Coverage: [ded, hollow_bottom] + [hollow_top, max_limit]\n","# Gap:      [hollow_bottom, hollow_top]  (no coverage)\n","#\n","# Grids are coarser than the non-hollow experiment to keep the\n","# combinatorial explosion manageable.  Expand for production runs.\n","\n","all_ranges = np.geomspace(10_000, 1_000_000_000, 40)\n","\n","ded_grid = list(filter(lambda x: x < 250_000_000, all_ranges))\n","\n","hollow_grid = list(filter(lambda x: (x > 100_000 and x < 650_000_000), all_ranges))\n","\n","max_limit_grid = list(filter(lambda x: x > 500_000, all_ranges))\n","\n","### 3 outer parameters to sweep: revenue, base loss ratio, inflection factor\n","base_loss_ratio_grid = np.linspace(0.4, 0.9, 11)\n","\n","inflection_factor_grid = np.array([\n","    1.0,\n","    2.0,\n","    3.0,\n","])\n","\n","revenue_grid = np.array([\n","    5_000_000,\n","    25_000_000,\n","    50_000_000,\n","])\n","\n","# Grid for parallel sweeping to determine demand curves\n","scan_grid = list(itertools.product(revenue_grid, base_loss_ratio_grid, inflection_factor_grid))\n","\n","# Count valid 4-tuples (ded < hb < ht < ml)\n","n_valid = 0\n","for d in ded_grid:\n","    for i_hb, hb in enumerate(hollow_grid):\n","        if hb < d:\n","            continue\n","        for ht in hollow_grid[i_hb + 1:]:\n","            for ml in max_limit_grid:\n","                if ml < ht:\n","                    continue\n","                n_valid += 1\n","\n","print(f\"Deductible grid:   {len(ded_grid)} values \"\n","      f\"(${ded_grid[0]:,.0f} -- ${ded_grid[-1]:,.0f})\")\n","print(f\"Hollow grid:       {len(hollow_grid)} values \"\n","      f\"(${hollow_grid[0]:,.0f} -- ${hollow_grid[-1]:,.0f})\")\n","print(f\"Max limit grid:    {len(max_limit_grid)} values \"\n","      f\"(${max_limit_grid[0]:,.0f} -- ${max_limit_grid[-1]:,.0f})\")\n","print(f\"Revenue grid:      {len(revenue_grid)} values \"\n","      f\"(${revenue_grid[0]:,.0f} -- ${revenue_grid[-1]:,.0f})\")\n","print(f\"\\nValid (ded, hb, ht, ml) tuples per worker: {n_valid:,}\")\n","print(f\"Scan grid: {len(scan_grid)} (revenue, lr, inflection) combos\")\n","print(f\"Total evaluations: ~{n_valid * len(scan_grid):,}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VPJbzkn28r2w","outputId":"1367ebee-829d-45fe-a683-cfb98a146123"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cache miss -- computing hollow tower experiment (Sobol + IS + Sharpe + min-adequate)...\n","\n","Hollow tower optimization: 99 combos x ~87,507 valid (ded, hb, ht, ml) tuples\n","  Main pool:  100,000 Sobol paths per worker\n","  IS pool:    2,000 tail paths (threshold $50,000,000)\n","  Scoring:    Sharpe ratio (growth_rate / growth_vol)\n","  Constraint: P(ruin) <= 0.25%\n","  Selection:  min-adequate (tolerance 0.5% on covered_width)\n","Parallel on 44 cores\n","\n"]},{"output_type":"stream","name":"stderr","text":["[Parallel(n_jobs=44)]: Using backend LokyBackend with 44 concurrent workers.\n"]}],"source":["import os\n","import pickle\n","\n","from tqdm.auto import tqdm\n","\n","CACHE_DIR = 'cache'\n","os.makedirs(CACHE_DIR, exist_ok=True)\n","\n","# Versioned cache names (v2 = Sobol + IS + Sharpe + min-adequate)\n","records_base = os.path.join(CACHE_DIR, 'hollow_tower_experiment')\n","optimal_base = os.path.join(CACHE_DIR, 'hollow_optimal_towers')\n","\n","N_CORES = multiprocessing.cpu_count()\n","\n","# --- Optimization criteria ---\n","RUIN_LIMIT       = 0.0025      # hard constraint: P(ruin) <= 0.25%\n","SENS_N_PATHS     = 100_000     # main Sobol pool size\n","IS_N_PATHS       = 2_000       # importance-sampled tail paths\n","IS_THRESHOLD     = 50_000_000  # tail classification threshold ($50M)\n","SHARPE_TOLERANCE = 0.005       # 0.5% tolerance for min-adequate selection\n","\n","# --- Test override ---\n","# SENS_N_PATHS     = 500     # main Sobol pool size\n","# IS_N_PATHS       = 20       # importance-sampled tail paths\n","\n","warnings.filterwarnings(\"ignore\")\n","logging.getLogger(\"ergodic_insurance\").setLevel(logging.ERROR)\n","\n","\n","# --- Helper: split a layer region around a hollow section ---\n","def _get_covered_regions(eff_attach, eff_ceiling, hollow_bottom, hollow_top):\n","    \"\"\"Return sub-regions of [eff_attach, eff_ceiling] after removing the hollow.\"\"\"\n","    if hollow_top <= eff_attach or hollow_bottom >= eff_ceiling:\n","        return [(eff_attach, eff_ceiling)]\n","    if hollow_bottom <= eff_attach and hollow_top >= eff_ceiling:\n","        return []\n","    if hollow_bottom > eff_attach and hollow_top < eff_ceiling:\n","        return [(eff_attach, hollow_bottom), (hollow_top, eff_ceiling)]\n","    if hollow_bottom <= eff_attach:\n","        return [(hollow_top, eff_ceiling)]\n","    return [(eff_attach, hollow_bottom)]\n","\n","\n","# --- Hollow-tower make_program (for premium computation) ---\n","def make_program(ded,\n","                base_loss_ratio: float,\n","                loss_ratio_inflection: float,\n","                max_limit=None,\n","                hollow_bottom=None,\n","                hollow_top=None,\n","                pricers=None):\n","    \"\"\"Build multi-layer tower with optional hollow (gap) section.\n","\n","    Coverage bands:\n","        [ded, hollow_bottom]  +  [hollow_top, max_limit]\n","    Gap (hollow):\n","        [hollow_bottom, hollow_top]  -- no coverage, retained by the company.\n","    \"\"\"\n","    layer_defs = [\n","        (0, 5_000_000, 0),\n","        (5_000_000, 10_000_000, 0),\n","        (10_000_000, 25_000_000, 0),\n","        (25_000_000, 50_000_000, 0),\n","        (50_000_000, 100_000_000, 0),\n","        (100_000_000, 150_000_000, 0),\n","        (150_000_000, 200_000_000, 0),\n","        (200_000_000, 250_000_000, 0),\n","        (250_000_000, 300_000_000, 0),\n","        (300_000_000, 350_000_000, 0),\n","        (350_000_000, 400_000_000, 0),\n","        (400_000_000, 450_000_000, 0),\n","        (450_000_000, 500_000_000, 0),\n","    ]\n","    layers = []\n","    for attach, ceiling, reinst in layer_defs:\n","        if ded >= ceiling:\n","            continue\n","        if max_limit is not None and max_limit <= attach:\n","            continue\n","        eff_attach = max(attach, ded)\n","        eff_ceiling = min(ceiling, max_limit) if max_limit is not None else ceiling\n","        if eff_attach >= eff_ceiling:\n","            continue\n","        if hollow_bottom is not None and hollow_top is not None:\n","            regions = _get_covered_regions(eff_attach, eff_ceiling,\n","                                           hollow_bottom, hollow_top)\n","        else:\n","            regions = [(eff_attach, eff_ceiling)]\n","        for r_attach, r_ceiling in regions:\n","            limit = r_ceiling - r_attach\n","            if limit <= 0:\n","                continue\n","            rol = analytical_rate_on_line(r_attach, limit,\n","                                          base_loss_ratio,\n","                                          loss_ratio_inflection, pricers)\n","            layers.append(EnhancedInsuranceLayer(\n","                attachment_point=r_attach,\n","                limit=limit,\n","                base_premium_rate=rol,\n","                reinstatements=reinst,\n","            ))\n","    name = f\"Hollow Tower Ded=${ded:,.0f}\"\n","    if hollow_bottom is not None:\n","        name += f\" Gap=[${hollow_bottom:,.0f}-${hollow_top:,.0f}]\"\n","    if max_limit is not None:\n","        name += f\" Lim=${max_limit:,.0f}\"\n","    return InsuranceProgram(layers=layers, deductible=ded, name=name)\n","\n","\n","def _optimize_tower_for_revenue(args):\n","    \"\"\"Sweep all valid (ded, hb, ht, max_limit) combos for one (revenue, lr, inflection).\n","\n","    Methodology:\n","    - Sobol quasi-random sequences (SENS_N_PATHS paths) for main estimation\n","    - Importance sampling via rejection (IS_N_PATHS tail paths above IS_THRESHOLD)\n","    - Stratified estimator combining main + IS pools for growth/vol/ruin\n","    - Vectorized retained-loss computation (analytical, bypasses process_claim)\n","    \"\"\"\n","    revenue, base_lr, inf_lr, ded_arr, hollow_arr, ml_arr, seed_base = args\n","    initial_assets = revenue / ATR\n","\n","    # --- Generate main Sobol pool ---\n","    main_shocks, main_losses, main_max_loss = generate_sobol_loss_pool(\n","        SENS_N_PATHS, revenue, seed=seed_base)\n","    tail_mask = main_max_loss > IS_THRESHOLD\n","\n","    # --- Generate IS pool via rejection ---\n","    is_shocks, is_losses, _p_tail = generate_is_pool_rejection(\n","        IS_N_PATHS, revenue, threshold=IS_THRESHOLD,\n","        seed_offset=seed_base + 1_000_000, verbose=False)\n","\n","    pricers = make_layer_pricers(cur_revenue=revenue)\n","\n","    # --- Pre-compute path-level quantities (main pool, once per worker) ---\n","    rev_m = initial_assets * ATR * np.exp(\n","        REV_VOL * main_shocks[:, 0] - 0.5 * REV_VOL**2)\n","    oi_m  = rev_m * OPERATING_MARGIN\n","    sc_m  = (rev_m / revenue) ** FREQ_SCALING_EXPONENT\n","    sl_m  = main_losses * sc_m[:, None]   # (SENS_N_PATHS, MAX_EVENTS)\n","\n","    # --- Pre-compute path-level quantities (IS pool) ---\n","    rev_i = initial_assets * ATR * np.exp(\n","        REV_VOL * is_shocks[:, 0] - 0.5 * REV_VOL**2)\n","    oi_i  = rev_i * OPERATING_MARGIN\n","    sc_i  = (rev_i / revenue) ** FREQ_SCALING_EXPONENT\n","    sl_i  = is_losses * sc_i[:, None]     # (IS_N_PATHS, MAX_EVENTS)\n","\n","    n_events = sl_m.shape[1]\n","    records = []\n","\n","    for ded in ded_arr:\n","        for i_hb, hb in enumerate(hollow_arr):\n","            if hb < ded:\n","                continue\n","            for ht in hollow_arr[i_hb + 1:]:\n","                for ml in ml_arr:\n","                    if ml < ht:\n","                        continue\n","\n","                    # Premium via existing pricing infrastructure\n","                    program = make_program(ded, base_lr, inf_lr,\n","                                           max_limit=ml, hollow_bottom=hb,\n","                                           hollow_top=ht, pricers=pricers)\n","                    premium = program.calculate_premium()\n","\n","                    # --- Vectorized retained loss (main pool) ---\n","                    # retained = loss - band1_covered - band2_covered\n","                    # band1 covers [ded, hb], band2 covers [ht, ml]\n","                    ret_m = np.zeros(SENS_N_PATHS)\n","                    for j in range(n_events):\n","                        x = sl_m[:, j]\n","                        cov = (np.maximum(0, np.minimum(x, hb) - ded) +\n","                               np.maximum(0, np.minimum(x, ml) - ht))\n","                        ret_m += x - cov\n","                    W_m = np.maximum(initial_assets + oi_m - ret_m - premium, 0.0)\n","\n","                    # --- Vectorized retained loss (IS pool) ---\n","                    ret_i = np.zeros(IS_N_PATHS)\n","                    for j in range(n_events):\n","                        x = sl_i[:, j]\n","                        cov = (np.maximum(0, np.minimum(x, hb) - ded) +\n","                               np.maximum(0, np.minimum(x, ml) - ht))\n","                        ret_i += x - cov\n","                    W_i = np.maximum(initial_assets + oi_i - ret_i - premium, 0.0)\n","\n","                    # --- Stratified statistics ---\n","                    growth_rate, growth_vol, ruin_prob = compute_stratified_stats(\n","                        W_m, W_i, tail_mask, initial_assets)\n","\n","                    records.append({\n","                        'revenue': revenue,\n","                        'base_loss_ratio': base_lr,\n","                        'loss_ratio_inflation_factor': inf_lr,\n","                        'initial_assets': initial_assets,\n","                        'ded': ded,\n","                        'hollow_bottom': hb,\n","                        'hollow_top': ht,\n","                        'max_limit': ml,\n","                        'growth_rate': growth_rate,\n","                        'growth_vol': growth_vol,\n","                        'ruin_prob': ruin_prob,\n","                        'premium': premium,\n","                        'mean_wealth': float(np.mean(W_m)),\n","                        'covered_width': (hb - ded) + (ml - ht),\n","                    })\n","\n","    return records\n","\n","\n","# === Load from cache or compute ===\n","try:\n","    df_tower = pd.read_parquet(records_base + '.parquet')\n","    print(f\"Loaded: {records_base}.parquet ({len(df_tower):,} rows)\")\n","    df_optimal = pd.read_parquet(optimal_base + '.parquet')\n","    print(f\"Loaded: {optimal_base}.parquet ({len(df_optimal)} rows)\")\n","except Exception:\n","    print(f\"Cache miss -- computing hollow tower experiment \"\n","          f\"(Sobol + IS + Sharpe + min-adequate)...\\n\")\n","\n","    inputs = [(rev, base_lr, inf_lr, ded_grid, hollow_grid, max_limit_grid, SEED + i)\n","              for i, (rev, base_lr, inf_lr) in enumerate(scan_grid)]\n","    print(f\"Hollow tower optimization: {len(scan_grid)} combos x \"\n","          f\"~{n_valid:,} valid (ded, hb, ht, ml) tuples\")\n","    print(f\"  Main pool:  {SENS_N_PATHS:,} Sobol paths per worker\")\n","    print(f\"  IS pool:    {IS_N_PATHS:,} tail paths (threshold ${IS_THRESHOLD:,.0f})\")\n","    print(f\"  Scoring:    Sharpe ratio (growth_rate / growth_vol)\")\n","    print(f\"  Constraint: P(ruin) <= {RUIN_LIMIT:.2%}\")\n","    print(f\"  Selection:  min-adequate (tolerance {SHARPE_TOLERANCE:.1%} on covered_width)\")\n","    print(f\"Parallel on {N_CORES} cores\\n\")\n","\n","    t0 = time.time()\n","    try:\n","        from joblib import Parallel, delayed\n","        results = Parallel(n_jobs=min(N_CORES, len(scan_grid)), verbose=10)(\n","            delayed(_optimize_tower_for_revenue)(inp) for inp in inputs\n","        )\n","    except ImportError:\n","        print(\"joblib not available, trying ProcessPoolExecutor...\")\n","        try:\n","            with ProcessPoolExecutor(max_workers=min(N_CORES, len(scan_grid))) as ex:\n","                results = list(tqdm(\n","                    ex.map(_optimize_tower_for_revenue, inputs),\n","                    total=len(inputs), desc=\"Hollow tower optimization\"))\n","        except Exception:\n","            print(\"ProcessPoolExecutor failed, running serially...\")\n","            results = [_optimize_tower_for_revenue(inp)\n","                       for inp in tqdm(inputs, desc=\"Hollow tower (serial)\")]\n","    except Exception as e:\n","        print(f\"Parallel failed ({e}), running serially...\")\n","        results = [_optimize_tower_for_revenue(inp)\n","                   for inp in tqdm(inputs, desc=\"Hollow tower (serial)\")]\n","\n","    all_records = []\n","    for r in results:\n","        all_records.extend(r)\n","\n","    elapsed = time.time() - t0\n","    print(f\"\\nDone in {elapsed:.1f}s ({len(all_records):,} evaluations)\")\n","\n","    df_tower = pd.DataFrame(all_records)\n","\n","    # --- Select optimal hollow tower per (revenue, base_loss_ratio, inflection) ---\n","    # Scoring:   Sharpe = growth_rate / growth_vol\n","    # Selection: min-adequate = smallest covered_width within SHARPE_TOLERANCE of best\n","    optimal_towers = []\n","    for rev, base_lr, inf_lr in scan_grid:\n","        mask = ((df_tower['revenue'] == rev) &\n","                (np.isclose(df_tower['base_loss_ratio'], base_lr)) &\n","                (np.isclose(df_tower['loss_ratio_inflation_factor'], inf_lr)))\n","        df_combo = df_tower[mask]\n","        feasible = df_combo[df_combo['ruin_prob'] <= RUIN_LIMIT]\n","\n","        if len(feasible) > 0:\n","            g = feasible['growth_rate'].values\n","            v = feasible['growth_vol'].values\n","            sharpe = np.where(v > 1e-12, g / v, np.sign(g) * 1e12)\n","            best_sharpe = np.max(sharpe)\n","\n","            # Near-optimal: within SHARPE_TOLERANCE of best\n","            threshold_val = best_sharpe - abs(best_sharpe) * SHARPE_TOLERANCE\n","            near_opt_mask = sharpe >= threshold_val\n","            near_opt = feasible[near_opt_mask].copy()\n","            near_opt['_sharpe'] = sharpe[near_opt_mask]\n","\n","            # Min-adequate: smallest covered_width, then lowest max_limit\n","            best_idx = near_opt.sort_values(\n","                ['covered_width', 'max_limit']).index[0]\n","        else:\n","            best_idx = df_combo['ruin_prob'].idxmin()\n","\n","        row = df_tower.loc[best_idx]\n","        g_val = row['growth_rate']\n","        v_val = row['growth_vol']\n","        sharpe_val = g_val / v_val if v_val > 1e-12 else np.sign(g_val) * 1e12\n","\n","        covered_width = (row['hollow_bottom'] - row['ded']) + \\\n","                        (row['max_limit'] - row['hollow_top'])\n","        optimal_towers.append({\n","            'revenue': rev,\n","            'base_loss_ratio': base_lr,\n","            'loss_ratio_inflation_factor': inf_lr,\n","            'initial_assets': row['initial_assets'],\n","            'optimal_ded': row['ded'],\n","            'optimal_hollow_bottom': row['hollow_bottom'],\n","            'optimal_hollow_top': row['hollow_top'],\n","            'optimal_max_limit': row['max_limit'],\n","            'growth_rate': g_val,\n","            'growth_vol': v_val,\n","            'sharpe_ratio': sharpe_val,\n","            'ruin_prob': row['ruin_prob'],\n","            'premium': row['premium'],\n","            'covered_width': covered_width,\n","            'hollow_width': row['hollow_top'] - row['hollow_bottom'],\n","        })\n","\n","    df_optimal = pd.DataFrame(optimal_towers)\n","\n","    print(f\"\\nOptimal hollow towers: {len(df_optimal)} combos \"\n","          f\"({len(revenue_grid)} revenues x {len(base_loss_ratio_grid)} LRs \"\n","          f\"x {len(inflection_factor_grid)} inflections)\")\n","    print(df_optimal[['revenue', 'base_loss_ratio', 'loss_ratio_inflation_factor',\n","                       'optimal_ded', 'optimal_hollow_bottom', 'optimal_hollow_top',\n","                       'optimal_max_limit', 'sharpe_ratio', 'ruin_prob']].head(20).to_string())\n","\n","    # --- Cache Results ---\n","    try:\n","        df_tower.to_parquet(records_base + '.parquet', compression='zstd')\n","        sz = os.path.getsize(records_base + '.parquet') / 1024\n","        print(f\"\\nSaved: {records_base}.parquet ({len(df_tower):,} rows, {sz:.0f} KB)\")\n","    except Exception as e:\n","        print(f\"Parquet save failed: {e}\")\n","\n","    df_tower.to_pickle(records_base + '.pkl')\n","    print(f\"Saved: {records_base}.pkl ({len(df_tower):,} rows)\")\n","\n","    try:\n","        df_optimal.to_parquet(optimal_base + '.parquet', compression='zstd')\n","        print(f\"Saved: {optimal_base}.parquet ({len(df_optimal)} rows)\")\n","    except Exception as e:\n","        print(f\"Parquet save failed: {e}\")\n","\n","    df_optimal.to_pickle(optimal_base + '.pkl')\n","    print(f\"Saved: {optimal_base}.pkl ({len(df_optimal)} rows)\")\n","\n","    print(f\"\\nCache directory: {os.path.abspath(CACHE_DIR)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b4663a54"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"g76F0o86DRpI"},"source":["### Hollow Tower Demand Curve Visualization (3x3 Facet Plot)\n","\n","Each facet shows how the **optimal hollow tower** varies with the base loss ratio (x-axis):\n","\n","- **Columns** = starting revenue (\\$5M, \\$25M, \\$50M) — revenue increases left → right\n","- **Rows** = excess-layer inflection factor (1×, 2×, 3×) — excess pricing increases bottom → top\n","- **Blue line** = optimal max limit (top of tower)\n","- **Orange line** = optimal deductible (retention / bottom of tower)\n","- **Gray lines + shading** = hollow section (gap in coverage — losses here are fully retained)\n","\n","The hollow section represents a deliberate gap where the company self-insures,\n","buying primary coverage below and excess coverage above.  As the base loss ratio\n","rises (insurance becomes cheaper), the optimal hollow section should narrow or shift."]},{"cell_type":"code","source":["df_optimal_old = df_optimal.copy(deep=True)\n","\n","for idx, row in df_optimal.iterrows():\n","  if row['optimal_hollow_bottom'] <= row['optimal_ded']:\n","    df_optimal.at[idx, 'optimal_hollow_bottom'] = row['optimal_hollow_top']\n","    df_optimal.at[idx, 'optimal_ded'] = row['optimal_hollow_top']\n","    # print(row)\n","  if row['optimal_hollow_top'] >= row['optimal_max_limit']:\n","    df_optimal.at[idx, 'optimal_hollow_top'] = row['optimal_hollow_bottom']\n","    df_optimal.at[idx, 'optimal_max_limit'] = row['optimal_hollow_bottom']\n","    # print(row)"],"metadata":{"id":"rFcardbv8Hcc"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DfSw5ES-DRpI"},"outputs":[],"source":["from scipy.interpolate import PchipInterpolator\n","import matplotlib.ticker as mticker\n","\n","def dollar_fmt(x, pos):\n","    if x >= 1e9:\n","        return f'${x/1e9:.0f}B'\n","    elif x >= 1e6:\n","        return f'${x/1e6:.0f}M'\n","    elif x >= 1e3:\n","        return f'${x/1e3:.0f}K'\n","    return f'${x:.0f}'\n","\n","def tick_dollar_fmt(x):\n","    if x >= 1e6:\n","        return f'${x/1e6:.0f}M'\n","    elif x >= 1e3:\n","        return f'${x/1e3:.0f}K'\n","    return f'${x:.0f}'\n","\n","# Sort grids: columns = revenue ascending, rows = inflection descending\n","rev_sorted = sorted(revenue_grid)\n","inf_sorted = sorted(inflection_factor_grid, reverse=True)\n","\n","n_rows = len(inf_sorted)\n","n_cols = len(rev_sorted)\n","\n","fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 3 * n_rows),\n","                         dpi=150, sharex=True, sharey=False)\n","\n","# Ensure axes is always 2D\n","if n_rows == 1 and n_cols == 1:\n","    axes = np.array([[axes]])\n","elif n_rows == 1:\n","    axes = axes[np.newaxis, :]\n","elif n_cols == 1:\n","    axes = axes[:, np.newaxis]\n","\n","# Common y-axis bounds across all facets\n","all_ded = df_optimal['optimal_ded'].values\n","all_lim = df_optimal['optimal_max_limit'].values\n","pos_ded = all_ded[all_ded > 0]\n","y_min = max(pos_ded.min() * 0.5, 1_000) if len(pos_ded) > 0 else 1_000\n","y_max = all_lim.max() * 2.0\n","\n","y_ticks = [100_000, 1_000_000, 10_000_000, 100_000_000, 500_000_000]\n","\n","for row_idx, inf_f in enumerate(inf_sorted):\n","    for col_idx, rev in enumerate(rev_sorted):\n","        ax = axes[row_idx, col_idx]\n","\n","        # Filter data for this facet\n","        mask = (np.isclose(df_optimal['loss_ratio_inflation_factor'], inf_f)) & \\\n","               (df_optimal['revenue'] == rev)\n","        df_f = df_optimal[mask].sort_values('base_loss_ratio')\n","\n","        if len(df_f) < 2:\n","            ax.text(0.5, 0.5, 'Insufficient data', transform=ax.transAxes,\n","                    ha='center', va='center', fontsize=9, color='gray')\n","            continue\n","\n","        x = df_f['base_loss_ratio'].values\n","        y_ded = np.maximum(df_f['optimal_ded'].values, 1_000)  # floor for log scale\n","        y_hb  = np.maximum(df_f['optimal_hollow_bottom'].values, 1_000)\n","        y_ht  = df_f['optimal_hollow_top'].values\n","        y_lim = df_f['optimal_max_limit'].values\n","\n","        # PCHIP interpolation in log-space for smooth curves\n","        x_fine = np.linspace(x.min(), x.max(), 200)\n","\n","        if len(x) >= 4:\n","            ded_interp = PchipInterpolator(x, np.log10(y_ded))\n","            hb_interp  = PchipInterpolator(x, np.log10(y_hb))\n","            ht_interp  = PchipInterpolator(x, np.log10(y_ht))\n","            lim_interp = PchipInterpolator(x, np.log10(y_lim))\n","            ded_smooth = 10 ** ded_interp(x_fine)\n","            hb_smooth  = 10 ** hb_interp(x_fine)\n","            ht_smooth  = 10 ** ht_interp(x_fine)\n","            lim_smooth = 10 ** lim_interp(x_fine)\n","        else:\n","            ded_smooth = 10 ** np.interp(x_fine, x, np.log10(y_ded))\n","            hb_smooth  = 10 ** np.interp(x_fine, x, np.log10(y_hb))\n","            ht_smooth  = 10 ** np.interp(x_fine, x, np.log10(y_ht))\n","            lim_smooth = 10 ** np.interp(x_fine, x, np.log10(y_lim))\n","\n","        # Hollow section — gray lines with shaded gap\n","        ax.plot(x_fine, hb_smooth, color='gray', linewidth=1.5, linestyle='--')\n","        ax.plot(x_fine, ht_smooth, color='gray', linewidth=1.5, linestyle='--')\n","        ax.fill_between(x_fine, hb_smooth, ht_smooth,\n","                        alpha=0.25, color='gray', label='Hollow (no coverage)')\n","\n","        # Plot smooth lines — max limit (blue) and deductible (orange)\n","        ax.plot(x_fine, lim_smooth, color='steelblue', linewidth=2, label='Max Limit')\n","        ax.plot(x_fine, ded_smooth, color='darkorange', linewidth=2, label='Deductible')\n","\n","        # Plot actual data points\n","        ax.scatter(x, y_lim, color='steelblue', s=20, zorder=5, alpha=0.7)\n","        ax.scatter(x, y_ded, color='darkorange', s=20, zorder=5, alpha=0.7)\n","        ax.scatter(x, y_hb, color='gray', s=12, zorder=5, alpha=0.5, marker='v')\n","        ax.scatter(x, y_ht, color='gray', s=12, zorder=5, alpha=0.5, marker='^')\n","\n","        ax.set_yscale('log')\n","        ax.set_ylim(y_min, y_max)\n","        ax.grid(True, alpha=0.3, which='both')\n","\n","        ax.yaxis.set_major_formatter(mticker.FuncFormatter(dollar_fmt))\n","        ax.set_yticks(y_ticks)\n","        ax.set_yticklabels([tick_dollar_fmt(v) for v in y_ticks])\n","\n","        for pos in y_ticks:\n","            ax.axhline(y=pos, color='black', linestyle='dashed', alpha=0.4)\n","\n","        # Column titles (top row only)\n","        if row_idx == 0:\n","            ax.set_title(f'Revenue = ${rev/1e6:.0f}M', fontsize=14, fontweight='bold')\n","\n","        # Row labels (left column only)\n","        if col_idx == 0:\n","            ax.set_ylabel(f'Inflection = {inf_f:.0f}x\\nDemanded Amount ($)', fontsize=14)\n","\n","        # X-axis label (bottom row only)\n","        if row_idx == n_rows - 1:\n","            ax.set_xlabel('Base Loss Ratio', fontsize=14)\n","\n","# Shared legend from first facet\n","handles, labels = axes[0, 0].get_legend_handles_labels()\n","if handles:\n","    fig.legend(handles, labels, loc='upper center', ncol=4, fontsize=12,\n","               bbox_to_anchor=(0.5, 0.99), frameon=True, edgecolor='black')\n","\n","fig.suptitle('Hollow Tower Demand Curves: Optimal Retention, Gap & Limit vs. Base Loss Ratio\\n'\n","             f'Sharpe ratio optimization, \\u22640.25% ruin constraint, {SENS_N_PATHS} paths',\n","             fontsize=14, fontweight='bold', y=1.03)\n","\n","fig.patch.set_facecolor('white')\n","for row in axes:\n","    for ax in row:\n","        ax.set_facecolor('white')\n","        for spine in ax.spines.values():\n","            spine.set_color('black')\n","        ax.tick_params(colors='black')\n","\n","plt.tight_layout(rect=[0, 0, 1, 0.96], h_pad=4.0, w_pad=4.0)\n","plt.savefig(os.path.join(CACHE_DIR, 'hollow_demand_cur7ve_3x3.png'),\n","            dpi=150, bbox_inches='tight', facecolor='white')\n","plt.show()\n","\n","print(f\"\\nChart saved to: {os.path.join(CACHE_DIR, 'hollow_demand_curve_3x3.png')}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LnzESIdkiQG4"},"outputs":[],"source":["from scipy.interpolate import PchipInterpolator\n","import matplotlib.ticker as mticker\n","\n","def dollar_fmt(x, pos):\n","    if x >= 1e9:\n","        return f'${x/1e9:.0f}B'\n","    elif x >= 1e6:\n","        return f'${x/1e6:.0f}M'\n","    elif x >= 1e3:\n","        return f'${x/1e3:.0f}K'\n","    return f'${x:.0f}'\n","\n","def tick_dollar_fmt(x):\n","    if x >= 1e6:\n","        return f'${x/1e6:.0f}M'\n","    elif x >= 1e3:\n","        return f'${x/1e3:.0f}K'\n","    return f'${x:.0f}'\n","\n","fig, ax = plt.subplots(figsize=(10, 6), dpi=150)\n","\n","# Common y-axis bounds\n","all_ded = df_optimal['optimal_ded'].values\n","all_lim = df_optimal['optimal_max_limit'].values\n","pos_ded = all_ded[all_ded > 0]\n","y_min = max(pos_ded.min() * 0.5, 1_000) if len(pos_ded) > 0 else 1_000\n","y_max = all_lim.max() * 2.0\n","\n","y_ticks = [100_000, 1_000_000, 10_000_000, 100_000_000, 500_000_000]\n","\n","inf_f = inf_sorted[-1]   # 1.0x (cheapest excess)\n","rev = rev_sorted[0]       # $5M (smallest revenue)\n","\n","# Filter data for this facet\n","mask = (np.isclose(df_optimal['loss_ratio_inflation_factor'], inf_f)) & \\\n","        (df_optimal['revenue'] == rev)\n","df_f = df_optimal[mask].sort_values('base_loss_ratio')\n","\n","x = df_f['base_loss_ratio'].values\n","y_ded = np.maximum(df_f['optimal_ded'].values, 1_000)  # floor for log scale\n","y_hb  = np.maximum(df_f['optimal_hollow_bottom'].values, 1_000)\n","y_ht  = df_f['optimal_hollow_top'].values\n","y_lim = df_f['optimal_max_limit'].values\n","\n","# PCHIP interpolation in log-space for smooth curves\n","x_fine = np.linspace(x.min(), x.max(), 200)\n","\n","if len(x) >= 4:\n","    ded_interp = PchipInterpolator(x, np.log10(y_ded))\n","    hb_interp  = PchipInterpolator(x, np.log10(y_hb))\n","    ht_interp  = PchipInterpolator(x, np.log10(y_ht))\n","    lim_interp = PchipInterpolator(x, np.log10(y_lim))\n","    ded_smooth = 10 ** ded_interp(x_fine)\n","    hb_smooth  = 10 ** hb_interp(x_fine)\n","    ht_smooth  = 10 ** ht_interp(x_fine)\n","    lim_smooth = 10 ** lim_interp(x_fine)\n","else:\n","    ded_smooth = 10 ** np.interp(x_fine, x, np.log10(y_ded))\n","    hb_smooth  = 10 ** np.interp(x_fine, x, np.log10(y_hb))\n","    ht_smooth  = 10 ** np.interp(x_fine, x, np.log10(y_ht))\n","    lim_smooth = 10 ** np.interp(x_fine, x, np.log10(y_lim))\n","\n","# Hollow section — gray lines with shaded gap\n","ax.plot(x_fine, hb_smooth, color='gray', linewidth=1.5, linestyle='--')\n","ax.plot(x_fine, ht_smooth, color='gray', linewidth=1.5, linestyle='--')\n","ax.fill_between(x_fine, hb_smooth, ht_smooth,\n","                alpha=0.25, color='gray', label='Hollow (no coverage)')\n","\n","# Plot smooth lines — max limit (blue) and deductible (orange)\n","ax.plot(x_fine, lim_smooth, color='steelblue', linewidth=2, label='Max Limit')\n","ax.plot(x_fine, ded_smooth, color='darkorange', linewidth=2, label='Deductible')\n","\n","# Plot actual data points\n","ax.scatter(x, y_lim, color='steelblue', s=20, zorder=5, alpha=0.7)\n","ax.scatter(x, y_ded, color='darkorange', s=20, zorder=5, alpha=0.7)\n","ax.scatter(x, y_hb, color='gray', s=15, zorder=5, alpha=0.5, marker='v')\n","ax.scatter(x, y_ht, color='gray', s=15, zorder=5, alpha=0.5, marker='^')\n","\n","ax.set_yscale('log')\n","ax.set_ylim(y_min, y_max)\n","ax.grid(True, alpha=0.3, which='both')\n","\n","ax.yaxis.set_major_formatter(mticker.FuncFormatter(dollar_fmt))\n","ax.set_yticks(y_ticks)\n","ax.set_yticklabels([tick_dollar_fmt(v) for v in y_ticks])\n","\n","for pos in y_ticks:\n","    ax.axhline(y=pos, color='black', linestyle='dashed', alpha=0.4)\n","\n","ax.legend(loc='best', fontsize=12, frameon=True, edgecolor='black')\n","\n","fig.suptitle('Hollow Tower Demand Curve: Optimal Retention, Gap & Limit vs. Loss Ratio\\n'\n","             f'Inflection = {inf_f:.0f}x, revenue = ${rev/1e6:.0f}M, {SENS_N_PATHS} paths',\n","             fontsize=14, fontweight='bold', y=1.08)\n","\n","fig.patch.set_facecolor('white')\n","ax.set_facecolor('white')\n","for spine in ax.spines.values():\n","    spine.set_color('black')\n","ax.tick_params(colors='black')\n","\n","ax.set_xlabel('Loss Ratio', fontsize=14)\n","ax.set_ylabel('Demanded Amount ($)', fontsize=14)\n","\n","plt.tight_layout()\n","plt.savefig(os.path.join(CACHE_DIR, 'hollow_demand_curve_base.png'),\n","            dpi=150, bbox_inches='tight', facecolor='white')\n","plt.show()\n","\n","print(f\"\\nChart saved to: {os.path.join(CACHE_DIR, 'hollow_demand_curve_base.png')}\")\n","\n","# Summary table\n","print(f\"\\nBase case summary (inflection={inf_f:.0f}x, revenue=${rev/1e6:.0f}M):\")\n","print(f\"  {'LR':>6s}  {'Ded':>10s}  {'Hollow Bot':>12s}  {'Hollow Top':>12s}  \"\n","      f\"{'Max Limit':>12s}  {'Premium':>10s}  {'Growth':>8s}\")\n","print(f\"  {'------':>6s}  {'--------':>10s}  {'----------':>12s}  {'----------':>12s}  \"\n","      f\"{'----------':>12s}  {'--------':>10s}  {'------':>8s}\")\n","for _, row in df_f.iterrows():\n","    print(f\"  {row['base_loss_ratio']:>6.2f}\"\n","          f\"  ${row['optimal_ded']:>9,.0f}\"\n","          f\"  ${row['optimal_hollow_bottom']:>11,.0f}\"\n","          f\"  ${row['optimal_hollow_top']:>11,.0f}\"\n","          f\"  ${row['optimal_max_limit']:>11,.0f}\"\n","          f\"  ${row['premium']:>9,.0f}\"\n","          f\"  {row['growth_rate']:>+7.1%}\")"]},{"cell_type":"markdown","metadata":{"id":"9IzsE7tgosHe"},"source":["### Demand Elasticity Analysis\n","\n","The **price elasticity of demand** measures how responsive the optimal hollow tower is\n","to changes in market pricing.  We compute:\n","\n","$$E = \\frac{d(\\ln Q)}{d(\\ln \\text{LR})}$$\n","\n","where $Q$ is the demanded amount (deductible, hollow boundaries, or max limit) and LR is the base loss ratio.\n","\n","- $|E| > 1$: **Elastic** — demand responds more than proportionally to price changes\n","- $|E| < 1$: **Inelastic** — demand responds less than proportionally\n","- $E > 0$ for max limit: cheaper insurance → buy more coverage on top\n","- $E < 0$ for deductible: cheaper insurance → lower retention (buy more from the bottom)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zouvCQVhosHe"},"outputs":[],"source":["## --- Demand Elasticity Chart (Base Case) ---\n","\n","from scipy.interpolate import PchipInterpolator\n","import matplotlib.ticker as mticker\n","\n","# Re-extract base case data (same filter as cell-16)\n","inf_f = inf_sorted[-1]   # 1.0x (cheapest excess)\n","rev = rev_sorted[0]       # $5M (smallest revenue)\n","\n","mask = (np.isclose(df_optimal['loss_ratio_inflation_factor'], inf_f)) & \\\n","       (df_optimal['revenue'] == rev)\n","df_f = df_optimal[mask].sort_values('base_loss_ratio')\n","\n","x = df_f['base_loss_ratio'].values\n","y_ded = np.maximum(df_f['optimal_ded'].values, 1_000)\n","y_hb  = np.maximum(df_f['optimal_hollow_bottom'].values, 1_000)\n","y_ht  = df_f['optimal_hollow_top'].values\n","y_lim = df_f['optimal_max_limit'].values\n","\n","# PCHIP in log-space\n","ded_interp = PchipInterpolator(x, np.log10(y_ded))\n","hb_interp  = PchipInterpolator(x, np.log10(y_hb))\n","ht_interp  = PchipInterpolator(x, np.log10(y_ht))\n","lim_interp = PchipInterpolator(x, np.log10(y_lim))\n","\n","x_fine = np.linspace(x.min(), x.max(), 200)\n","\n","# Elasticity: E = LR * ln(10) * d(log10 Q) / d(LR)\n","ded_d1 = ded_interp.derivative()(x_fine)\n","hb_d1  = hb_interp.derivative()(x_fine)\n","ht_d1  = ht_interp.derivative()(x_fine)\n","lim_d1 = lim_interp.derivative()(x_fine)\n","\n","ded_elast = x_fine * np.log(10) * ded_d1\n","hb_elast  = x_fine * np.log(10) * hb_d1\n","ht_elast  = x_fine * np.log(10) * ht_d1\n","lim_elast = x_fine * np.log(10) * lim_d1\n","\n","# --- Plot ---\n","fig, ax = plt.subplots(figsize=(10, 6), dpi=150)\n","\n","ax.plot(x_fine, lim_elast, color='#2ca02c', linewidth=2.5, linestyle='-',\n","        label='Max Limit elasticity')\n","ax.plot(x_fine, ded_elast, color='#d62728', linewidth=2.5, linestyle='--',\n","        label='Deductible elasticity')\n","ax.plot(x_fine, hb_elast, color='gray', linewidth=1.5, linestyle='-.',\n","        label='Hollow Bottom elasticity', alpha=0.7)\n","ax.plot(x_fine, ht_elast, color='dimgray', linewidth=1.5, linestyle=':',\n","        label='Hollow Top elasticity', alpha=0.7)\n","\n","# Reference lines\n","ax.axhline(y=0, color='black', linewidth=1, alpha=0.8)\n","ax.axhline(y=1, color='gray', linewidth=1, linestyle=':', alpha=0.6,\n","           label='Unit elasticity ($|E|=1$)')\n","ax.axhline(y=-1, color='gray', linewidth=1, linestyle=':', alpha=0.6)\n","\n","# Shade elastic vs inelastic regions\n","ax.axhspan(-1, 1, color='lightyellow', alpha=0.3, zorder=0)\n","ax.text(x.min() + 0.01, 0.5, 'Inelastic\\n$|E| < 1$',\n","        fontsize=10, color='gray', alpha=0.7, va='center')\n","\n","ax.set_xlabel('Base Loss Ratio', fontsize=14)\n","ax.set_ylabel('Price Elasticity of Demand  $E$', fontsize=14)\n","ax.legend(loc='best', fontsize=10, frameon=True, edgecolor='black')\n","ax.grid(True, alpha=0.3)\n","\n","fig.suptitle('Hollow Tower Demand Elasticity: How Responsive Is the Optimal Tower to Price?\\n'\n","             f'Base case: inflection = {inf_f:.0f}x, revenue = ${rev/1e6:.0f}M, '\n","             f'{SENS_N_PATHS:,} paths',\n","             fontsize=13, fontweight='bold', y=1.06)\n","\n","fig.patch.set_facecolor('white')\n","ax.set_facecolor('white')\n","for spine in ax.spines.values():\n","    spine.set_color('black')\n","ax.tick_params(colors='black')\n","\n","plt.tight_layout()\n","plt.savefig(os.path.join(CACHE_DIR, 'hollow_demand_elasticity_base.png'),\n","            dpi=150, bbox_inches='tight', facecolor='white')\n","plt.show()\n","print(f\"\\nChart saved to: {os.path.join(CACHE_DIR, 'hollow_demand_elasticity_base.png')}\")\n","\n","# Print elasticity at key loss ratios\n","print(\"\\nElasticity at key loss ratios:\")\n","print(f\"  {'LR':>6s}  {'E(Limit)':>10s}  {'E(Ded)':>10s}  {'E(HB)':>10s}  {'E(HT)':>10s}  {'Interpretation'}\")\n","print(f\"  {'------':>6s}  {'--------':>10s}  {'------':>10s}  {'------':>10s}  {'------':>10s}  {'-' * 20}\")\n","for lr_check in [0.45, 0.55, 0.65, 0.70, 0.80]:\n","    idx = np.argmin(np.abs(x_fine - lr_check))\n","    e_lim = lim_elast[idx]\n","    e_ded = ded_elast[idx]\n","    e_hb  = hb_elast[idx]\n","    e_ht  = ht_elast[idx]\n","    any_elastic = abs(e_lim) > 1 or abs(e_ded) > 1 or abs(e_hb) > 1 or abs(e_ht) > 1\n","    interp = \"Elastic\" if any_elastic else \"Inelastic\"\n","    print(f\"  {lr_check:>6.2f}  {e_lim:>+10.2f}  {e_ded:>+10.2f}  {e_hb:>+10.2f}  {e_ht:>+10.2f}  {interp}\")"]},{"cell_type":"markdown","metadata":{"id":"zqcUbPuyosHf"},"source":["### Hollow Tower Coverage Bands & Premium Efficiency\n","\n","Two additional views of the base case hollow tower demand curve:\n","\n","1. **Coverage Bands** — Two shaded regions show the lower and upper coverage bands,\n","   with the hollow gap visible between them.  The covered width (sum of both bands)\n","   as a fraction of revenue reveals how much exposure is transferred.\n","\n","2. **Premium vs. Growth Rate** — Shows the cost-benefit tradeoff: how much premium\n","   the company pays at each loss ratio, and what ergodic growth rate it achieves.\n","   The hollow tower should achieve comparable growth at lower premium by dropping\n","   coverage in the mid-layer where premium-per-unit-of-protection is least efficient."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KcQr-c6sosHf"},"outputs":[],"source":["## --- Chart: Hollow Tower Coverage Bands + Premium vs Growth (base case) ---\n","\n","from scipy.interpolate import PchipInterpolator\n","import matplotlib.ticker as mticker\n","\n","def dollar_fmt(x, pos):\n","    if x >= 1e9:  return f'${x/1e9:.0f}B'\n","    elif x >= 1e6: return f'${x/1e6:.0f}M'\n","    elif x >= 1e3: return f'${x/1e3:.0f}K'\n","    return f'${x:.0f}'\n","\n","# Base case filter\n","inf_f = inf_sorted[-1]   # 1.0x\n","rev = rev_sorted[0]       # $5M\n","\n","mask = (np.isclose(df_optimal['loss_ratio_inflation_factor'], inf_f)) & \\\n","       (df_optimal['revenue'] == rev)\n","df_f = df_optimal[mask].sort_values('base_loss_ratio')\n","\n","x = df_f['base_loss_ratio'].values\n","y_ded = np.maximum(df_f['optimal_ded'].values, 1_000)\n","y_hb  = np.maximum(df_f['optimal_hollow_bottom'].values, 1_000)\n","y_ht  = df_f['optimal_hollow_top'].values\n","y_lim = df_f['optimal_max_limit'].values\n","y_prem = df_f['premium'].values\n","y_growth = df_f['growth_rate'].values\n","y_covered = df_f['covered_width'].values\n","y_hollow = df_f['hollow_width'].values\n","\n","# PCHIP interpolants\n","x_fine = np.linspace(x.min(), x.max(), 200)\n","ded_interp = PchipInterpolator(x, np.log10(y_ded))\n","hb_interp  = PchipInterpolator(x, np.log10(y_hb))\n","ht_interp  = PchipInterpolator(x, np.log10(y_ht))\n","lim_interp = PchipInterpolator(x, np.log10(y_lim))\n","ded_smooth = 10 ** ded_interp(x_fine)\n","hb_smooth  = 10 ** hb_interp(x_fine)\n","ht_smooth  = 10 ** ht_interp(x_fine)\n","lim_smooth = 10 ** lim_interp(x_fine)\n","\n","prem_interp = PchipInterpolator(x, y_prem)\n","prem_smooth = prem_interp(x_fine)\n","\n","growth_interp = PchipInterpolator(x, y_growth)\n","growth_smooth = growth_interp(x_fine)\n","\n","covered_interp = PchipInterpolator(x, np.log10(np.maximum(y_covered, 1)))\n","covered_smooth = 10 ** covered_interp(x_fine)\n","\n","# ---- Figure with 2 subplots stacked ----\n","fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10), dpi=150,\n","                                gridspec_kw={'height_ratios': [1, 1]})\n","\n","# --- Panel 1: Coverage Bands with Hollow Gap ---\n","# Lower coverage band (ded to hollow_bottom)\n","ax1.fill_between(x_fine, ded_smooth, hb_smooth,\n","                 alpha=0.2, color='steelblue', label='Lower coverage band')\n","# Upper coverage band (hollow_top to max_limit)\n","ax1.fill_between(x_fine, ht_smooth, lim_smooth,\n","                 alpha=0.2, color='darkorange', label='Upper coverage band')\n","# Hollow gap\n","ax1.fill_between(x_fine, hb_smooth, ht_smooth,\n","                 alpha=0.15, color='gray', hatch='///', label='Hollow (no coverage)')\n","\n","ax1.plot(x_fine, lim_smooth, color='steelblue', linewidth=2, label='Max Limit')\n","ax1.plot(x_fine, ded_smooth, color='darkorange', linewidth=2, label='Deductible')\n","ax1.plot(x_fine, hb_smooth, color='gray', linewidth=1.5, linestyle='--')\n","ax1.plot(x_fine, ht_smooth, color='gray', linewidth=1.5, linestyle='--')\n","\n","# Covered width on secondary axis\n","ax1b = ax1.twinx()\n","ax1b.plot(x_fine, covered_smooth, color='#9467bd', linewidth=2, linestyle='-.',\n","          label='Covered Width', alpha=0.85)\n","ax1b.set_ylabel('Covered Width ($)', fontsize=12, color='#9467bd')\n","ax1b.yaxis.set_major_formatter(mticker.FuncFormatter(dollar_fmt))\n","ax1b.tick_params(axis='y', colors='#9467bd')\n","ax1b.set_yscale('log')\n","\n","ax1.set_yscale('log')\n","y_ticks = [100_000, 1_000_000, 10_000_000, 100_000_000, 500_000_000]\n","ax1.set_yticks(y_ticks)\n","ax1.yaxis.set_major_formatter(mticker.FuncFormatter(dollar_fmt))\n","ax1.set_ylabel('Optimal Tower Bounds ($)', fontsize=12)\n","ax1.set_xlabel('Base Loss Ratio', fontsize=12)\n","ax1.grid(True, alpha=0.3, which='both')\n","\n","# Combined legend\n","lines1, labels1 = ax1.get_legend_handles_labels()\n","lines2, labels2 = ax1b.get_legend_handles_labels()\n","ax1.legend(lines1 + lines2, labels1 + labels2,\n","           loc='upper left', fontsize=9, frameon=True, edgecolor='black')\n","\n","ax1.set_title('Hollow Tower Coverage Bands',\n","              fontsize=13, fontweight='bold')\n","\n","# --- Panel 2: Premium vs Growth Rate ---\n","ax2.plot(x_fine, prem_smooth, color='#8c564b', linewidth=2.5, linestyle='-',\n","         label='Annual Premium')\n","ax2.scatter(x, y_prem, color='#8c564b', s=25, zorder=5, alpha=0.7)\n","ax2.set_ylabel('Annual Premium ($)', fontsize=12, color='#8c564b')\n","ax2.yaxis.set_major_formatter(mticker.FuncFormatter(dollar_fmt))\n","ax2.tick_params(axis='y', colors='#8c564b')\n","\n","# Growth rate on secondary axis\n","ax2b = ax2.twinx()\n","ax2b.plot(x_fine, growth_smooth * 100, color='#2ca02c', linewidth=2.5,\n","          linestyle='--', label='Ergodic Growth Rate')\n","ax2b.scatter(x, y_growth * 100, color='#2ca02c', s=25, zorder=5, alpha=0.7)\n","ax2b.set_ylabel('Ergodic Growth Rate (%)', fontsize=12, color='#2ca02c')\n","ax2b.tick_params(axis='y', colors='#2ca02c')\n","ax2b.axhline(y=0, color='#2ca02c', linewidth=0.8, linestyle=':', alpha=0.5)\n","\n","ax2.set_xlabel('Base Loss Ratio', fontsize=12)\n","ax2.grid(True, alpha=0.3)\n","\n","# Combined legend\n","lines1, labels1 = ax2.get_legend_handles_labels()\n","lines2, labels2 = ax2b.get_legend_handles_labels()\n","ax2.legend(lines1 + lines2, labels1 + labels2,\n","           loc='best', fontsize=10, frameon=True, edgecolor='black')\n","\n","ax2.set_title('Premium Cost vs. Ergodic Growth Rate',\n","              fontsize=13, fontweight='bold')\n","\n","# Style both panels\n","for ax in [ax1, ax2]:\n","    ax.set_facecolor('white')\n","    for spine in ax.spines.values():\n","        spine.set_color('black')\n","    ax.tick_params(colors='black')\n","for ax in [ax1b, ax2b]:\n","    for spine in ax.spines.values():\n","        spine.set_color('black')\n","\n","fig.patch.set_facecolor('white')\n","fig.suptitle('Hollow Tower Structure & Economic Impact vs. Market Pricing\\n'\n","             f'Base case: inflection = {inf_f:.0f}x, revenue = ${rev/1e6:.0f}M, '\n","             f'{SENS_N_PATHS:,} paths',\n","             fontsize=14, fontweight='bold', y=1.02)\n","\n","plt.tight_layout()\n","plt.savefig(os.path.join(CACHE_DIR, 'hollow_coverage_band_and_premium.png'),\n","            dpi=150, bbox_inches='tight', facecolor='white')\n","plt.show()\n","print(f\"\\nChart saved to: {os.path.join(CACHE_DIR, 'hollow_coverage_band_and_premium.png')}\")\n","\n","# Summary statistics\n","print(f\"\\nBase case summary:\")\n","print(f\"  {'LR':>6s}  {'Ded':>10s}  {'H.Bot':>10s}  {'H.Top':>10s}  \"\n","      f\"{'Max Lim':>12s}  {'Covered':>12s}  {'Premium':>10s}  {'Growth':>8s}\")\n","print(f\"  {'------':>6s}  {'--------':>10s}  {'--------':>10s}  {'--------':>10s}  \"\n","      f\"{'----------':>12s}  {'----------':>12s}  {'--------':>10s}  {'------':>8s}\")\n","for _, row in df_f.iterrows():\n","    print(f\"  {row['base_loss_ratio']:>6.2f}\"\n","          f\"  ${row['optimal_ded']:>9,.0f}\"\n","          f\"  ${row['optimal_hollow_bottom']:>9,.0f}\"\n","          f\"  ${row['optimal_hollow_top']:>9,.0f}\"\n","          f\"  ${row['optimal_max_limit']:>11,.0f}\"\n","          f\"  ${row['covered_width']:>11,.0f}\"\n","          f\"  ${row['premium']:>9,.0f}\"\n","          f\"  {row['growth_rate']:>+7.1%}\")"]}],"metadata":{"accelerator":"TPU","colab":{"gpuType":"V6E1","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"}},"nbformat":4,"nbformat_minor":0}
