{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-header",
   "metadata": {},
   "source": [
    "# Reconciliation #08: Monte Carlo Convergence\n",
    "\n",
    "## Overview\n",
    "- **What this notebook tests:** Monte Carlo estimates (ruin probability, mean final wealth, growth rates) converge to stable values as sample size increases, and convergence diagnostics (MCSE, ESS) correctly indicate when enough simulations have been run.\n",
    "- **Prerequisites:** `pip install -e .` from the project root.\n",
    "- **Estimated runtime:** < 60 seconds\n",
    "- **Audience:** Developers, actuaries, and risk managers verifying Monte Carlo convergence behaviour.\n",
    "\n",
    "## Checks Performed\n",
    "1. **MCSE Monotonicity:** Monte Carlo Standard Error decreases monotonically as sample size increases.\n",
    "2. **MCSE Scaling:** MCSE decreases approximately as 1/sqrt(n).\n",
    "3. **Estimate Stability:** Final estimates at n=1000 and n=2000 agree within reasonable tolerance.\n",
    "4. **ESS Growth:** Effective Sample Size grows with sample size.\n",
    "5. **Coefficient of Variation Decrease:** CV of running estimates decreases with more samples.\n",
    "\n",
    "## Approach\n",
    "We run individual `Simulation` instances at increasing sample sizes (50, 100, 200, 500, 1000, 2000) with different seeds, collecting key metrics (ruin probability, mean final wealth, simple growth measure) from each. At each sample size we compute convergence diagnostics using `ConvergenceDiagnostics` and verify that the estimates stabilize."
   ]
  },
  {
   "cell_type": "code",
   "id": "s6nhsep8jgl",
   "source": "\"\"\"Google Colab setup: mount Drive and install package dependencies.\n\nRun this cell first. If prompted to restart the runtime, do so, then re-run all cells.\nThis cell is a no-op when running locally.\n\"\"\"\nimport sys, os\nif 'google.colab' in sys.modules:\n    from google.colab import drive\n    drive.mount('/content/drive')\n\n    NOTEBOOK_DIR = '/content/drive/My Drive/Colab Notebooks/ei_notebooks/reconciliation'\n\n    if os.path.exists(os.path.join(NOTEBOOK_DIR, '_reconciliation_helpers.py')):\n        print(f\"Found helper module in {NOTEBOOK_DIR}\")\n        os.chdir(NOTEBOOK_DIR)\n        if NOTEBOOK_DIR not in sys.path:\n            sys.path.append(NOTEBOOK_DIR)\n    else:\n        print(f\"WARNING: _reconciliation_helpers.py not found in {NOTEBOOK_DIR}\")\n        print(\"Please update NOTEBOOK_DIR in this cell to the correct folder path.\")\n\n    !pip install git+https://github.com/AlexFiliakov/Ergodic-Insurance-Limits.git -q 2>&1 | tail -3\n    print('\\nSetup complete. If you see numpy/scipy import errors below,')\n    print('restart the runtime (Runtime > Restart runtime) and re-run all cells.')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Setup: imports and notebook header.\"\"\"\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Reconciliation helper utilities\n",
    "sys.path.insert(0, os.path.dirname(os.path.abspath(\".\")))\n",
    "from _reconciliation_helpers import (\n",
    "    ReconciliationChecker, final_summary, section_header,\n",
    "    notebook_header, timed_cell, fmt_dollar, display_df,\n",
    "    create_standard_manufacturer, create_standard_loss_generator,\n",
    ")\n",
    "\n",
    "# Core framework imports\n",
    "from ergodic_insurance.simulation import Simulation, SimulationResults\n",
    "from ergodic_insurance.convergence import ConvergenceDiagnostics\n",
    "from ergodic_insurance.manufacturer import WidgetManufacturer\n",
    "from ergodic_insurance.loss_distributions import ManufacturingLossGenerator\n",
    "\n",
    "# Display the standard notebook header\n",
    "notebook_header(\n",
    "    number=8,\n",
    "    title=\"Monte Carlo Convergence\",\n",
    "    description=(\n",
    "        \"Verifies that Monte Carlo estimates (ruin probability, growth rates, mean final wealth) \"\n",
    "        \"converge to stable values as sample size increases, and that convergence diagnostics \"\n",
    "        \"(MCSE, ESS) correctly indicate when enough simulations have been run.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Suppress verbose logging\n",
    "import logging\n",
    "logging.getLogger(\"ergodic_insurance\").setLevel(logging.WARNING)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
    "\n",
    "# Fixed seed for reproducibility\n",
    "BASE_SEED = 42\n",
    "np.random.seed(BASE_SEED)\n",
    "\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section1-header",
   "metadata": {},
   "source": [
    "## Section 1: Configure Base Scenario\n",
    "\n",
    "We create a standard manufacturer and loss generator configuration that will be\n",
    "used across all sample sizes. Each simulation uses a 10-year time horizon with\n",
    "different seeds to produce independent paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configure-scenario",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Configure the base scenario for convergence experiments.\"\"\"\n",
    "section_header(\"1. Configure Base Scenario\")\n",
    "\n",
    "# Simulation parameters\n",
    "TIME_HORIZON = 10  # years per simulation path\n",
    "SAMPLE_SIZES = [50, 100, 200, 500, 1000, 2000]\n",
    "MAX_SAMPLES = max(SAMPLE_SIZES)  # 2000\n",
    "\n",
    "# Manufacturer parameters (standard reconciliation defaults)\n",
    "INITIAL_ASSETS = 10_000_000\n",
    "\n",
    "print(f\"Time horizon:    {TIME_HORIZON} years per path\")\n",
    "print(f\"Sample sizes:    {SAMPLE_SIZES}\")\n",
    "print(f\"Max samples:     {MAX_SAMPLES}\")\n",
    "print(f\"Initial assets:  {fmt_dollar(INITIAL_ASSETS)}\")\n",
    "print(f\"Base seed:       {BASE_SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section2-header",
   "metadata": {},
   "source": [
    "## Section 2: Run Convergence Experiment\n",
    "\n",
    "We run the maximum number of simulations (2000) in a single pass, collecting\n",
    "key metrics from each path. We then compute statistics at each sample size\n",
    "by using the first N results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-experiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Run 2000 independent simulation paths and collect metrics.\"\"\"\n",
    "section_header(\"2. Run Convergence Experiment\")\n",
    "\n",
    "with timed_cell(\"Monte Carlo simulation (2000 paths)\"):\n",
    "    # Storage for per-path metrics\n",
    "    final_wealth = np.zeros(MAX_SAMPLES)\n",
    "    survived = np.zeros(MAX_SAMPLES, dtype=bool)\n",
    "    growth_rates = np.zeros(MAX_SAMPLES)\n",
    "\n",
    "    for i in range(MAX_SAMPLES):\n",
    "        # Create fresh manufacturer and loss generator with unique seed\n",
    "        seed_i = BASE_SEED + i\n",
    "        mfr = create_standard_manufacturer(initial_assets=INITIAL_ASSETS)\n",
    "        loss_gen = create_standard_loss_generator(seed=seed_i)\n",
    "\n",
    "        # Run simulation\n",
    "        sim = Simulation(\n",
    "            manufacturer=mfr,\n",
    "            loss_generator=loss_gen,\n",
    "            time_horizon=TIME_HORIZON,\n",
    "            seed=seed_i,\n",
    "            copy=False,  # We already created a fresh manufacturer\n",
    "        )\n",
    "        result = sim.run()\n",
    "\n",
    "        # Collect metrics\n",
    "        fw = float(result.equity[-1])\n",
    "        final_wealth[i] = fw\n",
    "        survived[i] = result.insolvency_year is None\n",
    "\n",
    "        # Compute simple annualized growth rate\n",
    "        if fw > 0 and result.insolvency_year is None:\n",
    "            growth_rates[i] = (fw / INITIAL_ASSETS) ** (1.0 / TIME_HORIZON) - 1.0\n",
    "        else:\n",
    "            growth_rates[i] = -1.0  # total loss\n",
    "\n",
    "print(f\"\\nCompleted {MAX_SAMPLES} simulation paths.\")\n",
    "print(f\"Survival rate:     {np.mean(survived):.1%}\")\n",
    "print(f\"Mean final wealth: {fmt_dollar(np.mean(final_wealth))}\")\n",
    "print(f\"Median growth:     {np.median(growth_rates[survived]):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compute-running-stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Compute running estimates and diagnostics at each sample size.\"\"\"\n",
    "\n",
    "# Initialize ConvergenceDiagnostics with relaxed thresholds for small samples\n",
    "diag = ConvergenceDiagnostics(\n",
    "    r_hat_threshold=1.1,\n",
    "    min_ess=100,  # Relaxed for our small sample sizes\n",
    "    relative_mcse_threshold=0.10,\n",
    ")\n",
    "\n",
    "# Storage for metrics at each sample size\n",
    "results_table = []\n",
    "\n",
    "for n in SAMPLE_SIZES:\n",
    "    # Use the first n samples\n",
    "    fw_n = final_wealth[:n]\n",
    "    surv_n = survived[:n]\n",
    "    gr_n = growth_rates[:n]\n",
    "\n",
    "    # --- Key estimates ---\n",
    "    ruin_prob = 1.0 - np.mean(surv_n)\n",
    "    mean_fw = np.mean(fw_n)\n",
    "    mean_gr = np.mean(gr_n[surv_n]) if np.any(surv_n) else np.nan\n",
    "\n",
    "    # --- MCSE for each metric ---\n",
    "    # For final wealth (continuous): use ConvergenceDiagnostics\n",
    "    mcse_fw = diag.calculate_mcse(fw_n)\n",
    "    ess_fw = diag.calculate_ess(fw_n)\n",
    "\n",
    "    # For growth rate (continuous, survivors only)\n",
    "    gr_survivors = gr_n[surv_n]\n",
    "    if len(gr_survivors) > 4:\n",
    "        mcse_gr = diag.calculate_mcse(gr_survivors)\n",
    "        ess_gr = diag.calculate_ess(gr_survivors)\n",
    "    else:\n",
    "        mcse_gr = np.nan\n",
    "        ess_gr = np.nan\n",
    "\n",
    "    # For ruin probability (binary): MCSE = sqrt(p*(1-p)/n)\n",
    "    mcse_ruin = np.sqrt(ruin_prob * (1 - ruin_prob) / n) if 0 < ruin_prob < 1 else 0.0\n",
    "\n",
    "    # Coefficient of variation of the running mean (stability measure)\n",
    "    # Use rolling sub-samples to estimate CV\n",
    "    if n >= 50:\n",
    "        n_blocks = min(10, n // 10)\n",
    "        block_size = n // n_blocks\n",
    "        block_means = [np.mean(fw_n[j*block_size:(j+1)*block_size]) for j in range(n_blocks)]\n",
    "        cv_fw = np.std(block_means) / np.mean(block_means) if np.mean(block_means) > 0 else np.inf\n",
    "    else:\n",
    "        cv_fw = np.nan\n",
    "\n",
    "    results_table.append({\n",
    "        \"n\": n,\n",
    "        \"ruin_prob\": ruin_prob,\n",
    "        \"mean_final_wealth\": mean_fw,\n",
    "        \"mean_growth_rate\": mean_gr,\n",
    "        \"mcse_ruin\": mcse_ruin,\n",
    "        \"mcse_fw\": mcse_fw,\n",
    "        \"mcse_gr\": mcse_gr,\n",
    "        \"ess_fw\": ess_fw,\n",
    "        \"ess_gr\": ess_gr,\n",
    "        \"cv_fw\": cv_fw,\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results_table)\n",
    "\n",
    "# Display summary table\n",
    "df_display = df_results.copy()\n",
    "df_display[\"ruin_prob\"] = df_display[\"ruin_prob\"].map(lambda x: f\"{x:.3%}\")\n",
    "df_display[\"mean_final_wealth\"] = df_display[\"mean_final_wealth\"].map(lambda x: f\"${x:,.0f}\")\n",
    "df_display[\"mean_growth_rate\"] = df_display[\"mean_growth_rate\"].map(\n",
    "    lambda x: f\"{x:.3%}\" if not np.isnan(x) else \"N/A\"\n",
    ")\n",
    "df_display[\"mcse_ruin\"] = df_display[\"mcse_ruin\"].map(lambda x: f\"{x:.4f}\")\n",
    "df_display[\"mcse_fw\"] = df_display[\"mcse_fw\"].map(lambda x: f\"${x:,.0f}\")\n",
    "df_display[\"mcse_gr\"] = df_display[\"mcse_gr\"].map(\n",
    "    lambda x: f\"{x:.5f}\" if not np.isnan(x) else \"N/A\"\n",
    ")\n",
    "df_display[\"ess_fw\"] = df_display[\"ess_fw\"].map(lambda x: f\"{x:.0f}\")\n",
    "df_display[\"ess_gr\"] = df_display[\"ess_gr\"].map(\n",
    "    lambda x: f\"{x:.0f}\" if not np.isnan(x) else \"N/A\"\n",
    ")\n",
    "df_display[\"cv_fw\"] = df_display[\"cv_fw\"].map(\n",
    "    lambda x: f\"{x:.4f}\" if not np.isnan(x) else \"N/A\"\n",
    ")\n",
    "\n",
    "display_df(df_display, title=\"Convergence Metrics by Sample Size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section3-header",
   "metadata": {},
   "source": [
    "## Section 3: MCSE Analysis\n",
    "\n",
    "Verify that Monte Carlo Standard Error (MCSE) decreases monotonically with sample\n",
    "size and approximately follows the theoretical 1/sqrt(n) scaling. A monotonically\n",
    "decreasing MCSE is the fundamental indicator that additional samples improve precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mcse-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"MCSE monotonicity and scaling checks.\"\"\"\n",
    "section_header(\"3. MCSE Analysis\")\n",
    "\n",
    "checker_mcse = ReconciliationChecker(section=\"MCSE Analysis\")\n",
    "\n",
    "with timed_cell(\"MCSE checks\"):\n",
    "    mcse_fw_values = df_results[\"mcse_fw\"].values\n",
    "    mcse_ruin_values = df_results[\"mcse_ruin\"].values\n",
    "    sample_sizes_arr = df_results[\"n\"].values\n",
    "\n",
    "    # Check 1: MCSE for final wealth decreases monotonically\n",
    "    fw_monotonic = all(\n",
    "        mcse_fw_values[i] >= mcse_fw_values[i + 1]\n",
    "        for i in range(len(mcse_fw_values) - 1)\n",
    "    )\n",
    "    checker_mcse.check(\n",
    "        fw_monotonic,\n",
    "        message=\"MCSE(final wealth) decreases monotonically with sample size\",\n",
    "        detail=f\"Values: {[f'${v:,.0f}' for v in mcse_fw_values]}\",\n",
    "    )\n",
    "\n",
    "    # Check 2: MCSE for ruin probability decreases monotonically\n",
    "    # Filter to sample sizes where ruin occurs (non-zero MCSE)\n",
    "    nonzero_ruin = mcse_ruin_values[mcse_ruin_values > 0]\n",
    "    if len(nonzero_ruin) > 1:\n",
    "        ruin_monotonic = all(\n",
    "            nonzero_ruin[i] >= nonzero_ruin[i + 1]\n",
    "            for i in range(len(nonzero_ruin) - 1)\n",
    "        )\n",
    "    else:\n",
    "        ruin_monotonic = True  # Trivially true if zero/one non-zero\n",
    "    checker_mcse.check(\n",
    "        ruin_monotonic,\n",
    "        message=\"MCSE(ruin prob) decreases monotonically with sample size\",\n",
    "        detail=f\"Values: {[f'{v:.4f}' for v in mcse_ruin_values]}\",\n",
    "    )\n",
    "\n",
    "    # Check 3: MCSE scales approximately as 1/sqrt(n)\n",
    "    # Compare MCSE ratio between n=200 and n=2000 to theoretical sqrt(200/2000)\n",
    "    idx_200 = list(SAMPLE_SIZES).index(200)\n",
    "    idx_2000 = list(SAMPLE_SIZES).index(2000)\n",
    "    actual_ratio = mcse_fw_values[idx_2000] / mcse_fw_values[idx_200]\n",
    "    theoretical_ratio = np.sqrt(200.0 / 2000.0)  # = sqrt(0.1) ~ 0.316\n",
    "\n",
    "    # Allow 50% tolerance on the ratio (Monte Carlo has sampling variability)\n",
    "    ratio_close = abs(actual_ratio - theoretical_ratio) / theoretical_ratio < 0.50\n",
    "    checker_mcse.check(\n",
    "        ratio_close,\n",
    "        message=\"MCSE scales approximately as 1/sqrt(n)\",\n",
    "        detail=(\n",
    "            f\"Actual ratio MCSE(2000)/MCSE(200) = {actual_ratio:.3f}, \"\n",
    "            f\"theoretical = {theoretical_ratio:.3f}, \"\n",
    "            f\"relative error = {abs(actual_ratio - theoretical_ratio) / theoretical_ratio:.1%}\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Check 4: MCSE at n=2000 is at least 3x smaller than at n=50\n",
    "    idx_50 = list(SAMPLE_SIZES).index(50)\n",
    "    reduction_factor = mcse_fw_values[idx_50] / mcse_fw_values[idx_2000]\n",
    "    checker_mcse.check(\n",
    "        reduction_factor >= 3.0,\n",
    "        message=\"MCSE at n=2000 is at least 3x smaller than at n=50\",\n",
    "        detail=f\"Reduction factor: {reduction_factor:.1f}x\",\n",
    "    )\n",
    "\n",
    "checker_mcse.display_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mcse-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Convergence plots: running estimates with error bands and MCSE decay.\"\"\"\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# --- Plot 1: Running mean of final wealth ---\n",
    "ax = axes[0, 0]\n",
    "running_mean_fw = np.cumsum(final_wealth) / np.arange(1, MAX_SAMPLES + 1)\n",
    "running_std_fw = np.array([final_wealth[:i+1].std(ddof=1) if i > 0 else 0\n",
    "                           for i in range(MAX_SAMPLES)])\n",
    "se_fw = running_std_fw / np.sqrt(np.arange(1, MAX_SAMPLES + 1))\n",
    "n_range = np.arange(1, MAX_SAMPLES + 1)\n",
    "ax.plot(n_range, running_mean_fw, lw=1.2, color=\"steelblue\", label=\"Running mean\")\n",
    "ax.fill_between(n_range, running_mean_fw - 2 * se_fw, running_mean_fw + 2 * se_fw,\n",
    "                alpha=0.2, color=\"steelblue\", label=\"95% CI\")\n",
    "ax.axhline(running_mean_fw[-1], ls=\"--\", color=\"red\", alpha=0.5,\n",
    "           label=f\"Final: {fmt_dollar(running_mean_fw[-1])}\")\n",
    "ax.set_xlabel(\"Number of Simulations\")\n",
    "ax.set_ylabel(\"Mean Final Wealth ($)\")\n",
    "ax.set_title(\"Running Mean: Final Wealth\")\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# --- Plot 2: Running ruin probability ---\n",
    "ax = axes[0, 1]\n",
    "running_ruin = np.cumsum(~survived) / np.arange(1, MAX_SAMPLES + 1)\n",
    "ruin_se = np.sqrt(running_ruin * (1 - running_ruin) / np.arange(1, MAX_SAMPLES + 1))\n",
    "ax.plot(n_range, running_ruin * 100, lw=1.2, color=\"darkorange\", label=\"Running estimate\")\n",
    "ax.fill_between(n_range, (running_ruin - 2 * ruin_se) * 100,\n",
    "                (running_ruin + 2 * ruin_se) * 100,\n",
    "                alpha=0.2, color=\"darkorange\", label=\"95% CI\")\n",
    "ax.axhline(running_ruin[-1] * 100, ls=\"--\", color=\"red\", alpha=0.5,\n",
    "           label=f\"Final: {running_ruin[-1]:.2%}\")\n",
    "ax.set_xlabel(\"Number of Simulations\")\n",
    "ax.set_ylabel(\"Ruin Probability (%)\")\n",
    "ax.set_title(\"Running Estimate: Ruin Probability\")\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# --- Plot 3: MCSE decay for final wealth ---\n",
    "ax = axes[1, 0]\n",
    "ax.plot(SAMPLE_SIZES, mcse_fw_values, \"o-\", color=\"green\", lw=2, label=\"Observed MCSE\")\n",
    "# Theoretical 1/sqrt(n) curve (scaled to match n=50 value)\n",
    "theoretical_mcse = mcse_fw_values[0] * np.sqrt(SAMPLE_SIZES[0]) / np.sqrt(SAMPLE_SIZES)\n",
    "ax.plot(SAMPLE_SIZES, theoretical_mcse, \"--\", color=\"gray\",\n",
    "        lw=1.5, label=r\"Theoretical $1/\\sqrt{n}$\")\n",
    "ax.set_xlabel(\"Sample Size\")\n",
    "ax.set_ylabel(\"MCSE ($)\")\n",
    "ax.set_title(\"MCSE Decay: Final Wealth\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# --- Plot 4: Running mean of growth rate (survivors) ---\n",
    "ax = axes[1, 1]\n",
    "# Compute running mean over survivors only\n",
    "surv_gr = growth_rates[survived]\n",
    "if len(surv_gr) > 0:\n",
    "    running_mean_gr = np.cumsum(surv_gr) / np.arange(1, len(surv_gr) + 1)\n",
    "    running_std_gr = np.array([surv_gr[:i+1].std(ddof=1) if i > 0 else 0\n",
    "                               for i in range(len(surv_gr))])\n",
    "    se_gr = running_std_gr / np.sqrt(np.arange(1, len(surv_gr) + 1))\n",
    "    n_surv = np.arange(1, len(surv_gr) + 1)\n",
    "    ax.plot(n_surv, running_mean_gr * 100, lw=1.2, color=\"purple\", label=\"Running mean\")\n",
    "    ax.fill_between(n_surv, (running_mean_gr - 2 * se_gr) * 100,\n",
    "                    (running_mean_gr + 2 * se_gr) * 100,\n",
    "                    alpha=0.2, color=\"purple\", label=\"95% CI\")\n",
    "    ax.axhline(running_mean_gr[-1] * 100, ls=\"--\", color=\"red\", alpha=0.5,\n",
    "               label=f\"Final: {running_mean_gr[-1]:.2%}\")\n",
    "ax.set_xlabel(\"Number of Surviving Paths\")\n",
    "ax.set_ylabel(\"Mean Growth Rate (%)\")\n",
    "ax.set_title(\"Running Mean: Growth Rate (Survivors)\")\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"Monte Carlo Convergence Diagnostics\", fontweight=\"bold\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section4-header",
   "metadata": {},
   "source": [
    "## Section 4: ESS Analysis\n",
    "\n",
    "Verify that Effective Sample Size (ESS) grows with the nominal sample size.\n",
    "ESS accounts for autocorrelation in the sample chain. For independent Monte Carlo\n",
    "draws, ESS should be close to the nominal sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ess-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"ESS checks: verify ESS grows with sample size.\"\"\"\n",
    "section_header(\"4. ESS Analysis\")\n",
    "\n",
    "checker_ess = ReconciliationChecker(section=\"ESS Analysis\")\n",
    "\n",
    "with timed_cell(\"ESS checks\"):\n",
    "    ess_fw_values = df_results[\"ess_fw\"].values\n",
    "\n",
    "    # Check 1: ESS increases with sample size (monotonically)\n",
    "    ess_increasing = all(\n",
    "        ess_fw_values[i] <= ess_fw_values[i + 1]\n",
    "        for i in range(len(ess_fw_values) - 1)\n",
    "    )\n",
    "    checker_ess.check(\n",
    "        ess_increasing,\n",
    "        message=\"ESS(final wealth) increases monotonically with sample size\",\n",
    "        detail=f\"Values: {[f'{v:.0f}' for v in ess_fw_values]}\",\n",
    "    )\n",
    "\n",
    "    # Check 2: ESS at n=2000 is at least 500\n",
    "    ess_at_2000 = ess_fw_values[-1]\n",
    "    checker_ess.check(\n",
    "        ess_at_2000 >= 500,\n",
    "        message=\"ESS at n=2000 is at least 500 (reasonable efficiency)\",\n",
    "        detail=f\"ESS = {ess_at_2000:.0f}\",\n",
    "    )\n",
    "\n",
    "    # Check 3: ESS efficiency (ESS/n) is at least 25%\n",
    "    # For independent draws, efficiency should be near 100%, but with\n",
    "    # heavy-tailed distributions the initial-positive-sequence estimator\n",
    "    # may report lower values.\n",
    "    efficiency = ess_at_2000 / 2000.0\n",
    "    checker_ess.check(\n",
    "        efficiency >= 0.25,\n",
    "        message=\"ESS efficiency at n=2000 is at least 25%\",\n",
    "        detail=f\"Efficiency = {efficiency:.1%} (ESS={ess_at_2000:.0f}, n=2000)\",\n",
    "    )\n",
    "\n",
    "    # Check 4: ESS for growth rate (survivors) also grows\n",
    "    ess_gr_values = df_results[\"ess_gr\"].dropna().values\n",
    "    if len(ess_gr_values) > 1:\n",
    "        ess_gr_increasing = all(\n",
    "            ess_gr_values[i] <= ess_gr_values[i + 1]\n",
    "            for i in range(len(ess_gr_values) - 1)\n",
    "        )\n",
    "        checker_ess.check(\n",
    "            ess_gr_increasing,\n",
    "            message=\"ESS(growth rate) increases with sample size\",\n",
    "            detail=f\"Values: {[f'{v:.0f}' for v in ess_gr_values]}\",\n",
    "        )\n",
    "    else:\n",
    "        checker_ess.check(\n",
    "            True,\n",
    "            message=\"ESS(growth rate) increases with sample size (too few data points)\",\n",
    "            detail=\"Insufficient survivor data at multiple sample sizes\",\n",
    "        )\n",
    "\n",
    "# Display ESS summary\n",
    "ess_df = df_results[[\"n\", \"ess_fw\", \"ess_gr\"]].copy()\n",
    "ess_df[\"ess_efficiency\"] = ess_df[\"ess_fw\"] / ess_df[\"n\"]\n",
    "ess_df[\"ess_fw\"] = ess_df[\"ess_fw\"].map(lambda x: f\"{x:.0f}\")\n",
    "ess_df[\"ess_gr\"] = ess_df[\"ess_gr\"].map(lambda x: f\"{x:.0f}\" if not np.isnan(x) else \"N/A\")\n",
    "ess_df[\"ess_efficiency\"] = ess_df[\"ess_efficiency\"].map(lambda x: f\"{x:.1%}\")\n",
    "display_df(ess_df, title=\"Effective Sample Size Summary\")\n",
    "\n",
    "checker_ess.display_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section5-header",
   "metadata": {},
   "source": [
    "## Section 5: Stability Check\n",
    "\n",
    "Verify that estimates stabilize as sample size increases. Specifically:\n",
    "- Final estimates at n=1000 and n=2000 agree within reasonable tolerance.\n",
    "- Coefficient of variation of block means decreases with sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stability-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Stability checks: estimates at n=1000 and n=2000 should agree.\"\"\"\n",
    "section_header(\"5. Stability Check\")\n",
    "\n",
    "checker_stability = ReconciliationChecker(section=\"Estimate Stability\")\n",
    "\n",
    "with timed_cell(\"Stability checks\"):\n",
    "    idx_1000 = list(SAMPLE_SIZES).index(1000)\n",
    "    idx_2000 = list(SAMPLE_SIZES).index(2000)\n",
    "\n",
    "    # --- Final wealth stability ---\n",
    "    fw_1000 = df_results.iloc[idx_1000][\"mean_final_wealth\"]\n",
    "    fw_2000 = df_results.iloc[idx_2000][\"mean_final_wealth\"]\n",
    "    # Tolerance: 10% relative (reasonable for 1000 vs 2000 samples)\n",
    "    rel_diff_fw = abs(fw_2000 - fw_1000) / abs(fw_2000) if fw_2000 != 0 else np.inf\n",
    "    checker_stability.check(\n",
    "        rel_diff_fw < 0.10,\n",
    "        message=\"Mean final wealth at n=1000 and n=2000 agree within 10%\",\n",
    "        detail=f\"n=1000: {fmt_dollar(fw_1000)}, n=2000: {fmt_dollar(fw_2000)}, relative diff: {rel_diff_fw:.2%}\",\n",
    "    )\n",
    "\n",
    "    # --- Ruin probability stability ---\n",
    "    ruin_1000 = df_results.iloc[idx_1000][\"ruin_prob\"]\n",
    "    ruin_2000 = df_results.iloc[idx_2000][\"ruin_prob\"]\n",
    "    # Absolute tolerance for ruin (which can be near zero)\n",
    "    abs_diff_ruin = abs(ruin_2000 - ruin_1000)\n",
    "    checker_stability.check(\n",
    "        abs_diff_ruin < 0.05,\n",
    "        message=\"Ruin probability at n=1000 and n=2000 agree within 5pp\",\n",
    "        detail=f\"n=1000: {ruin_1000:.3%}, n=2000: {ruin_2000:.3%}, abs diff: {abs_diff_ruin:.3%}\",\n",
    "    )\n",
    "\n",
    "    # --- Growth rate stability ---\n",
    "    gr_1000 = df_results.iloc[idx_1000][\"mean_growth_rate\"]\n",
    "    gr_2000 = df_results.iloc[idx_2000][\"mean_growth_rate\"]\n",
    "    if not np.isnan(gr_1000) and not np.isnan(gr_2000):\n",
    "        abs_diff_gr = abs(gr_2000 - gr_1000)\n",
    "        checker_stability.check(\n",
    "            abs_diff_gr < 0.02,\n",
    "            message=\"Mean growth rate at n=1000 and n=2000 agree within 2pp\",\n",
    "            detail=f\"n=1000: {gr_1000:.3%}, n=2000: {gr_2000:.3%}, abs diff: {abs_diff_gr:.3%}\",\n",
    "        )\n",
    "    else:\n",
    "        checker_stability.check(\n",
    "            True,\n",
    "            message=\"Mean growth rate stability (insufficient survivor data)\",\n",
    "            detail=\"Cannot compare: one or both sample sizes have NaN growth rate\",\n",
    "        )\n",
    "\n",
    "    # --- Coefficient of variation decreases ---\n",
    "    cv_values = df_results[\"cv_fw\"].dropna().values\n",
    "    if len(cv_values) >= 2:\n",
    "        cv_first = cv_values[0]\n",
    "        cv_last = cv_values[-1]\n",
    "        checker_stability.check(\n",
    "            cv_last < cv_first,\n",
    "            message=\"CV of block means decreases from smallest to largest sample size\",\n",
    "            detail=f\"CV(n={SAMPLE_SIZES[0]}) = {cv_first:.4f}, CV(n={SAMPLE_SIZES[-1]}) = {cv_last:.4f}\",\n",
    "        )\n",
    "    else:\n",
    "        checker_stability.check(\n",
    "            True,\n",
    "            message=\"CV decrease check (insufficient data points)\",\n",
    "            detail=\"Need at least 2 non-NaN CV values\",\n",
    "        )\n",
    "\n",
    "checker_stability.display_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convergence-diagnostics",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Run full ConvergenceDiagnostics.check_convergence on the largest sample.\"\"\"\n",
    "\n",
    "print(\"Full convergence diagnostics on n=2000 sample:\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Create two independent chains from the full sample for R-hat check\n",
    "chain1 = final_wealth[:1000]\n",
    "chain2 = final_wealth[1000:2000]\n",
    "chains = np.array([chain1, chain2])\n",
    "\n",
    "# Use check_convergence which expects (n_chains, n_iterations) or (n_chains, n_iterations, n_metrics)\n",
    "# We'll pass shape (2, 1000, 2) for two metrics: final_wealth and growth_rate\n",
    "gr_chain1 = growth_rates[:1000]\n",
    "gr_chain2 = growth_rates[1000:2000]\n",
    "\n",
    "multi_chains = np.stack(\n",
    "    [np.column_stack([chain1, gr_chain1]),\n",
    "     np.column_stack([chain2, gr_chain2])],\n",
    "    axis=0,\n",
    ")  # shape: (2, 1000, 2)\n",
    "\n",
    "conv_results = diag.check_convergence(\n",
    "    multi_chains,\n",
    "    metric_names=[\"final_wealth\", \"growth_rate\"],\n",
    ")\n",
    "\n",
    "for name, stats in conv_results.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  R-hat:           {stats.r_hat:.4f}\")\n",
    "    print(f\"  ESS:             {stats.ess:.0f}\")\n",
    "    print(f\"  MCSE:            {stats.mcse:.6f}\")\n",
    "    print(f\"  Autocorrelation: {stats.autocorrelation:.4f}\")\n",
    "    print(f\"  Converged:       {stats.converged}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section6-header",
   "metadata": {},
   "source": [
    "## Section 6: Combined Summary\n",
    "\n",
    "A consolidated view of all convergence checks, providing a single dashboard\n",
    "to confirm that Monte Carlo estimates converge properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combined-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Combined summary of all convergence checks.\"\"\"\n",
    "section_header(\"6. Combined Summary\")\n",
    "\n",
    "sections = [\n",
    "    (\"MCSE Analysis\", checker_mcse),\n",
    "    (\"ESS Analysis\", checker_ess),\n",
    "    (\"Estimate Stability\", checker_stability),\n",
    "]\n",
    "\n",
    "summary_rows = []\n",
    "for name, checker in sections:\n",
    "    passed, failed = checker.summary_counts\n",
    "    total = passed + failed\n",
    "    summary_rows.append({\n",
    "        \"Check Section\": name,\n",
    "        \"Passed\": passed,\n",
    "        \"Failed\": failed,\n",
    "        \"Total\": total,\n",
    "        \"Status\": \"ALL PASS\" if failed == 0 else f\"{failed} FAILED\",\n",
    "    })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_rows)\n",
    "display_df(df_summary, title=\"Convergence Check Summary\")\n",
    "\n",
    "total_passed = sum(r[\"Passed\"] for r in summary_rows)\n",
    "total_failed = sum(r[\"Failed\"] for r in summary_rows)\n",
    "total_checks = total_passed + total_failed\n",
    "print(f\"\\nTotal: {total_passed}/{total_checks} checks passed\")\n",
    "if total_failed == 0:\n",
    "    print(\"All Monte Carlo convergence checks PASSED.\")\n",
    "else:\n",
    "    print(f\"WARNING: {total_failed} check(s) FAILED.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-header",
   "metadata": {},
   "source": [
    "## Final Result\n",
    "\n",
    "The cell below produces the final PASS/FAIL banner. If any check failed,\n",
    "it raises an `AssertionError` so that `nbconvert` or CI will catch the failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Final PASS/FAIL summary -- raises AssertionError on failure.\"\"\"\n",
    "final_summary(checker_mcse, checker_ess, checker_stability)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
