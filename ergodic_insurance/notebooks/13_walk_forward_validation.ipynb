{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walk-Forward Validation for Insurance Strategies\n",
    "\n",
    "This notebook demonstrates the walk-forward validation system for testing and comparing different insurance strategies.\n",
    "\n",
    "## Overview\n",
    "\n",
    "Walk-forward validation helps:\n",
    "1. Test strategies across multiple time windows\n",
    "2. Detect overfitting in optimization-based strategies\n",
    "3. Measure strategy consistency and robustness\n",
    "4. Rank strategies based on out-of-sample performance\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add parent directory to path\n",
    "parent_dir = Path.cwd().parent.parent\n",
    "if str(parent_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(parent_dir))\n",
    "\n",
    "# Import our modules\n",
    "from ergodic_insurance.config import Config\n",
    "from ergodic_insurance.manufacturer import WidgetManufacturer\n",
    "from ergodic_insurance.strategy_backtester import (\n",
    "    NoInsuranceStrategy,\n",
    "    ConservativeFixedStrategy,\n",
    "    AggressiveFixedStrategy,\n",
    "    OptimizedStaticStrategy,\n",
    "    AdaptiveStrategy,\n",
    "    StrategyBacktester\n",
    ")\n",
    "from ergodic_insurance.validation_metrics import (\n",
    "    MetricCalculator,\n",
    "    PerformanceTargets\n",
    ")\n",
    "from ergodic_insurance.walk_forward_validator import WalkForwardValidator\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Insurance Strategies\n",
    "\n",
    "We'll test five different strategies with varying risk profiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create strategies to test\n",
    "strategies = [\n",
    "    NoInsuranceStrategy(),\n",
    "    ConservativeFixedStrategy(\n",
    "        primary_limit=5000000,\n",
    "        excess_limit=20000000,\n",
    "        higher_limit=25000000,\n",
    "        deductible=50000\n",
    "    ),\n",
    "    AggressiveFixedStrategy(\n",
    "        primary_limit=2000000,\n",
    "        excess_limit=5000000,\n",
    "        deductible=250000\n",
    "    ),\n",
    "    OptimizedStaticStrategy(\n",
    "        target_roe=0.15,\n",
    "        max_ruin_prob=0.01\n",
    "    ),\n",
    "    AdaptiveStrategy(\n",
    "        base_deductible=100000,\n",
    "        base_primary=3000000,\n",
    "        base_excess=10000000,\n",
    "        adaptation_window=3,\n",
    "        adjustment_factor=0.2\n",
    "    )\n",
    "]\n",
    "\n",
    "# Display strategy descriptions\n",
    "for strategy in strategies:\n",
    "    print(f\"- {strategy.name}: {strategy.get_description()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set Performance Targets (Optional)\n",
    "\n",
    "Define minimum acceptable performance criteria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define performance targets\n",
    "performance_targets = PerformanceTargets(\n",
    "    min_roe=0.10,  # Minimum 10% ROE\n",
    "    max_ruin_probability=0.05,  # Maximum 5% ruin probability\n",
    "    min_sharpe_ratio=0.8,  # Minimum Sharpe ratio\n",
    "    max_drawdown=0.30,  # Maximum 30% drawdown\n",
    "    min_growth_rate=0.05  # Minimum 5% growth rate\n",
    ")\n",
    "\n",
    "print(\"Performance Targets Set:\")\n",
    "print(f\"  Min ROE: {performance_targets.min_roe:.1%}\")\n",
    "print(f\"  Max Ruin Probability: {performance_targets.max_ruin_probability:.1%}\")\n",
    "print(f\"  Min Sharpe Ratio: {performance_targets.min_sharpe_ratio:.2f}\")\n",
    "print(f\"  Max Drawdown: {performance_targets.max_drawdown:.1%}\")\n",
    "print(f\"  Min Growth Rate: {performance_targets.min_growth_rate:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure Walk-Forward Validation\n",
    "\n",
    "Set up the validation with 3-year rolling windows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create validator with 3-year windows\n",
    "validator = WalkForwardValidator(\n",
    "    window_size=3,  # 3-year windows\n",
    "    step_size=1,    # 1-year step between windows\n",
    "    test_ratio=0.3, # 30% of window for testing (approximately 1 year)\n",
    "    performance_targets=performance_targets\n",
    ")\n",
    "\n",
    "# Show window configuration\n",
    "total_years = 10\n",
    "windows = validator.generate_windows(total_years)\n",
    "\n",
    "print(f\"Validation Configuration:\")\n",
    "print(f\"  Total years: {total_years}\")\n",
    "print(f\"  Number of windows: {len(windows)}\")\n",
    "print(f\"\\nFirst 3 windows:\")\n",
    "for window in windows[:3]:\n",
    "    print(f\"  {window}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Walk-Forward Validation\n",
    "\n",
    "This will test each strategy across all windows (may take a few minutes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure manufacturer and simulation settings\n",
    "from ergodic_insurance.config import ManufacturerConfig\n",
    "\n",
    "# Create manufacturer configuration\n",
    "manufacturer_config = ManufacturerConfig(\n",
    "    initial_assets=10000000,\n",
    "    asset_turnover_ratio=1.0,\n",
    "    base_operating_margin=0.08,\n",
    "    tax_rate=0.25,\n",
    "    retention_ratio=0.95  # Retain 95% of earnings (5% capex)\n",
    ")\n",
    "\n",
    "# Create manufacturer instance\n",
    "manufacturer = WidgetManufacturer(manufacturer_config)\n",
    "\n",
    "# Run validation (simplified for demo - use more simulations in production)\n",
    "print(\"Running walk-forward validation...\")\n",
    "print(\"(This may take a few minutes)\\n\")\n",
    "\n",
    "validation_result = validator.validate_strategies(\n",
    "    strategies=strategies,\n",
    "    n_years=total_years,\n",
    "    n_simulations=100,  # Use 1000+ for production\n",
    "    manufacturer=manufacturer\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Validation complete!\")\n",
    "print(f\"Best strategy: {validation_result.best_strategy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Results\n",
    "\n",
    "### Strategy Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display strategy rankings\n",
    "print(\"Strategy Rankings (by composite score):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if not validation_result.strategy_rankings.empty:\n",
    "    rankings_display = validation_result.strategy_rankings[[\n",
    "        'strategy', 'avg_roe', 'avg_ruin_prob', 'avg_sharpe',\n",
    "        'overfitting_score', 'consistency_score', 'composite_score'\n",
    "    ]].round(4)\n",
    "    \n",
    "    # Format percentages\n",
    "    rankings_display['avg_roe'] = rankings_display['avg_roe'].apply(lambda x: f\"{x:.2%}\")\n",
    "    rankings_display['avg_ruin_prob'] = rankings_display['avg_ruin_prob'].apply(lambda x: f\"{x:.2%}\")\n",
    "    \n",
    "    print(rankings_display.to_string(index=False))\n",
    "else:\n",
    "    print(\"No rankings available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze overfitting\n",
    "print(\"Overfitting Analysis:\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "for strategy_name, score in validation_result.overfitting_analysis.items():\n",
    "    if score < 0.2:\n",
    "        status = \"✓ Low (Good)\"\n",
    "        color = 'green'\n",
    "    elif score < 0.4:\n",
    "        status = \"⚠ Moderate\"\n",
    "        color = 'orange'\n",
    "    else:\n",
    "        status = \"✗ High (Poor)\"\n",
    "        color = 'red'\n",
    "    \n",
    "    print(f\"{strategy_name:25} {score:.3f} - {status}\")\n",
    "\n",
    "# Visualize overfitting scores\n",
    "if validation_result.overfitting_analysis:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    strategies_list = list(validation_result.overfitting_analysis.keys())\n",
    "    scores = list(validation_result.overfitting_analysis.values())\n",
    "    \n",
    "    bars = ax.bar(strategies_list, scores)\n",
    "    \n",
    "    # Color based on severity\n",
    "    for bar, score in zip(bars, scores):\n",
    "        if score < 0.2:\n",
    "            bar.set_color('green')\n",
    "        elif score < 0.4:\n",
    "            bar.set_color('orange')\n",
    "        else:\n",
    "            bar.set_color('red')\n",
    "    \n",
    "    ax.set_title('Overfitting Scores by Strategy', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Strategy')\n",
    "    ax.set_ylabel('Overfitting Score')\n",
    "    ax.axhline(y=0.2, color='orange', linestyle='--', alpha=0.5, label='Moderate threshold')\n",
    "    ax.axhline(y=0.4, color='red', linestyle='--', alpha=0.5, label='High threshold')\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, max(scores) * 1.2 if scores else 1)\n",
    "    \n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Across Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot performance metrics across windows\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Collect data\n",
    "strategy_names = list(validation_result.window_results[0].strategy_performances.keys())\n",
    "windows_idx = list(range(len(validation_result.window_results)))\n",
    "\n",
    "metrics_data = {\n",
    "    'Out-Sample ROE': {s: [] for s in strategy_names},\n",
    "    'Ruin Probability': {s: [] for s in strategy_names},\n",
    "    'Sharpe Ratio': {s: [] for s in strategy_names},\n",
    "    'Growth Rate': {s: [] for s in strategy_names}\n",
    "}\n",
    "\n",
    "for window_result in validation_result.window_results:\n",
    "    for strategy in strategy_names:\n",
    "        if strategy in window_result.strategy_performances:\n",
    "            perf = window_result.strategy_performances[strategy]\n",
    "            if perf.out_sample_metrics:\n",
    "                metrics_data['Out-Sample ROE'][strategy].append(perf.out_sample_metrics.roe)\n",
    "                metrics_data['Ruin Probability'][strategy].append(perf.out_sample_metrics.ruin_probability)\n",
    "                metrics_data['Sharpe Ratio'][strategy].append(perf.out_sample_metrics.sharpe_ratio)\n",
    "                metrics_data['Growth Rate'][strategy].append(perf.out_sample_metrics.growth_rate)\n",
    "\n",
    "# Plot each metric\n",
    "for ax, (metric_name, metric_values) in zip(axes.flat, metrics_data.items()):\n",
    "    for strategy in strategy_names:\n",
    "        if metric_values[strategy]:\n",
    "            ax.plot(windows_idx[:len(metric_values[strategy])], \n",
    "                   metric_values[strategy], \n",
    "                   marker='o', \n",
    "                   label=strategy,\n",
    "                   linewidth=2)\n",
    "    \n",
    "    ax.set_title(metric_name, fontweight='bold')\n",
    "    ax.set_xlabel('Window')\n",
    "    ax.set_ylabel(metric_name)\n",
    "    ax.legend(loc='best', fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Strategy Performance Across Validation Windows', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze consistency\n",
    "print(\"Strategy Consistency Scores:\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "consistency_data = []\n",
    "for strategy_name, score in validation_result.consistency_scores.items():\n",
    "    if score > 0.8:\n",
    "        status = \"✓ High\"\n",
    "    elif score > 0.6:\n",
    "        status = \"⚠ Moderate\"\n",
    "    else:\n",
    "        status = \"✗ Low\"\n",
    "    \n",
    "    print(f\"{strategy_name:25} {score:.3f} - {status}\")\n",
    "    consistency_data.append({'Strategy': strategy_name, 'Consistency': score})\n",
    "\n",
    "# Visualize consistency\n",
    "if consistency_data:\n",
    "    df_consistency = pd.DataFrame(consistency_data)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    bars = ax.barh(df_consistency['Strategy'], df_consistency['Consistency'])\n",
    "    \n",
    "    # Color based on level\n",
    "    for bar, score in zip(bars, df_consistency['Consistency']):\n",
    "        if score > 0.8:\n",
    "            bar.set_color('green')\n",
    "        elif score > 0.6:\n",
    "            bar.set_color('orange')\n",
    "        else:\n",
    "            bar.set_color('red')\n",
    "    \n",
    "    ax.set_xlabel('Consistency Score')\n",
    "    ax.set_title('Strategy Consistency Across Windows', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.axvline(x=0.6, color='orange', linestyle='--', alpha=0.5)\n",
    "    ax.axvline(x=0.8, color='green', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance Target Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate each strategy against performance targets\n",
    "print(\"Performance Target Evaluation:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for window_result in validation_result.window_results[-1:]:  # Use last window\n",
    "    for strategy_name, performance in window_result.strategy_performances.items():\n",
    "        if performance.out_sample_metrics:\n",
    "            meets_targets, failures = performance_targets.evaluate(performance.out_sample_metrics)\n",
    "            \n",
    "            print(f\"\\n{strategy_name}:\")\n",
    "            if meets_targets:\n",
    "                print(\"  ✓ Meets all targets\")\n",
    "            else:\n",
    "                print(\"  ✗ Fails targets:\")\n",
    "                for failure in failures:\n",
    "                    print(f\"    - {failure}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Validation Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive reports\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Create temporary directory for reports\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    report_dir = Path(temp_dir)\n",
    "    \n",
    "    print(\"Generating validation reports...\")\n",
    "    report_files = validator.generate_report(\n",
    "        validation_result=validation_result,\n",
    "        output_dir=str(report_dir),\n",
    "        include_visualizations=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\nGenerated files:\")\n",
    "    for file_type, file_path in report_files.items():\n",
    "        if isinstance(file_path, Path):\n",
    "            print(f\"  - {file_type}: {file_path.name}\")\n",
    "    \n",
    "    # Read and display markdown summary\n",
    "    if 'markdown' in report_files:\n",
    "        md_content = report_files['markdown'].read_text()\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"MARKDOWN SUMMARY (first 1000 chars):\")\n",
    "        print(\"=\"*60)\n",
    "        print(md_content[:1000] + \"...\" if len(md_content) > 1000 else md_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Key Insights and Recommendations\n",
    "\n",
    "Based on the walk-forward validation results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"KEY INSIGHTS:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Best strategy\n",
    "print(f\"\\n1. RECOMMENDED STRATEGY: {validation_result.best_strategy}\")\n",
    "\n",
    "# Check for overfitting\n",
    "overfit_strategies = [s for s, score in validation_result.overfitting_analysis.items() if score > 0.4]\n",
    "if overfit_strategies:\n",
    "    print(f\"\\n2. OVERFITTING WARNING:\")\n",
    "    for s in overfit_strategies:\n",
    "        print(f\"   - {s} shows significant overfitting\")\n",
    "else:\n",
    "    print(f\"\\n2. OVERFITTING: No strategies show significant overfitting\")\n",
    "\n",
    "# Consistency leaders\n",
    "if validation_result.consistency_scores:\n",
    "    most_consistent = max(validation_result.consistency_scores.items(), key=lambda x: x[1])\n",
    "    print(f\"\\n3. MOST CONSISTENT: {most_consistent[0]} (score: {most_consistent[1]:.3f})\")\n",
    "\n",
    "# Performance vs targets\n",
    "print(f\"\\n4. PERFORMANCE TARGETS:\")\n",
    "if not validation_result.strategy_rankings.empty:\n",
    "    high_performers = validation_result.strategy_rankings[\n",
    "        validation_result.strategy_rankings['avg_roe'] >= performance_targets.min_roe\n",
    "    ]['strategy'].tolist()\n",
    "    \n",
    "    if high_performers:\n",
    "        print(f\"   Strategies meeting ROE target: {', '.join(high_performers)}\")\n",
    "    else:\n",
    "        print(f\"   No strategies meet the minimum ROE target\")\n",
    "\n",
    "# Final recommendation\n",
    "print(f\"\\n5. FINAL RECOMMENDATION:\")\n",
    "print(f\"   Deploy {validation_result.best_strategy} strategy based on:\")\n",
    "print(f\"   - Strong out-of-sample performance\")\n",
    "print(f\"   - Acceptable overfitting risk\")\n",
    "print(f\"   - Good consistency across market conditions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This walk-forward validation system provides:\n",
    "\n",
    "1. **Robust Testing**: Strategies tested across multiple time windows\n",
    "2. **Overfitting Detection**: Identifies strategies that perform well in-sample but poorly out-of-sample\n",
    "3. **Consistency Measurement**: Evaluates strategy stability across different periods\n",
    "4. **Performance Tracking**: Comprehensive metrics for informed decision-making\n",
    "5. **Actionable Insights**: Clear recommendations based on empirical evidence\n",
    "\n",
    "The system helps ensure that selected insurance strategies will perform well in real-world conditions, not just in backtests."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
