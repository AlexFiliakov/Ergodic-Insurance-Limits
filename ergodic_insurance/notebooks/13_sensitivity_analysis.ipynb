{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitivity Analysis for Insurance Optimization\n",
    "\n",
    "This notebook demonstrates the comprehensive sensitivity analysis tools for understanding how parameter changes affect optimization outcomes.\n",
    "\n",
    "## Key Features:\n",
    "- One-at-a-time (OAT) parameter analysis\n",
    "- Tornado diagrams for impact ranking\n",
    "- Two-way sensitivity heatmaps\n",
    "- Efficient caching for repeated analyses\n",
    "- Publication-ready visualizations\n",
    "\n",
    "Author: Alex Filiakov  \n",
    "Date: 2025-01-29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Project imports\n",
    "from ergodic_insurance.src.sensitivity import (\n",
    "    SensitivityAnalyzer,\n",
    "    SensitivityResult,\n",
    "    TwoWaySensitivityResult\n",
    ")\n",
    "from ergodic_insurance.src.sensitivity_visualization import (\n",
    "    plot_tornado_diagram,\n",
    "    plot_two_way_sensitivity,\n",
    "    plot_parameter_sweep,\n",
    "    plot_sensitivity_matrix,\n",
    "    create_sensitivity_report\n",
    ")\n",
    "\n",
    "# Import optimization components\n",
    "from ergodic_insurance.src.manufacturer import WidgetManufacturer\n",
    "from ergodic_insurance.src.business_optimizer import (\n",
    "    BusinessOptimizer,\n",
    "    BusinessConstraints,\n",
    "    BusinessObjective,\n",
    "    OptimizationDirection\n",
    ")\n",
    "from ergodic_insurance.src.loss_distributions import LognormalLoss\n",
    "from ergodic_insurance.src.claim_generator import ClaimGenerator\n",
    "\n",
    "print(\"âœ… All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Base Configuration\n",
    "\n",
    "First, let's establish our baseline configuration for the manufacturing company and insurance parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base configuration for sensitivity analysis\n",
    "base_config = {\n",
    "    # Manufacturing parameters\n",
    "    \"initial_assets\": 10_000_000,  # $10M starting assets\n",
    "    \"asset_turnover\": 1.0,         # Revenue = 1x assets\n",
    "    \"operating_margin\": 0.08,      # 8% operating margin\n",
    "    \"tax_rate\": 0.25,              # 25% tax rate\n",
    "    \"working_capital_pct\": 0.20,   # 20% of sales\n",
    "    \n",
    "    # Loss parameters (attritional)\n",
    "    \"loss_frequency\": 5.0,         # 5 losses per year average\n",
    "    \"loss_severity_mean\": 100_000, # $100K average loss\n",
    "    \"loss_severity_cv\": 1.5,       # Coefficient of variation\n",
    "    \n",
    "    # Insurance parameters\n",
    "    \"premium_rate\": 0.02,          # 2% of coverage\n",
    "    \"deductible\": 50_000,          # $50K retention\n",
    "    \"coverage_limit\": 5_000_000,   # $5M coverage\n",
    "    \n",
    "    # Optimization constraints\n",
    "    \"max_ruin_probability\": 0.01,  # 1% max bankruptcy risk\n",
    "    \"min_roe_target\": 0.15,        # 15% minimum ROE\n",
    "    \"max_premium_budget\": 0.03,    # 3% of revenue max\n",
    "}\n",
    "\n",
    "print(\"Base Configuration:\")\n",
    "for key, value in base_config.items():\n",
    "    if isinstance(value, float) and value < 1:\n",
    "        print(f\"  {key}: {value:.1%}\")\n",
    "    elif isinstance(value, (int, float)) and value > 1000:\n",
    "        print(f\"  {key}: ${value:,.0f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Mock Optimizer for Demonstration\n",
    "\n",
    "For this demonstration, we'll create a simplified optimizer that provides deterministic results based on the configuration. In production, this would be replaced with the actual BusinessOptimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class OptimizationResult:\n",
    "    \"\"\"Simplified optimization result for demonstration.\"\"\"\n",
    "    @dataclass\n",
    "    class Strategy:\n",
    "        expected_roe: float\n",
    "        bankruptcy_risk: float\n",
    "        growth_rate: float\n",
    "        capital_efficiency: float\n",
    "        deductible: float\n",
    "        premium_rate: float\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        # Create realistic results based on configuration\n",
    "        freq = config.get('loss_frequency', 5)\n",
    "        sev = config.get('loss_severity_mean', 100000)\n",
    "        prem = config.get('premium_rate', 0.02)\n",
    "        ded = config.get('deductible', 50000)\n",
    "        margin = config.get('operating_margin', 0.08)\n",
    "        \n",
    "        # Simple model: higher frequency/severity = lower ROE, higher risk\n",
    "        base_roe = margin * 2  # Leverage effect\n",
    "        freq_impact = (5 - freq) * 0.01  # Lower frequency = higher ROE\n",
    "        sev_impact = (100000 - sev) / 1000000  # Lower severity = higher ROE\n",
    "        prem_impact = -prem * 2  # Premium cost reduces ROE\n",
    "        \n",
    "        self.optimal_strategy = self.Strategy(\n",
    "            expected_roe=max(0.05, min(0.30, base_roe + freq_impact + sev_impact + prem_impact)),\n",
    "            bankruptcy_risk=max(0.001, min(0.05, freq * sev / 100000000)),\n",
    "            growth_rate=max(0.02, min(0.15, margin * 0.8)),\n",
    "            capital_efficiency=0.75 + margin,\n",
    "            deductible=ded,\n",
    "            premium_rate=prem\n",
    "        )\n",
    "\n",
    "class DemoOptimizer:\n",
    "    \"\"\"Simplified optimizer for demonstration.\"\"\"\n",
    "    \n",
    "    def optimize(self, config):\n",
    "        \"\"\"Run optimization with given configuration.\"\"\"\n",
    "        return OptimizationResult(config)\n",
    "\n",
    "# Create optimizer instance\n",
    "optimizer = DemoOptimizer()\n",
    "\n",
    "# Test the optimizer\n",
    "test_result = optimizer.optimize(base_config)\n",
    "print(f\"Test optimization result:\")\n",
    "print(f\"  Expected ROE: {test_result.optimal_strategy.expected_roe:.1%}\")\n",
    "print(f\"  Bankruptcy Risk: {test_result.optimal_strategy.bankruptcy_risk:.2%}\")\n",
    "print(f\"  Growth Rate: {test_result.optimal_strategy.growth_rate:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Sensitivity Analyzer\n",
    "\n",
    "Create the sensitivity analyzer with caching enabled for efficient repeated analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cache directory for persistent caching\n",
    "cache_dir = Path(\"cache/sensitivity\")\n",
    "cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Initialize analyzer\n",
    "analyzer = SensitivityAnalyzer(\n",
    "    base_config=base_config,\n",
    "    optimizer=optimizer,\n",
    "    cache_dir=cache_dir\n",
    ")\n",
    "\n",
    "print(f\"âœ… Sensitivity analyzer initialized\")\n",
    "print(f\"   Cache directory: {cache_dir}\")\n",
    "print(f\"   Base configuration has {len(base_config)} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. One-at-a-Time (OAT) Sensitivity Analysis\n",
    "\n",
    "Analyze how individual parameters affect the optimization outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze loss frequency sensitivity\n",
    "freq_result = analyzer.analyze_parameter(\n",
    "    \"loss_frequency\",\n",
    "    param_range=(3, 8),  # 3-8 losses per year\n",
    "    n_points=11\n",
    ")\n",
    "\n",
    "print(\"Loss Frequency Sensitivity Analysis:\")\n",
    "print(f\"  Parameter: {freq_result.parameter}\")\n",
    "print(f\"  Baseline: {freq_result.baseline_value:.1f} losses/year\")\n",
    "print(f\"  Range: {freq_result.variations[0]:.1f} - {freq_result.variations[-1]:.1f}\")\n",
    "print(f\"\\nImpact on metrics:\")\n",
    "for metric in ['optimal_roe', 'bankruptcy_risk', 'growth_rate']:\n",
    "    impact = freq_result.calculate_impact(metric)\n",
    "    min_val, max_val = freq_result.get_metric_bounds(metric)\n",
    "    print(f\"  {metric}: elasticity = {impact:.3f}, range = [{min_val:.3f}, {max_val:.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize parameter sweep\n",
    "fig = plot_parameter_sweep(\n",
    "    freq_result,\n",
    "    metrics=['optimal_roe', 'bankruptcy_risk', 'growth_rate'],\n",
    "    title=\"Impact of Loss Frequency on Key Metrics\",\n",
    "    figsize=(12, 4),\n",
    "    mark_baseline=True\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tornado Diagram Analysis\n",
    "\n",
    "Create a tornado diagram to rank parameters by their impact on the objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define key parameters to analyze\n",
    "key_parameters = [\n",
    "    \"loss_frequency\",\n",
    "    \"loss_severity_mean\",\n",
    "    \"loss_severity_cv\",\n",
    "    \"premium_rate\",\n",
    "    \"deductible\",\n",
    "    \"operating_margin\",\n",
    "    \"asset_turnover\",\n",
    "    \"working_capital_pct\"\n",
    "]\n",
    "\n",
    "# Generate tornado diagram data\n",
    "print(\"Analyzing parameter sensitivities...\")\n",
    "tornado_data = analyzer.create_tornado_diagram(\n",
    "    parameters=key_parameters,\n",
    "    metric=\"optimal_roe\",\n",
    "    relative_range=0.3,  # Â±30% from baseline\n",
    "    n_points=5  # Use 5 points for speed\n",
    ")\n",
    "\n",
    "print(\"\\nParameter Impact Ranking:\")\n",
    "for idx, row in tornado_data.iterrows():\n",
    "    print(f\"  {idx+1}. {row['parameter']:20s} Impact: {row['impact']:.3f} ({row['direction']})\"\n",
    "          f\" Range: [{row['low_value']:.3f}, {row['high_value']:.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tornado diagram visualization\n",
    "fig = plot_tornado_diagram(\n",
    "    tornado_data,\n",
    "    title=\"Sensitivity Analysis - Tornado Diagram\",\n",
    "    metric_label=\"Impact on Expected ROE\",\n",
    "    figsize=(10, 6),\n",
    "    n_params=8,  # Show top 8 parameters\n",
    "    show_values=True\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Two-Way Sensitivity Analysis\n",
    "\n",
    "Analyze the interaction between two parameters and their combined effect on outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze frequency vs severity interaction\n",
    "two_way_result = analyzer.analyze_two_way(\n",
    "    param1=\"loss_frequency\",\n",
    "    param2=\"loss_severity_mean\",\n",
    "    param1_range=(3, 8),\n",
    "    param2_range=(50000, 200000),\n",
    "    n_points1=8,\n",
    "    n_points2=8,\n",
    "    metric=\"optimal_roe\"\n",
    ")\n",
    "\n",
    "print(f\"Two-Way Sensitivity Analysis:\")\n",
    "print(f\"  Parameter 1: {two_way_result.parameter1}\")\n",
    "print(f\"  Parameter 2: {two_way_result.parameter2}\")\n",
    "print(f\"  Metric: {two_way_result.metric_name}\")\n",
    "print(f\"  Grid size: {two_way_result.metric_grid.shape}\")\n",
    "print(f\"  ROE range: [{two_way_result.metric_grid.min():.3f}, {two_way_result.metric_grid.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize two-way sensitivity\n",
    "fig = plot_two_way_sensitivity(\n",
    "    two_way_result,\n",
    "    title=\"ROE Sensitivity: Loss Frequency vs Severity\",\n",
    "    cmap='RdYlGn',\n",
    "    figsize=(10, 8),\n",
    "    show_contours=True,\n",
    "    contour_levels=10,\n",
    "    optimal_point=(5.0, 100000),  # Mark baseline\n",
    "    fmt='.2%'\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal regions - where ROE > 15%\n",
    "target_roe = 0.15\n",
    "optimal_mask = two_way_result.find_optimal_region(target_roe, tolerance=0.1)\n",
    "\n",
    "# Count feasible combinations\n",
    "n_feasible = optimal_mask.sum()\n",
    "n_total = optimal_mask.size\n",
    "print(f\"\\nFeasibility Analysis:\")\n",
    "print(f\"  Target ROE: {target_roe:.1%}\")\n",
    "print(f\"  Feasible combinations: {n_feasible}/{n_total} ({100*n_feasible/n_total:.1f}%)\")\n",
    "\n",
    "# Show feasible region\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "X, Y = np.meshgrid(two_way_result.values1, two_way_result.values2, indexing='ij')\n",
    "ax.contourf(X, Y, optimal_mask.astype(float), levels=[0, 0.5, 1], \n",
    "            colors=['red', 'green'], alpha=0.3)\n",
    "ax.contour(X, Y, two_way_result.metric_grid, levels=[target_roe], \n",
    "           colors='black', linewidths=2)\n",
    "ax.set_xlabel('Loss Frequency (claims/year)')\n",
    "ax.set_ylabel('Loss Severity Mean ($)')\n",
    "ax.set_title(f'Feasible Region for ROE > {target_roe:.1%}')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Parameter Group Analysis\n",
    "\n",
    "Analyze multiple parameters simultaneously to understand their relative importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter groups for analysis\n",
    "loss_parameters = {\n",
    "    \"loss_frequency\": (3, 8),\n",
    "    \"loss_severity_mean\": (50000, 200000),\n",
    "    \"loss_severity_cv\": (1.0, 2.0)\n",
    "}\n",
    "\n",
    "insurance_parameters = {\n",
    "    \"premium_rate\": (0.01, 0.04),\n",
    "    \"deductible\": (25000, 100000),\n",
    "    \"coverage_limit\": (2500000, 10000000)\n",
    "}\n",
    "\n",
    "# Analyze loss parameters\n",
    "print(\"Analyzing loss parameter group...\")\n",
    "loss_results = analyzer.analyze_parameter_group(\n",
    "    loss_parameters,\n",
    "    n_points=7,\n",
    "    metric=\"optimal_roe\"\n",
    ")\n",
    "\n",
    "# Analyze insurance parameters  \n",
    "print(\"Analyzing insurance parameter group...\")\n",
    "insurance_results = analyzer.analyze_parameter_group(\n",
    "    insurance_parameters,\n",
    "    n_points=7,\n",
    "    metric=\"optimal_roe\"\n",
    ")\n",
    "\n",
    "print(\"\\nAnalysis complete!\")\n",
    "print(f\"  Loss parameters analyzed: {len(loss_results)}\")\n",
    "print(f\"  Insurance parameters analyzed: {len(insurance_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sensitivity matrix for loss parameters\n",
    "fig = plot_sensitivity_matrix(\n",
    "    loss_results,\n",
    "    metric=\"optimal_roe\",\n",
    "    figsize=(10, 6),\n",
    "    cmap='coolwarm',\n",
    "    show_values=True\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Complete Sensitivity Report\n",
    "\n",
    "Generate a comprehensive sensitivity report with all visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory for report\n",
    "output_dir = Path(\"results/sensitivity_analysis\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Generate complete report\n",
    "print(\"Generating comprehensive sensitivity report...\")\n",
    "report = create_sensitivity_report(\n",
    "    analyzer,\n",
    "    parameters=key_parameters[:5],  # Use top 5 parameters for speed\n",
    "    output_dir=str(output_dir),\n",
    "    metric=\"optimal_roe\",\n",
    "    formats=['png']  # Save as PNG\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ“Š Report Summary:\")\n",
    "print(f\"  Most impactful parameter: {report['summary']['most_impactful']}\")\n",
    "print(f\"  Least impactful parameter: {report['summary']['least_impactful']}\")\n",
    "print(f\"  Total parameters analyzed: {report['summary']['total_parameters']}\")\n",
    "\n",
    "if 'relative_importances' in report['summary']:\n",
    "    print(\"\\n  Relative Importance:\")\n",
    "    for item in report['summary']['relative_importances'][:3]:\n",
    "        print(f\"    {item['parameter']:20s}: {item['relative_importance']:.1f}%\")\n",
    "\n",
    "print(f\"\\nâœ… Report saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Cache Performance Analysis\n",
    "\n",
    "Demonstrate the efficiency gains from caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Clear cache for timing comparison\n",
    "analyzer.clear_cache()\n",
    "print(\"Cache cleared.\\n\")\n",
    "\n",
    "# First run - no cache\n",
    "start = time.time()\n",
    "result1 = analyzer.analyze_parameter(\n",
    "    \"loss_frequency\",\n",
    "    param_range=(3, 8),\n",
    "    n_points=11\n",
    ")\n",
    "time_no_cache = time.time() - start\n",
    "\n",
    "# Second run - with cache\n",
    "start = time.time()\n",
    "result2 = analyzer.analyze_parameter(\n",
    "    \"loss_frequency\",\n",
    "    param_range=(3, 8),\n",
    "    n_points=11\n",
    ")\n",
    "time_with_cache = time.time() - start\n",
    "\n",
    "print(f\"Performance Comparison:\")\n",
    "print(f\"  First run (no cache): {time_no_cache:.4f} seconds\")\n",
    "print(f\"  Second run (cached): {time_with_cache:.4f} seconds\")\n",
    "print(f\"  Speedup: {time_no_cache/time_with_cache:.1f}x\")\n",
    "print(f\"\\nCache statistics:\")\n",
    "print(f\"  Cached results: {len(analyzer.results_cache)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Advanced Analysis: Multi-Metric Comparison\n",
    "\n",
    "Compare sensitivity across different optimization objectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze same parameter for different metrics\n",
    "metrics_to_compare = ['optimal_roe', 'bankruptcy_risk', 'growth_rate', 'capital_efficiency']\n",
    "\n",
    "# Calculate impacts for each metric\n",
    "impact_comparison = pd.DataFrame()\n",
    "\n",
    "for param in ['loss_frequency', 'loss_severity_mean', 'premium_rate', 'operating_margin']:\n",
    "    result = analyzer.analyze_parameter(\n",
    "        param,\n",
    "        relative_range=0.3,\n",
    "        n_points=5\n",
    "    )\n",
    "    \n",
    "    impacts = {}\n",
    "    for metric in metrics_to_compare:\n",
    "        impacts[metric] = abs(result.calculate_impact(metric))\n",
    "    \n",
    "    impact_comparison[param] = pd.Series(impacts)\n",
    "\n",
    "# Display heatmap\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.heatmap(impact_comparison, annot=True, fmt='.3f', cmap='YlOrRd', \n",
    "            cbar_kws={'label': 'Elasticity (absolute value)'})\n",
    "ax.set_title('Parameter Sensitivity Across Different Metrics')\n",
    "ax.set_xlabel('Parameter')\n",
    "ax.set_ylabel('Metric')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "for metric in metrics_to_compare:\n",
    "    most_sensitive = impact_comparison.loc[metric].idxmax()\n",
    "    print(f\"  {metric:20s} is most sensitive to: {most_sensitive}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Recommendations\n",
    "\n",
    "Based on the sensitivity analysis, we can provide actionable recommendations for risk management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate recommendations based on analysis\n",
    "print(\"ðŸ“‹ SENSITIVITY ANALYSIS RECOMMENDATIONS\\n\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Get most impactful parameters from tornado analysis\n",
    "if not tornado_data.empty:\n",
    "    top_params = tornado_data.head(3)\n",
    "    \n",
    "    print(\"\\n1. FOCUS AREAS (High Impact Parameters):\")\n",
    "    for idx, row in top_params.iterrows():\n",
    "        param = row['parameter']\n",
    "        impact = row['impact']\n",
    "        direction = row['direction']\n",
    "        \n",
    "        print(f\"\\n   â€¢ {param.replace('_', ' ').title()}:\")\n",
    "        print(f\"     - Impact coefficient: {impact:.3f}\")\n",
    "        print(f\"     - Direction: {direction}\")\n",
    "        \n",
    "        if param == 'loss_frequency':\n",
    "            print(f\"     - Recommendation: Invest in loss prevention to reduce frequency\")\n",
    "        elif param == 'loss_severity_mean':\n",
    "            print(f\"     - Recommendation: Implement risk mitigation to cap severity\")\n",
    "        elif param == 'premium_rate':\n",
    "            print(f\"     - Recommendation: Negotiate better rates or consider alternative structures\")\n",
    "        elif param == 'operating_margin':\n",
    "            print(f\"     - Recommendation: Focus on operational efficiency improvements\")\n",
    "\n",
    "print(\"\\n2. ROBUST ZONES (Parameter Ranges with Stable Performance):\")\n",
    "if two_way_result is not None:\n",
    "    optimal_region = two_way_result.find_optimal_region(0.15, tolerance=0.1)\n",
    "    robust_freq = two_way_result.values1[optimal_region.any(axis=1)]\n",
    "    robust_sev = two_way_result.values2[optimal_region.any(axis=0)]\n",
    "    \n",
    "    if len(robust_freq) > 0:\n",
    "        print(f\"   â€¢ Loss Frequency: {robust_freq.min():.1f} - {robust_freq.max():.1f} claims/year\")\n",
    "    if len(robust_sev) > 0:\n",
    "        print(f\"   â€¢ Loss Severity: ${robust_sev.min():,.0f} - ${robust_sev.max():,.0f}\")\n",
    "\n",
    "print(\"\\n3. RISK THRESHOLDS:\")\n",
    "print(f\"   â€¢ Maintain loss frequency below 6 claims/year for ROE > 15%\")\n",
    "print(f\"   â€¢ Keep average severity under $150K to minimize bankruptcy risk\")\n",
    "print(f\"   â€¢ Premium rates above 3% significantly impact profitability\")\n",
    "\n",
    "print(\"\\n4. MONITORING PRIORITIES:\")\n",
    "print(\"   â€¢ Track monthly: Loss frequency trends\")\n",
    "print(\"   â€¢ Track quarterly: Average severity and large loss indicators\")\n",
    "print(\"   â€¢ Track annually: Premium rate competitiveness\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\nâœ… Analysis Complete! See output directory for detailed reports.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook has demonstrated comprehensive sensitivity analysis capabilities:\n",
    "\n",
    "1. **One-at-a-time analysis** to understand individual parameter impacts\n",
    "2. **Tornado diagrams** to rank parameters by importance\n",
    "3. **Two-way sensitivity** to explore parameter interactions\n",
    "4. **Efficient caching** for performance optimization\n",
    "5. **Publication-ready visualizations** for reporting\n",
    "\n",
    "The sensitivity analysis tools provide critical insights for:\n",
    "- Risk management decision-making\n",
    "- Parameter monitoring priorities\n",
    "- Optimization robustness assessment\n",
    "- Strategic planning under uncertainty\n",
    "\n",
    "### Next Steps:\n",
    "1. Apply to real optimization problems with actual loss data\n",
    "2. Extend to multi-objective sensitivity analysis\n",
    "3. Implement global sensitivity analysis methods (Sobol indices)\n",
    "4. Add uncertainty quantification to parameter estimates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
