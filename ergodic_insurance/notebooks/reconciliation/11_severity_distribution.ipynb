{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6aqk80hy_1x"
   },
   "source": [
    "# Severity Exploration\n",
    "\n",
    "## Overview\n",
    "In Part I, we set up a manufacturer with a loss structure (attritional, large, and catastrophic losses).\n",
    "In Part II, we plot the true severity distribution and explore its shape.\n",
    "In Part III, we draw a 5-year sample from the \"true\" distribution, and approximate it using Maximum Likelihood. The kicker is we have no significant catastrophic losses in the dataset, so we simulate an industry database of $5M+ catastrophic losses from a pool of around 10,000 companies per year (this annual number of companies is Poisson-distributed) and explore the catastrophic tail shape from that data.\n",
    "\n",
    "- **Audience**: [Practitioner] / [Developer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2049,
     "status": "ok",
     "timestamp": 1771697609273,
     "user": {
      "displayName": "Alex Filiakov",
      "userId": "03712643796933532488"
     },
     "user_tz": 300
    },
    "id": "n1opuX5vy_12",
    "outputId": "b476fd7a-fcfd-44c5-9d46-f4fe0095cb32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "\n",
      "Setup complete. If you see numpy/scipy import errors below,\n",
      "restart the runtime (Runtime > Restart runtime) and re-run all cells.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Google Colab setup: mount Drive and install package dependencies.\n",
    "\n",
    "Run this cell first. If prompted to restart the runtime, do so, then re-run all cells.\n",
    "This cell is a no-op when running locally.\n",
    "\"\"\"\n",
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    NOTEBOOK_DIR = '/content/drive/My Drive/Colab Notebooks/ei_notebooks/optimization'\n",
    "\n",
    "    os.chdir(NOTEBOOK_DIR)\n",
    "    if NOTEBOOK_DIR not in sys.path:\n",
    "        sys.path.append(NOTEBOOK_DIR)\n",
    "\n",
    "    !pip install ergodic-insurance -q 2>&1 | tail -3\n",
    "    print('\\nSetup complete. If you see numpy/scipy import errors below,')\n",
    "    print('restart the runtime (Runtime > Restart runtime) and re-run all cells.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geeprRCGy_14"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 681,
     "status": "ok",
     "timestamp": 1771697609985,
     "user": {
      "displayName": "Alex Filiakov",
      "userId": "03712643796933532488"
     },
     "user_tz": 300
    },
    "id": "wqNTpoVZy_15",
    "outputId": "50b9fe14-d265-4beb-bbed-6b7e551b1828"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPU cores: 44\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "import multiprocessing\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from ergodic_insurance.pareto_frontier import (\n",
    "    Objective, ObjectiveType, ParetoFrontier, ParetoPoint,\n",
    ")\n",
    "from ergodic_insurance.config import ManufacturerConfig\n",
    "from ergodic_insurance.manufacturer import WidgetManufacturer\n",
    "from ergodic_insurance.insurance_program import (\n",
    "    EnhancedInsuranceLayer, InsuranceProgram,\n",
    ")\n",
    "from ergodic_insurance.loss_distributions import ManufacturingLossGenerator\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "N_CORES = multiprocessing.cpu_count()\n",
    "print(f\"Number of CPU cores: {N_CORES}\")  # Available parallel cores for sensitivity sweep\n",
    "CI = False      # Set True to skip heavy computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fhE0J3eM2MIL"
   },
   "source": [
    "## Part I: Parameter Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52krAvZH0IP2"
   },
   "source": [
    "### Manufacturing Company Configuration\n",
    "\n",
    "Baseline company parameters (the experiment varies revenue via `revenue_grid`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1771697609999,
     "user": {
      "displayName": "Alex Filiakov",
      "userId": "03712643796933532488"
     },
     "user_tz": 300
    },
    "id": "JVqpuH1e0amx",
    "outputId": "26b89d6c-112e-42e2-99af-1751b26c3fac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MANUFACTURING COMPANY PROFILE\n",
      "============================================================\n",
      "Total Assets:          $     5,000,000\n",
      "Annual Revenue:        $    10,000,000\n",
      "Operating Income:      $     1,500,000\n",
      "Operating Margin:               15.0%\n",
      "Asset Turnover:                   2.0x\n",
      "Revenue Volatility:               0.5\n",
      "Tax Rate:                      25.0%\n",
      "Retention Ratio:               70.0%\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# --- Economic Parameters\n",
    "ATR = 2.0                # Asset turnover ratio\n",
    "OPERATING_MARGIN = 0.15  # 12% EBIT margin before Insurable Losses\n",
    "REV_VOL = 0.50           # Revenue volatility (annualized)\n",
    "INITIAL_ASSETS = 10_000_000\n",
    "\n",
    "# --- Company Configuration ---\n",
    "mfg_config = ManufacturerConfig(\n",
    "    initial_assets=INITIAL_ASSETS,          # $15M total assets\n",
    "    asset_turnover_ratio=ATR,               # Revenue = Assets Ãƒâ€” turnover = $22.5M\n",
    "    base_operating_margin=OPERATING_MARGIN, # 12% EBIT margin -> $2.7M/yr operating income\n",
    "    tax_rate=0.25,                          # 25% corporate tax\n",
    "    retention_ratio=0.70,                   # 70% earnings retained for growth\n",
    ")\n",
    "\n",
    "# Display company profile\n",
    "revenue = mfg_config.initial_assets * mfg_config.asset_turnover_ratio\n",
    "ebit = revenue * mfg_config.base_operating_margin\n",
    "print(\"=\" * 60)\n",
    "print(\"MANUFACTURING COMPANY PROFILE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total Assets:          ${mfg_config.initial_assets:>14,.0f}\")\n",
    "print(f\"Annual Revenue:        ${revenue:>14,.0f}\")\n",
    "print(f\"Operating Income:      ${ebit:>14,.0f}\")\n",
    "print(f\"Operating Margin:      {mfg_config.base_operating_margin:>14.1%}\")\n",
    "print(f\"Asset Turnover:        {mfg_config.asset_turnover_ratio:>14.1f}x\")\n",
    "print(f\"Revenue Volatility:    {REV_VOL:>14}\")\n",
    "print(f\"Tax Rate:              {mfg_config.tax_rate:>13.1%}\")\n",
    "print(f\"Retention Ratio:       {mfg_config.retention_ratio:>13.1%}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5kXbr46X1SEs"
   },
   "source": [
    "### Shared Simulation Infrastructure\n",
    "\n",
    "Loss model, analytical LEV-based layer pricing, CRN scenario generation, and simulation engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 225,
     "status": "ok",
     "timestamp": 1771697610259,
     "user": {
      "displayName": "Alex Filiakov",
      "userId": "03712643796933532488"
     },
     "user_tz": 300
    },
    "id": "sFTJeZfb1UJN",
    "outputId": "6d20651a-e94f-43dc-bfc0-ad91a4b0ab4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss model validation (10,000 one-year samples):\n",
      "  Expected annual loss:  $     810,152\n",
      "  Operating income:      $   1,500,000\n",
      "  Loss / Income ratio:   54%\n",
      "  Std dev annual loss:   $   2,232,722\n",
      "  Max annual loss:       $  78,067,932\n",
      "\n",
      "Analytical layer pricing (target LR = 70%):\n",
      "    Attachment         Limit       E[Loss]       Premium       ROL\n",
      "  ------------  ------------  ------------  ------------  --------\n",
      "  $     10,000  $  4,990,000  $    658,191  $    940,273   18.84%\n",
      "  $     25,000  $  4,975,000  $    638,549  $    912,213   18.34%\n",
      "  $     50,000  $  4,950,000  $    613,275  $    876,107   17.70%\n",
      "  $    250,000  $  4,750,000  $    493,355  $    704,793   14.84%\n",
      "  $  1,000,000  $  4,000,000  $    260,635  $    372,335    9.31%\n",
      "  $  2,000,000  $  3,000,000  $    121,171  $    173,101    5.77%\n",
      "  $  4,000,000  $  1,000,000  $     24,540  $     35,057    3.51%\n",
      "  $  5,000,000  $ 20,000,000  $    109,088  $    155,841    0.78%\n",
      "  $ 25,000,000  $ 25,000,000  $     20,511  $     29,301    0.12%\n",
      "  $ 50,000,000  $ 50,000,000  $     11,164  $     15,948    0.03%\n",
      "\n",
      "Pre-generating CRN loss pool (500 paths x 1 years)...\n",
      "  Done in 0.0s\n",
      "  Shape: 500 paths x 1 years\n",
      "  Mean annual loss: $700,309\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import warnings\n",
    "import logging\n",
    "import time\n",
    "\n",
    "# Suppress all warnings and verbose solver logging\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger(\"ergodic_insurance\").setLevel(logging.ERROR)\n",
    "\n",
    "from ergodic_insurance.hjb_solver import (\n",
    "    StateVariable, ControlVariable, StateSpace,\n",
    "    LogUtility, PowerUtility, ExpectedWealth,\n",
    "    HJBProblem, HJBSolver, HJBSolverConfig,\n",
    ")\n",
    "from ergodic_insurance.optimal_control import (\n",
    "    ControlSpace, StaticControl, HJBFeedbackControl,\n",
    "    TimeVaryingControl, OptimalController,\n",
    ")\n",
    "from ergodic_insurance.config import ManufacturerConfig\n",
    "from ergodic_insurance.manufacturer import WidgetManufacturer\n",
    "from ergodic_insurance.insurance_program import (\n",
    "    EnhancedInsuranceLayer, InsuranceProgram,\n",
    ")\n",
    "from ergodic_insurance.loss_distributions import (\n",
    "    ManufacturingLossGenerator, LognormalLoss, ParetoLoss,\n",
    ")\n",
    "from ergodic_insurance.insurance_pricing import LayerPricer\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "N_CORES = 40   # Available parallel cores for sensitivity sweep\n",
    "CI = False      # Set True to skip heavy computations\n",
    "\n",
    "# =====================================================\n",
    "# SHARED SIMULATION INFRASTRUCTURE\n",
    "# =====================================================\n",
    "# Used by Parts 5, 8, 9, 10, and 11.\n",
    "\n",
    "# --- Economic Parameters ---\n",
    "REFERENCE_REVENUE = ATR * INITIAL_ASSETS  # Fixed reference for loss calibration\n",
    "\n",
    "# --- Loss Scaling ---\n",
    "# Loss frequency (and CRN loss amounts) scale with the square root of\n",
    "# revenue.  This keeps the loss drag proportional to the company's\n",
    "# actual size for both insured and uninsured strategies.\n",
    "FREQ_SCALING_EXPONENT = 0.75\n",
    "\n",
    "# --- Amplified Loss Model ---\n",
    "ATTR_BASE_FREQ = 3\n",
    "ATTR_SEV_MEAN = 10_000\n",
    "ATTR_SEV_CV = 15\n",
    "\n",
    "LG_BASE_FREQ = 1.25\n",
    "LG_SEV_MEAN = 400_000\n",
    "LG_SEV_CV = 7\n",
    "\n",
    "CAT_BASE_FREQ = 0.15\n",
    "CAT_SEV_ALPHA = 2.01\n",
    "CAT_SEV_XM = 1_000_000\n",
    "\n",
    "LOSS_PARAMS = dict(\n",
    "    attritional_params={'base_frequency': ATTR_BASE_FREQ,\n",
    "                        'severity_mean': ATTR_SEV_MEAN,\n",
    "                        'severity_cv': ATTR_SEV_CV,\n",
    "                        'revenue_scaling_exponent': FREQ_SCALING_EXPONENT,\n",
    "                        'reference_revenue': REFERENCE_REVENUE},\n",
    "    large_params={'base_frequency': LG_BASE_FREQ,\n",
    "                  'severity_mean': LG_SEV_MEAN,\n",
    "                  'severity_cv': LG_SEV_CV,\n",
    "                  'revenue_scaling_exponent': FREQ_SCALING_EXPONENT,\n",
    "                  'reference_revenue': REFERENCE_REVENUE},\n",
    "    catastrophic_params={'base_frequency': CAT_BASE_FREQ,\n",
    "                         'severity_alpha': CAT_SEV_ALPHA,\n",
    "                         'severity_xm': CAT_SEV_XM,\n",
    "                         'revenue_scaling_exponent': FREQ_SCALING_EXPONENT,\n",
    "                         'reference_revenue': REFERENCE_REVENUE},\n",
    ")\n",
    "\n",
    "# Quick validation of the loss model\n",
    "_val_gen = ManufacturingLossGenerator(**LOSS_PARAMS, seed=99)\n",
    "_val_totals = []\n",
    "SCENARIOS = 10_000\n",
    "for _ in range(SCENARIOS):\n",
    "    _events, _stats = _val_gen.generate_losses(duration=1.0, revenue=REFERENCE_REVENUE)\n",
    "    _val_totals.append(_stats['total_amount'])\n",
    "_expected_annual_loss = np.mean(_val_totals)\n",
    "_operating_income = INITIAL_ASSETS * ATR * OPERATING_MARGIN\n",
    "print(f\"Loss model validation ({SCENARIOS:,.0f} one-year samples):\")\n",
    "print(f\"  Expected annual loss:  ${_expected_annual_loss:>12,.0f}\")\n",
    "print(f\"  Operating income:      ${_operating_income:>12,.0f}\")\n",
    "print(f\"  Loss / Income ratio:   {_expected_annual_loss / _operating_income:.0%}\")\n",
    "print(f\"  Std dev annual loss:   ${np.std(_val_totals):>12,.0f}\")\n",
    "print(f\"  Max annual loss:       ${np.max(_val_totals):>12,.0f}\")\n",
    "del _val_gen, _val_totals, _events, _stats\n",
    "\n",
    "\n",
    "# --- Analytical Layer Pricing via LEV ---\n",
    "# Instead of hardcoded rate-on-line values, we compute actuarially sound\n",
    "# premiums from the known severity distributions using limited expected\n",
    "# values (LEVs).  For each layer (attachment a, limit l):\n",
    "#\n",
    "#   E[layer loss] = sum_i  freq_i * [LEV_i(a+l) - LEV_i(a)]\n",
    "#   premium       = E[layer loss] / target_loss_ratio\n",
    "#   rate_on_line  = premium / limit\n",
    "#\n",
    "# This ensures the primary-layer ROL decreases naturally as the Ded\n",
    "# (retention) rises, producing the genuine cost-vs-variance tradeoff\n",
    "# that the HJB solver needs.\n",
    "#\n",
    "# The pricers are parameterized so that the sensitivity analysis (Part 9)\n",
    "# can adapt premiums to match the modified loss assumptions being tested.\n",
    "\n",
    "TARGET_LOSS_RATIO = 0.7  # Normal-market loss ratio\n",
    "LOSS_RATIO_INFLECTION = 1  # Factor by which the max layer loss ratio differs from the base\n",
    "\n",
    "def make_layer_pricers(large_freq=LG_BASE_FREQ,\n",
    "                        large_sev_mean=LG_SEV_MEAN,\n",
    "                        cur_revenue=REFERENCE_REVENUE) -> tuple:\n",
    "    \"\"\"Create a tuple of LayerPricers for a given loss parameterization.\n",
    "\n",
    "    Frequency scales as (revenue / reference)^0.5, matching the loss\n",
    "    model's sub-linear revenue scaling.  This keeps premium and loss\n",
    "    scaling consistent so that insured and uninsured strategies face\n",
    "    the same proportional cost growth.\n",
    "\n",
    "    Args:\n",
    "        large_freq: Large-loss annual frequency (default 1.0).\n",
    "        large_sev_mean: Large-loss mean severity (default $1M).\n",
    "        cur_revenue: Current revenue for frequency scaling.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (attritional, large, catastrophic) LayerPricers.\n",
    "    \"\"\"\n",
    "    scale = (cur_revenue / REFERENCE_REVENUE) ** FREQ_SCALING_EXPONENT\n",
    "    return (\n",
    "        LayerPricer(LognormalLoss(mean=ATTR_SEV_MEAN, cv=ATTR_SEV_CV),\n",
    "                    frequency=ATTR_BASE_FREQ * scale),\n",
    "        LayerPricer(LognormalLoss(mean=large_sev_mean, cv=LG_SEV_CV),\n",
    "                    frequency=large_freq * scale),\n",
    "        LayerPricer(ParetoLoss(alpha=CAT_SEV_ALPHA, xm=CAT_SEV_XM),\n",
    "                    frequency=CAT_BASE_FREQ * scale),\n",
    "    )\n",
    "\n",
    "\n",
    "# Default pricers for baseline loss model\n",
    "DEFAULT_PRICERS = make_layer_pricers()\n",
    "\n",
    "MIN_LAYER_MIDPOINT = np.mean([0, 5_000_000])\n",
    "MAX_LAYER_MIDPOINT = np.mean([450_000_000, 500_000_000])\n",
    "\n",
    "def analytical_layer_premium(attachment: float, limit: float,\n",
    "                            base_loss_ratio: float,\n",
    "                            loss_ratio_inflection: float,\n",
    "                            pricers=None) -> float:\n",
    "    \"\"\"Compute actuarial premium for a layer using LEV-based expected losses.\n",
    "\n",
    "    Premium = E[layer loss] / layer_loss_ratio, where:\n",
    "      E[layer loss] = sum over components of freq_i * (LEV_i(a+l) - LEV_i(a))\n",
    "\n",
    "    Args:\n",
    "        attachment: Layer attachment point.\n",
    "        limit: Layer limit (width of coverage).\n",
    "        pricers: Tuple of LayerPricers. Uses DEFAULT_PRICERS if None.\n",
    "        base_loss_ratio: Desired loss ratio for the layer.\n",
    "        loss_ratio_inflection: Factor by which the max layer loss ratio  differs from the base.\n",
    "    \"\"\"\n",
    "\n",
    "    pricers = pricers or DEFAULT_PRICERS\n",
    "    expected_loss = sum(p.expected_layer_loss(attachment, limit) for p in pricers)\n",
    "    cur_layer_midpoint = np.mean([attachment, attachment + limit])\n",
    "    layer_loss_ratio = base_loss_ratio + \\\n",
    "                        (1.0 / loss_ratio_inflection - 1.0) * base_loss_ratio \\\n",
    "                        * (cur_layer_midpoint - MIN_LAYER_MIDPOINT) \\\n",
    "                        / (MAX_LAYER_MIDPOINT - MIN_LAYER_MIDPOINT)\n",
    "    return expected_loss / layer_loss_ratio\n",
    "\n",
    "\n",
    "def analytical_rate_on_line(attachment: float, limit: float,\n",
    "                            base_loss_ratio: float,\n",
    "                            loss_ratio_inflection: float,\n",
    "                            pricers=None) -> float:\n",
    "    \"\"\"Compute rate-on-line for a layer: premium / limit.\"\"\"\n",
    "    if limit <= 0:\n",
    "        return 0.0\n",
    "    return analytical_layer_premium(attachment,\n",
    "                                    limit,\n",
    "                                    base_loss_ratio,\n",
    "                                    loss_ratio_inflection,\n",
    "                                    pricers) / limit\n",
    "\n",
    "\n",
    "# Validate: show how ROL varies across sample attachment points\n",
    "print(f\"\\nAnalytical layer pricing (target LR = {TARGET_LOSS_RATIO:.0%}):\")\n",
    "print(f\"  {'Attachment':>12s}  {'Limit':>12s}  {'E[Loss]':>12s}  {'Premium':>12s}  {'ROL':>8s}\")\n",
    "print(f\"  {'-'*12}  {'-'*12}  {'-'*12}  {'-'*12}  {'-'*8}\")\n",
    "for _a, _l in [(10_000, 4_990_000), (25_000, 4_975_000), (50_000, 4_950_000),\n",
    "                (250_000, 4_750_000), (1_000_000, 4_000_000),\n",
    "                (2_000_000, 3_000_000), (4_000_000, 1_000_000),\n",
    "                (5_000_000, 20_000_000), (25_000_000, 25_000_000), (50_000_000, 50_000_000)]:\n",
    "    _el = analytical_layer_premium(_a, _l, TARGET_LOSS_RATIO, 1.0) * TARGET_LOSS_RATIO\n",
    "    _p = analytical_layer_premium(_a, _l, TARGET_LOSS_RATIO, 1.0)\n",
    "    _r = analytical_rate_on_line(_a, _l, TARGET_LOSS_RATIO, 1.0)\n",
    "    print(f\"  ${_a:>11,.0f}  ${_l:>11,.0f}  ${_el:>11,.0f}  ${_p:>11,.0f}  {_r:>7.2%}\")\n",
    "\n",
    "\n",
    "# --- Insurance Tower Factory ---\n",
    "# Premium rates are computed analytically from the loss distribution,\n",
    "# ensuring that the primary-layer ROL decreases with higher retention.\n",
    "# The optional `pricers` argument lets the sensitivity analysis pass\n",
    "# in LayerPricers built from alternative loss assumptions, so that\n",
    "# premiums stay consistent with the loss environment being tested.\n",
    "\n",
    "def make_program(ded: float,\n",
    "                base_loss_ratio: float,\n",
    "                loss_ratio_inflection: float,\n",
    "                max_limit: float,\n",
    "                pricers=None) -> InsuranceProgram:\n",
    "    \"\"\"Create 4-layer tower with analytically priced premiums.\n",
    "\n",
    "    Uses LEV-based layer pricing from severity distributions so that\n",
    "    rate-on-line adjusts naturally with the retention level.\n",
    "\n",
    "    Args:\n",
    "        ded: Deductible.\n",
    "        max_limit: Maximum coverage limit (top of tower).\n",
    "        pricers: Tuple of LayerPricers. Uses DEFAULT_PRICERS if None.\n",
    "\n",
    "    Returns:\n",
    "        InsuranceProgram with actuarially sound premium loading.\n",
    "    \"\"\"\n",
    "    layer_defs = [\n",
    "        # (attachment, ceiling, reinstatements)\n",
    "        (0, 5_000_000, 0),\n",
    "        (5_000_000, 10_000_000, 0),\n",
    "        (10_000_000, 25_000_000, 0),\n",
    "        (25_000_000, 50_000_000, 0),\n",
    "        (50_000_000, 100_000_000, 0),\n",
    "        (100_000_000, 150_000_000, 0),\n",
    "        (150_000_000, 200_000_000, 0),\n",
    "        (200_000_000, 250_000_000, 0),\n",
    "        (250_000_000, 300_000_000, 0),\n",
    "        (300_000_000, 350_000_000, 0),\n",
    "        (350_000_000, 400_000_000, 0),\n",
    "        (400_000_000, 450_000_000, 0),\n",
    "        (450_000_000, 500_000_000, 0),\n",
    "    ]\n",
    "    layers = []\n",
    "    for attach, ceiling, reinst in layer_defs:\n",
    "        if ded >= ceiling:\n",
    "            continue  # Skip layers that are fully below the deductible\n",
    "        if max_limit is not None and ceiling > max_limit:\n",
    "            continue  # Skip layers that exceed the max limit constraint\n",
    "        # Now we're within the working layer\n",
    "        # The deductible is below, the max limit is above\n",
    "        effective_attach = max(attach, ded)\n",
    "        limit = ceiling - effective_attach\n",
    "        if limit <= 0:\n",
    "            continue\n",
    "        rol = analytical_rate_on_line(effective_attach, limit, base_loss_ratio, loss_ratio_inflection, pricers)\n",
    "        layers.append(EnhancedInsuranceLayer(\n",
    "            attachment_point=effective_attach,\n",
    "            limit=limit,\n",
    "            base_premium_rate=rol,\n",
    "            reinstatements=reinst,\n",
    "        ))\n",
    "    return InsuranceProgram(\n",
    "        layers=layers,\n",
    "        deductible=ded,\n",
    "        name=f\"Manufacturing Tower (Ded=${ded:,.0f})\",\n",
    "    )\n",
    "\n",
    "\n",
    "# --- CRN: Pre-generate Loss Scenarios ---\n",
    "def generate_loss_pool(n_paths, n_years, reference_revenue=REFERENCE_REVENUE, seed=SEED, specific_loss_params=None):\n",
    "    \"\"\"Pre-generate loss scenarios for Common Random Number comparison.\n",
    "    All strategies will face the exact same loss events and revenue shocks.\n",
    "    Losses are generated at a fixed reference revenue; the simulation\n",
    "    engine then scales event amounts by (actual_revenue / reference)^0.5\n",
    "    so that loss burden grows proportionally with the company.\n",
    "\n",
    "    Args:\n",
    "        n_paths: Number of simulation paths.\n",
    "        n_years: Number of years to simulate.\n",
    "        reference_revenue: Reference revenue for loss calibration.\n",
    "        seed: Base seed for random number generation.\n",
    "        specific_loss_params: A dictionary of loss parameters to use, overriding\n",
    "                              the global LOSS_PARAMS for this call.\n",
    "    \"\"\"\n",
    "    loss_params_to_use = specific_loss_params if specific_loss_params is not None else LOSS_PARAMS\n",
    "\n",
    "    ss = np.random.SeedSequence(seed)\n",
    "    children = ss.spawn(n_paths + 1)\n",
    "\n",
    "    # Shared revenue shocks\n",
    "    rev_rng = np.random.default_rng(children[0])\n",
    "    revenue_shocks = rev_rng.standard_normal((n_paths, n_years))\n",
    "\n",
    "    # Per-path loss event sequences\n",
    "    all_losses = []  # [path][year] -> List[LossEvent]\n",
    "    for i in range(n_paths):\n",
    "        gen = ManufacturingLossGenerator(\n",
    "            **loss_params_to_use, # Use the potentially overridden loss params\n",
    "            seed=int(children[i + 1].generate_state(1)[0] % (2**31)),\n",
    "        )\n",
    "        path_losses = []\n",
    "        for t in range(n_years):\n",
    "            events, _ = gen.generate_losses(duration=1.0, revenue=reference_revenue)\n",
    "            path_losses.append(events)\n",
    "        all_losses.append(path_losses)\n",
    "\n",
    "    return revenue_shocks, all_losses\n",
    "\n",
    "\n",
    "# --- CRN Simulation Engine ---\n",
    "def simulate_with_crn(ded,\n",
    "                    base_loss_ratio: float,\n",
    "                    loss_ratio_inflection: float,\n",
    "                    max_limit: float,\n",
    "                    revenue_shocks, loss_pool, n_years=1,\n",
    "                    initial_assets=INITIAL_ASSETS, pricers=None):\n",
    "    \"\"\"Simulate one static-Ded strategy across all CRN paths.\n",
    "\n",
    "    Uses the library's InsuranceProgram.process_claim() to correctly\n",
    "    allocate each loss through the insurance tower.\n",
    "\n",
    "    Loss amounts from the CRN pool are scaled by\n",
    "    (actual_revenue / REFERENCE_REVENUE)^FREQ_SCALING_EXPONENT so that\n",
    "    the loss burden grows proportionally with the company.  Premium is\n",
    "    repriced at actual revenue with the same exponent, keeping the\n",
    "    cost-of-risk consistent between insured and uninsured strategies.\n",
    "\n",
    "    Args:\n",
    "        ded: Deductible.\n",
    "        revenue_shocks: Pre-generated revenue shocks (n_paths x n_years).\n",
    "        loss_pool: Pre-generated loss events [path][year] -> List[LossEvent].\n",
    "        n_years: Simulation horizon.\n",
    "        initial_assets: Starting wealth.\n",
    "        pricers: Tuple of LayerPricers for premium calculation.\n",
    "            Uses DEFAULT_PRICERS if None (baseline loss assumptions).\n",
    "\n",
    "    Returns:\n",
    "        paths: array of shape (n_paths, n_years + 1) with asset values.\n",
    "    \"\"\"\n",
    "    n_paths = len(loss_pool)\n",
    "    paths = np.zeros((n_paths, n_years + 1))\n",
    "    paths[:, 0] = initial_assets\n",
    "\n",
    "    # Build program template and get fixed annual premium\n",
    "    if ded >= 100_000_000:\n",
    "        # \"No insurance\" -- skip tower entirely\n",
    "        annual_premium = 0.0\n",
    "        use_insurance = False\n",
    "        program_template = None\n",
    "    else:\n",
    "        program_template = make_program(ded,\n",
    "                                        base_loss_ratio,\n",
    "                                        loss_ratio_inflection,\n",
    "                                        max_limit,\n",
    "                                        pricers=pricers)\n",
    "        annual_premium = program_template.calculate_premium()\n",
    "        use_insurance = True\n",
    "\n",
    "    for i in range(n_paths):\n",
    "        assets = initial_assets\n",
    "        for t in range(n_years):\n",
    "            # Operating income with shared revenue shock\n",
    "            revenue = assets * ATR * np.exp(\n",
    "                REV_VOL * revenue_shocks[i, t] - 0.5 * REV_VOL**2\n",
    "            )\n",
    "            operating_income = revenue * OPERATING_MARGIN\n",
    "\n",
    "            # Scale CRN losses to current revenue (sqrt scaling)\n",
    "            loss_scale = (revenue / REFERENCE_REVENUE) ** FREQ_SCALING_EXPONENT\n",
    "\n",
    "            # Process losses through insurance tower\n",
    "            total_retained = 0.0\n",
    "            if use_insurance:\n",
    "                new_pricers = make_layer_pricers(cur_revenue=revenue)\n",
    "                program_update = make_program(ded,\n",
    "                                                base_loss_ratio,\n",
    "                                                loss_ratio_inflection,\n",
    "                                                max_limit,\n",
    "                                                pricers=new_pricers)\n",
    "                annual_premium = program_update.calculate_premium()\n",
    "                program = InsuranceProgram.create_fresh(program_update)\n",
    "                for event in loss_pool[i][t]:\n",
    "                    scaled_amount = event.amount * loss_scale\n",
    "                    result = program.process_claim(scaled_amount)\n",
    "                    total_retained += result.deductible_paid + result.uncovered_loss\n",
    "            else:\n",
    "                for event in loss_pool[i][t]:\n",
    "                    total_retained += event.amount * loss_scale\n",
    "\n",
    "            # Net income and asset update\n",
    "            assets = assets + operating_income - total_retained - annual_premium\n",
    "            assets = max(assets, 0.0)\n",
    "            paths[i, t + 1] = assets\n",
    "\n",
    "    return paths\n",
    "\n",
    "\n",
    "# Pre-generate the main CRN pool\n",
    "N_PATHS = 500\n",
    "N_YEARS = 1\n",
    "print(f\"\\nPre-generating CRN loss pool ({N_PATHS:,.0f} paths x {N_YEARS} years)...\")\n",
    "t0 = time.time()\n",
    "CRN_SHOCKS, CRN_LOSSES = generate_loss_pool(n_paths=N_PATHS, n_years=N_YEARS)\n",
    "print(f\"  Done in {time.time() - t0:.1f}s\")\n",
    "print(f\"  Shape: {CRN_SHOCKS.shape[0]:,} paths x {CRN_SHOCKS.shape[1]} years\")\n",
    "\n",
    "# Quick sanity: total losses per path-year\n",
    "_annual_totals = [\n",
    "    sum(e.amount for e in CRN_LOSSES[i][t])\n",
    "    for i in range(N_PATHS) for t in range(N_YEARS)\n",
    "]\n",
    "print(f\"  Mean annual loss: ${np.mean(_annual_totals):,.0f}\")\n",
    "del _annual_totals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1771697610322,
     "user": {
      "displayName": "Alex Filiakov",
      "userId": "03712643796933532488"
     },
     "user_tz": 300
    },
    "id": "sobol_is_infrastructure",
    "outputId": "08a86fb0-3a39-4394-f637-2a34df8f8834"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sobol loss pool validation (10,000 paths at reference revenue):\n",
      "  Mean annual loss:   $     835,429\n",
      "  Std dev:            $   2,902,205\n",
      "  Max single event:   $ 141,541,691\n",
      "  Paths with loss > $50M event: 4\n",
      "  Sobol dims: 23, Max events/path: 19\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# SOBOL QUASI-RANDOM LOSS GENERATION & IMPORTANCE SAMPLING\n",
    "# =====================================================\n",
    "# Quasi-random low-discrepancy sequences for variance reduction,\n",
    "# plus rejection-based importance sampling for tail events.\n",
    "\n",
    "from scipy.stats.qmc import Sobol as _Sobol\n",
    "from scipy.stats import norm as _sp_norm, poisson as _sp_poisson\n",
    "\n",
    "# Truncation limits for compound Poisson event counts\n",
    "MAX_ATTR_EVENTS  = 10\n",
    "MAX_LARGE_EVENTS = 6\n",
    "MAX_CAT_EVENTS   = 3\n",
    "MAX_EVENTS = MAX_ATTR_EVENTS + MAX_LARGE_EVENTS + MAX_CAT_EVENTS   # 19\n",
    "SOBOL_DIMS = 1 + (1 + MAX_ATTR_EVENTS) + (1 + MAX_LARGE_EVENTS) + (1 + MAX_CAT_EVENTS)  # 23\n",
    "\n",
    "\n",
    "def generate_sobol_loss_pool(n_paths, revenue, seed=0):\n",
    "    \"\"\"Generate loss scenarios using 23-dim Sobol quasi-random sequences.\n",
    "\n",
    "    Dimension layout:\n",
    "      0:       revenue shock  ~ N(0,1)\n",
    "      1-11:    attritional count (Poisson) + 10 severities (Lognormal)\n",
    "      12-18:   large count (Poisson) + 6 severities (Lognormal)\n",
    "      19-22:   cat count (Poisson) + 3 severities (Pareto)\n",
    "\n",
    "    Returns:\n",
    "        revenue_shocks:       (n_paths, 1) standard normal shocks\n",
    "        loss_amounts:         (n_paths, MAX_EVENTS) individual losses (0 = unused slot)\n",
    "        max_individual_loss:  (n_paths,) largest single loss per path\n",
    "    \"\"\"\n",
    "    scale = (revenue / REFERENCE_REVENUE) ** FREQ_SCALING_EXPONENT\n",
    "    attr_freq = ATTR_BASE_FREQ * scale\n",
    "    lg_freq   = LG_BASE_FREQ * scale\n",
    "    cat_freq  = CAT_BASE_FREQ * scale\n",
    "\n",
    "    attr_var = np.log(1 + ATTR_SEV_CV**2)\n",
    "    attr_mu  = np.log(ATTR_SEV_MEAN) - attr_var / 2\n",
    "    attr_sig = np.sqrt(attr_var)\n",
    "\n",
    "    lg_var = np.log(1 + LG_SEV_CV**2)\n",
    "    lg_mu  = np.log(LG_SEV_MEAN) - lg_var / 2\n",
    "    lg_sig = np.sqrt(lg_var)\n",
    "\n",
    "    sampler = _Sobol(d=SOBOL_DIMS, scramble=True, seed=seed)\n",
    "    m = int(np.ceil(np.log2(max(n_paths, 2))))\n",
    "    U = sampler.random(2**m)[:n_paths]\n",
    "    eps = 1e-10\n",
    "    col = 0\n",
    "\n",
    "    # Revenue shock ~ N(0,1)\n",
    "    revenue_shocks = _sp_norm.ppf(np.clip(U[:, col], eps, 1 - eps)).reshape(-1, 1)\n",
    "    col += 1\n",
    "\n",
    "    # Attritional: Poisson count + Lognormal severities\n",
    "    attr_n = _sp_poisson.ppf(np.clip(U[:, col], 0, 1 - eps), attr_freq).astype(int)\n",
    "    attr_n = np.minimum(attr_n, MAX_ATTR_EVENTS)\n",
    "    col += 1\n",
    "    attr_raw = np.exp(attr_mu + attr_sig * _sp_norm.ppf(\n",
    "        np.clip(U[:, col:col + MAX_ATTR_EVENTS], eps, 1 - eps)))\n",
    "    col += MAX_ATTR_EVENTS\n",
    "    attr_sevs = attr_raw * (np.arange(MAX_ATTR_EVENTS) < attr_n[:, None])\n",
    "\n",
    "    # Large: Poisson count + Lognormal severities\n",
    "    lg_n = _sp_poisson.ppf(np.clip(U[:, col], 0, 1 - eps), lg_freq).astype(int)\n",
    "    lg_n = np.minimum(lg_n, MAX_LARGE_EVENTS)\n",
    "    col += 1\n",
    "    lg_raw = np.exp(lg_mu + lg_sig * _sp_norm.ppf(\n",
    "        np.clip(U[:, col:col + MAX_LARGE_EVENTS], eps, 1 - eps)))\n",
    "    col += MAX_LARGE_EVENTS\n",
    "    lg_sevs = lg_raw * (np.arange(MAX_LARGE_EVENTS) < lg_n[:, None])\n",
    "\n",
    "    # Catastrophic: Poisson count + Pareto severities\n",
    "    cat_n = _sp_poisson.ppf(np.clip(U[:, col], 0, 1 - eps), cat_freq).astype(int)\n",
    "    cat_n = np.minimum(cat_n, MAX_CAT_EVENTS)\n",
    "    col += 1\n",
    "    cat_raw = CAT_SEV_XM / (1 - np.clip(\n",
    "        U[:, col:col + MAX_CAT_EVENTS], 0, 1 - eps)) ** (1 / CAT_SEV_ALPHA)\n",
    "    col += MAX_CAT_EVENTS\n",
    "    cat_sevs = cat_raw * (np.arange(MAX_CAT_EVENTS) < cat_n[:, None])\n",
    "\n",
    "    loss_amounts = np.hstack([attr_sevs, lg_sevs, cat_sevs])\n",
    "    max_individual_loss = np.max(loss_amounts, axis=1)\n",
    "\n",
    "    return revenue_shocks, loss_amounts, max_individual_loss\n",
    "\n",
    "\n",
    "def generate_is_pool_rejection(n_target, revenue, threshold=50_000_000,\n",
    "                               seed_offset=1_000_000, batch_size=260_000,\n",
    "                               verbose=False):\n",
    "    \"\"\"Generate importance-sampled tail paths via rejection sampling.\n",
    "\n",
    "    Generates Sobol batches, keeps only paths where the max individual\n",
    "    loss exceeds the threshold.  Continues until n_target paths collected.\n",
    "\n",
    "    Returns:\n",
    "        is_shocks:  (n_target, 1) revenue shocks for tail paths\n",
    "        is_losses:  (n_target, MAX_EVENTS) loss amounts for tail paths\n",
    "        p_tail:     estimated tail probability (acceptance rate)\n",
    "    \"\"\"\n",
    "    collected_shocks, collected_losses = [], []\n",
    "    n_generated = n_accepted = batch_num = 0\n",
    "\n",
    "    while n_accepted < n_target:\n",
    "        shocks, losses, max_loss = generate_sobol_loss_pool(\n",
    "            batch_size, revenue, seed=seed_offset + batch_num)\n",
    "        mask = max_loss > threshold\n",
    "        n_hit = int(mask.sum())\n",
    "        if n_hit > 0:\n",
    "            collected_shocks.append(shocks[mask])\n",
    "            collected_losses.append(losses[mask])\n",
    "            n_accepted += n_hit\n",
    "        n_generated += len(max_loss)\n",
    "        batch_num += 1\n",
    "        if verbose and batch_num % 10 == 0:\n",
    "            print(f\"  IS rejection: {n_accepted}/{n_target} from \"\n",
    "                  f\"{n_generated:,} ({n_accepted/max(n_generated,1):.6%})\")\n",
    "\n",
    "    is_shocks = np.vstack(collected_shocks)[:n_target]\n",
    "    is_losses = np.vstack(collected_losses)[:n_target]\n",
    "    p_tail = n_accepted / n_generated\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"  IS complete: {n_target} tail paths from {n_generated:,} \"\n",
    "              f\"({p_tail:.6%}, {batch_num} batches)\")\n",
    "\n",
    "    return is_shocks, is_losses, p_tail\n",
    "\n",
    "\n",
    "def compute_stratified_stats(W_T_main, W_T_is, tail_mask_main, initial_assets):\n",
    "    \"\"\"Combine main and IS pools via stratified estimation.\n",
    "\n",
    "    The tail probability is estimated from the main pool (unbiased).\n",
    "    Tail-stratum statistics use both main-pool tail paths and IS paths\n",
    "    for improved precision in the extreme-loss regime.\n",
    "\n",
    "    Returns:\n",
    "        (growth_rate, growth_vol, ruin_prob)\n",
    "    \"\"\"\n",
    "    N_main = len(W_T_main)\n",
    "    p_tail = tail_mask_main.sum() / N_main\n",
    "\n",
    "    log_g_main = np.log(np.maximum(W_T_main, 1.0) / initial_assets)\n",
    "    log_g_is   = np.log(np.maximum(W_T_is, 1.0) / initial_assets)\n",
    "\n",
    "    normal_mask = ~tail_mask_main\n",
    "    if normal_mask.any():\n",
    "        lg_n = log_g_main[normal_mask]\n",
    "        mean_n, meansq_n = lg_n.mean(), (lg_n**2).mean()\n",
    "        ruin_n = float((W_T_main[normal_mask] <= 0).mean())\n",
    "    else:\n",
    "        mean_n = meansq_n = ruin_n = 0.0\n",
    "\n",
    "    # Tail stratum: main-pool tail paths + IS paths\n",
    "    if tail_mask_main.any():\n",
    "        lg_t = np.concatenate([log_g_main[tail_mask_main], log_g_is])\n",
    "        wt_t = np.concatenate([W_T_main[tail_mask_main], W_T_is])\n",
    "    else:\n",
    "        lg_t, wt_t = log_g_is, W_T_is\n",
    "\n",
    "    mean_t, meansq_t = lg_t.mean(), (lg_t**2).mean()\n",
    "    ruin_t = float((wt_t <= 0).mean())\n",
    "\n",
    "    growth_rate = (1 - p_tail) * mean_n + p_tail * mean_t\n",
    "    E_sq = (1 - p_tail) * meansq_n + p_tail * meansq_t\n",
    "    growth_vol  = np.sqrt(max(E_sq - growth_rate**2, 0.0))\n",
    "    ruin_prob   = (1 - p_tail) * ruin_n + p_tail * ruin_t\n",
    "\n",
    "    return growth_rate, growth_vol, ruin_prob\n",
    "\n",
    "\n",
    "# Quick validation of the Sobol generator\n",
    "_val_shocks, _val_losses, _val_max = generate_sobol_loss_pool(10_000, REFERENCE_REVENUE, seed=99)\n",
    "_val_totals = _val_losses.sum(axis=1)\n",
    "print(f\"\\nSobol loss pool validation (10,000 paths at reference revenue):\")\n",
    "print(f\"  Mean annual loss:   ${np.mean(_val_totals):>12,.0f}\")\n",
    "print(f\"  Std dev:            ${np.std(_val_totals):>12,.0f}\")\n",
    "print(f\"  Max single event:   ${np.max(_val_max):>12,.0f}\")\n",
    "print(f\"  Paths with loss > $50M event: {(np.max(_val_losses, axis=1) > 50_000_000).sum()}\")\n",
    "print(f\"  Sobol dims: {SOBOL_DIMS}, Max events/path: {MAX_EVENTS}\")\n",
    "del _val_shocks, _val_losses, _val_max, _val_totals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Plotting the Severity Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III: Estimating Attritional and Large Loss Frequency and Severity with Shadow Moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part IV: Estimating the Tail Shape from Industry Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part V: Comparing the \"True\" Distribution with MLE Estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V6E1",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
