{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af904fa9",
   "metadata": {},
   "source": [
    "# Process Simulation Results"
   ]
  },
  {
   "cell_type": "code",
   "id": "1d7nmz1rxwg",
   "source": "\"\"\"Google Colab setup: mount Drive and install package dependencies.\n\nRun this cell first. If prompted to restart the runtime, do so, then re-run all cells.\nThis cell is a no-op when running locally.\n\"\"\"\nimport sys, os\nif 'google.colab' in sys.modules:\n    from google.colab import drive\n    drive.mount('/content/drive')\n\n    NOTEBOOK_DIR = '/content/drive/My Drive/Colab Notebooks/ei_notebooks/research/reproducible_research_2026_02_02_basic_volatility'\n\n    os.chdir(NOTEBOOK_DIR)\n    if NOTEBOOK_DIR not in sys.path:\n        sys.path.append(NOTEBOOK_DIR)\n\n    !pip install ergodic-insurance -q 2>&1 | tail -3\n    print('\\nSetup complete. If you see numpy/scipy import errors below,')\n    print('restart the runtime (Runtime > Restart runtime) and re-run all cells.')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "e515f889",
   "metadata": {},
   "source": [
    "## Peek at a Sample Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd9fb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle\n",
    "\n",
    "results_dir = r\"D:\\2026-01-31 Vol\"\n",
    "\n",
    "sample_file = \"Cap (5M) -    ATR (1.0) -    EBITABL (0.125) -    XS_Kurt (3032) -    Ded (100K) -    LR (0.7) -    Vol (0.15) -    250K Sims -    50 Yrs.pkl\"\n",
    "\n",
    "file_path = os.path.join(results_dir, sample_file)\n",
    "with open(file_path, \"rb\") as f:\n",
    "    loaded_sample_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2175fa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(loaded_sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6974a29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_sample_data.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88e419f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d4518a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(loaded_sample_data.final_assets[loaded_sample_data.final_assets <= 10_000]) / len(loaded_sample_data.final_assets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4a5eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(loaded_sample_data.final_assets)[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c5a12c",
   "metadata": {},
   "source": [
    "## Load All Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfff9377",
   "metadata": {},
   "source": [
    "### Get all the file paths and set up results parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6e9c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from scipy.stats import skew, kurtosis\n",
    "from time import perf_counter\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from ergodic_insurance.monte_carlo import MonteCarloResults\n",
    "\n",
    "\n",
    "def _parse_number(text):\n",
    "    text = text.strip().replace(\",\", \"\")\n",
    "    m = re.fullmatch(r'([+-]?\\d+(?:\\.\\d+)?)([KMB])?$', text, re.I)\n",
    "    if not m:\n",
    "        # fallback: plain int/float or leave as-is\n",
    "        try:\n",
    "            return int(text)\n",
    "        except ValueError:\n",
    "            try:\n",
    "                return float(text)\n",
    "            except ValueError:\n",
    "                return text\n",
    "    num = float(m.group(1))\n",
    "    mult = {\"K\": 1_000, \"M\": 1_000_000, \"B\": 1_000_000_000}.get((m.group(2) or \"\").upper(), 1)\n",
    "    val = num * mult\n",
    "    return int(val) if val.is_integer() else val\n",
    "\n",
    "\n",
    "def parse_config_key(key: str) -> dict:\n",
    "    parts = re.split(r\"\\s*-\\s*\", key.strip())\n",
    "    out = {}\n",
    "    for part in parts:\n",
    "        if not part:\n",
    "            continue\n",
    "\n",
    "        # e.g. \"Cap (100M)\"\n",
    "        m = re.match(r\"^([A-Za-z_%]+)\\s*\\(\\s*([^)]+)\\s*\\)$\", part)\n",
    "        if m:\n",
    "            out[m.group(1)] = _parse_number(m.group(2))\n",
    "            continue\n",
    "\n",
    "        # e.g. \"0K Sims\" or \"50 Yrs\"\n",
    "        m = re.match(r\"^([+-]?\\d+(?:\\.\\d+)?)\\s*([KMB])?\\s*([A-Za-z_]+)$\", part)\n",
    "        if m:\n",
    "            value = _parse_number((m.group(1) or \"\") + (m.group(2) or \"\"))\n",
    "            out[m.group(3)] = value\n",
    "            continue\n",
    "\n",
    "        # flags like \"NOINS\"\n",
    "        if part.upper() == \"NOINS\":\n",
    "            out[\"NOINS\"] = True\n",
    "        else:\n",
    "            out[\"NOINS\"] = False\n",
    "\n",
    "    return out\n",
    "\n",
    "results_dir = Path(results_dir)\n",
    "\n",
    "# sample_files = \"Cap (10M) - Ded (250K)*.pkl\" # Load a subset for testing\n",
    "sample_files = \"*.pkl\" # Load all files\n",
    "\n",
    "pkl_paths = sorted(results_dir.glob(sample_files))\n",
    "\n",
    "qs = np.arange(0.01, 1.00, 0.01) # Growth Rate Quantiles\n",
    "\n",
    "all_configurations = {}\n",
    "\n",
    "# Create an empty DataFrame with specified columns\n",
    "df_qs = [0.01, 0.05] + np.arange(0.1, 0.8, 0.1).tolist() + [0.75, 0.8, 0.85, 0.9, 0.95, 0.99]\n",
    "column_names = [key for key in parse_config_key(sample_file).keys() if key != 'Sims']\n",
    "column_names += ['growth_rate_mean', 'growth_rate_std', 'growth_rate_cv', 'growth_rate_skewness',\n",
    "                'growth_rate_kurtosis', 'growth_rate_hyperskewness', 'growth_rate_hyperkurtosis']\n",
    "column_names += [f'growth_rate_q{q*100:.0f}' for q in df_qs]\n",
    "df_cte = [0.005, 0.01, 0.05]\n",
    "column_names += [f'growth_rate_cte_q{q*100:.1f}' for q in df_cte]\n",
    "column_names += [f'risk_of_ruin_{age}' for age in loaded_sample_data.ruin_probability.keys()]\n",
    "# column_names += ['risk_of_ruin']\n",
    "all_df = pd.DataFrame(columns=column_names)\n",
    "column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78f3359",
   "metadata": {},
   "source": [
    "### Parse data for a sample row (test the parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15c32b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_row_data(sample_file, loaded_sample_data):\n",
    "    row_data = {key: val for (key, val) in parse_config_key(sample_file).items() if key != 'Sims'}\n",
    "    row_data['growth_rate_mean'] = loaded_sample_data.growth_rates.mean()\n",
    "    row_data['growth_rate_std'] = loaded_sample_data.growth_rates.std(ddof=1)\n",
    "    row_data['growth_rate_cv'] = row_data['growth_rate_std'] / row_data['growth_rate_mean']\n",
    "    row_data['growth_rate_skewness'] = skew(loaded_sample_data.growth_rates)\n",
    "    row_data['growth_rate_kurtosis'] = kurtosis(loaded_sample_data.growth_rates)  # Excess (-3)\n",
    "    standardized_data = (loaded_sample_data.growth_rates - row_data['growth_rate_mean']) / row_data['growth_rate_std']\n",
    "    row_data['growth_rate_hyperskewness'] = np.mean(standardized_data**5)\n",
    "    row_data['growth_rate_hyperkurtosis'] = np.mean(standardized_data**6)\n",
    "    row_data = row_data | {f'growth_rate_q{q*100:.0f}': val for q, val in zip(df_qs, np.quantile(loaded_sample_data.growth_rates, df_qs))}\n",
    "    row_data = row_data | {f'growth_rate_cte_q{q*100:.1f}': val for q, val in zip(df_cte, np.quantile(loaded_sample_data.growth_rates, df_cte))}\n",
    "    row_data = row_data | {f'risk_of_ruin_{age}': ror for (age, ror) in loaded_sample_data.ruin_probability.items()}\n",
    "    return row_data\n",
    "    # row_data['risk_of_ruin'] = len(loaded_sample_data.final_assets[loaded_sample_data.final_assets <= 10_000]) / len(loaded_sample_data.final_assets)\n",
    "\n",
    "sample_row_data = parse_row_data(sample_file, loaded_sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8be75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from scipy.stats import skew, kurtosis\n",
    "from time import perf_counter\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from ergodic_insurance.monte_carlo import MonteCarloResults\n",
    "\n",
    "parquet_df = r\"cache\\\\all_df.parquet\"\n",
    "parsed_params_file = r\"cache\\\\parsed_params_by_key.pkl\"\n",
    "\n",
    "# Ensure cache folder exists (to_parquet won't create it)\n",
    "Path(parquet_df).parent.mkdir(parents=True, exist_ok=True)\n",
    "Path(parsed_params_file).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "all_df = pd.DataFrame()\n",
    "\n",
    "try:\n",
    "    # Attempt to load from parquet file\n",
    "    all_df = pd.read_parquet(parquet_df, engine='pyarrow')\n",
    "    parsed_params_by_key = pd.read_pickle(parsed_params_file)\n",
    "\n",
    "    # If the parquet was created before the df-building fix, it may have 0 rows/cols.\n",
    "    # Treat that as cache-miss and rebuild from pickles.\n",
    "    if len(all_df) == 0 and len(parsed_params_by_key) > 0 and pkl_paths:\n",
    "        raise ValueError(\"Cached parquet is empty; rebuilding from pickle files.\")\n",
    "\n",
    "    print(f\"Loaded {len(all_df)} rows from {parquet_df} and {len(parsed_params_by_key)} configurations from {parsed_params_file}.\")\n",
    "except Exception as e:\n",
    "    print(f\"Parsing simulation pickle files (cache miss: {e})\")\n",
    "\n",
    "    if not pkl_paths:\n",
    "        print(f\"No pickle files found in {results_dir}.\")\n",
    "    else:\n",
    "        try:\n",
    "            iterator = tqdm(pkl_paths, desc=\"Processing pickle files\", unit=\"file\")\n",
    "        except Exception:\n",
    "            iterator = pkl_paths  # fallback without progress bar\n",
    "\n",
    "        start_time = perf_counter()\n",
    "        rows = []\n",
    "        all_configurations = {}\n",
    "        for idx, path in enumerate(iterator, 1):\n",
    "            if idx > 1:\n",
    "                elapsed = perf_counter() - start_time\n",
    "                avg = elapsed / (idx - 1)\n",
    "                remaining = avg * (len(pkl_paths) - (idx - 1))\n",
    "                if hasattr(iterator, \"set_postfix\"):\n",
    "                    iterator.set_postfix(avg_s=f\"{avg:.2f}\", eta_s=f\"{remaining:.1f}\")\n",
    "            try:\n",
    "                with open(path, \"rb\") as f:\n",
    "                    one_config = pickle.load(f)\n",
    "                    growth_rate = one_config.growth_rates.mean()\n",
    "                    growth_rate_ci = {str(q): val for q, val in zip(qs, np.quantile(one_config.growth_rates, qs))}\n",
    "                    ror = one_config.ruin_probability\n",
    "\n",
    "                    # Update the dictionary\n",
    "                    all_configurations[path.stem] = {\n",
    "                        \"growth_rate\": growth_rate,\n",
    "                        \"growth_rate_ci\": growth_rate_ci,\n",
    "                        \"risk_of_ruin\": ror,\n",
    "                        # \"annual_losses\": one_config.annual_losses,\n",
    "                    }\n",
    "\n",
    "                    # Collect the row data (don't concat into a 0-column df)\n",
    "                    parsed_row = parse_row_data(path.stem, one_config)\n",
    "                    rows.append(parsed_row)\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping {path.name}: {e}\")\n",
    "\n",
    "        print(f\"Loaded {len(all_configurations)} pickle files into all_configurations.\")\n",
    "\n",
    "        # Build DataFrame once (fast + preserves columns)\n",
    "        all_df = pd.DataFrame(rows)\n",
    "        if \"column_names\" in globals() and isinstance(column_names, list) and column_names:\n",
    "            # Keep your preferred column order, but don't drop unexpected columns\n",
    "            for c in column_names:\n",
    "                if c not in all_df.columns:\n",
    "                    all_df[c] = np.nan\n",
    "            extra_cols = [c for c in all_df.columns if c not in column_names]\n",
    "            all_df = all_df[column_names + extra_cols]\n",
    "\n",
    "        print(f\"all_df shape before write: {all_df.shape}\")\n",
    "        print(f\"First columns: {list(all_df.columns)[:10]}\")\n",
    "\n",
    "        # parsed_params_by_key merges parsed filename params with computed metrics\n",
    "        parsed_params_by_key = {k: (parse_config_key(k) | all_configurations[k]) for k in all_configurations.keys()}\n",
    "\n",
    "        # Replace 'None' string values in X_Th_%le with 0.0 and coerce to float if possible\n",
    "        if 'X_Th_%le' in all_df.columns:\n",
    "            all_df.loc[all_df['X_Th_%le'].eq('None'), 'X_Th_%le'] = 0.0\n",
    "            try:\n",
    "                all_df['X_Th_%le'] = all_df['X_Th_%le'].astype(float)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # Save all_df to a parquet file\n",
    "        print(f\"Writing {len(all_df)} rows to {parquet_df}...\")\n",
    "        all_df.to_parquet(parquet_df, engine='pyarrow', index=False)\n",
    "\n",
    "        # Save parsed_params_by_key to a pickle file\n",
    "        with open(parsed_params_file, \"wb\") as f:\n",
    "            pickle.dump(parsed_params_by_key, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print(f\"Wrote {len(parsed_params_by_key)} configurations to {parsed_params_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32ffa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No Limits are expected here since it's unlimited policies\n",
    "# Replace the first failing comprehension with this safe version:\n",
    "all_lims = sorted({sc['Pol_Lim'] for sc in parsed_params_by_key.values() if 'Pol_Lim' in sc})\n",
    "all_lims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6c582c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the first failing comprehension with this safe version:\n",
    "all_lims = sorted({sc['Ded'] for sc in parsed_params_by_key.values() if 'Ded' in sc})\n",
    "all_lims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801ad10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the first failing comprehension with this safe version:\n",
    "all_keys = set().union(*(sc.keys() for sc in parsed_params_by_key.values()))\n",
    "all_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84d256f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in all_keys:\n",
    "    all_key_vals = sorted({str(sc[key]) for sc in parsed_params_by_key.values() if key in sc and type(sc[key]) not in (list, dict, set)})\n",
    "    if all_key_vals != []:\n",
    "        print(f\"{key}: {all_key_vals}\")\n",
    "    all_key_vals = {type(sc[key]) for sc in parsed_params_by_key.values() if key in sc and type(sc[key]) in (list, dict, set)}\n",
    "    if all_key_vals != set():\n",
    "        print(f\"{key}: {all_key_vals}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e65f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "ins_only_vals = [val for val in parsed_params_by_key.values() if val.get('NOINS', False) is False]\n",
    "len(ins_only_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdfd6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = sorted(set(c['LR'] for c in ins_only_vals))\n",
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4091de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset to a single volatility parameter for plotting (like in the full sim)\n",
    "parsed_params_by_key = {k: v for k, v in parsed_params_by_key.items() if v['Vol'] == 0.15}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d4a4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No-Insurance Scenarios\n",
    "noins_scenarios = {k: v for k, v in parsed_params_by_key.items() if v.get(\"NOINS\")}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8db4be7",
   "metadata": {},
   "source": [
    "## Display Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c179333",
   "metadata": {},
   "source": [
    "### Asset Turnover Ratio vs Time Average Growth Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316ef73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 3: Small multiples with discrete operating margin values\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "all_scens = parsed_params_by_key | noins_scenarios\n",
    "\n",
    "if not isinstance(globals().get(\"all_scens\"), dict) or not all_scens:\n",
    "    raise RuntimeError(\"all_scens is missing or empty.\")\n",
    "\n",
    "def to_float(x):\n",
    "    try:\n",
    "        return float(x)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "# Parse scenarios\n",
    "sc_list = []\n",
    "for k, v in all_scens.items():\n",
    "    cap = to_float(v.get(\"Cap\"))\n",
    "    atr = to_float(v.get(\"ATR\"))\n",
    "    ebit = to_float(v.get(\"EBITABL\"))\n",
    "    gr = to_float(v.get(\"growth_rate\"))\n",
    "    ded = to_float(v.get(\"Ded\"))\n",
    "    lr_raw = v.get(\"LR\", np.nan)\n",
    "    lr_f = to_float(lr_raw)\n",
    "    if np.isfinite(lr_f) and lr_f > 1.0:\n",
    "        lr_f = lr_f / 100.0\n",
    "    noins = bool(v.get(\"NOINS\", False))\n",
    "\n",
    "    if not np.isfinite(cap) or not np.isfinite(gr) or not np.isfinite(atr) or not np.isfinite(ebit):\n",
    "        continue\n",
    "    if not noins and not np.isfinite(ded):\n",
    "        continue\n",
    "    \n",
    "    sc_list.append({\n",
    "        \"cap\": cap,\n",
    "        \"atr\": atr,\n",
    "        \"ebit\": ebit,\n",
    "        \"ded\": ded,\n",
    "        \"lr\": lr_f if np.isfinite(lr_f) else np.nan,\n",
    "        \"noins\": noins,\n",
    "        \"gr\": gr\n",
    "    })\n",
    "\n",
    "if not sc_list:\n",
    "    raise RuntimeError(\"No usable scenarios after filtering.\")\n",
    "\n",
    "sc_list\n",
    "\n",
    "def fmt_amount(n):\n",
    "    n = float(n)\n",
    "    if n >= 1_000_000_000: return f\"{n/1_000_000_000:.0f}B\"\n",
    "    if n >= 1_000_000:     return f\"{n/1_000_000:.0f}M\"\n",
    "    if n >= 1_000:         return f\"{n/1_000:.0f}K\"\n",
    "    return f\"{n:.0f}\"\n",
    "\n",
    "def ded_label(d):\n",
    "    d = float(d)\n",
    "    if d >= 1_000_000_000: return f\"Ded {d/1_000_000_000:.0f}B\"\n",
    "    if d >= 1_000_000:     return f\"Ded {d/1_000_000:.0f}M\"\n",
    "    if d >= 1_000:         return f\"Ded {d/1_000:.0f}K\"\n",
    "    return f\"Ded {d:.0f}\"\n",
    "\n",
    "# Choose 3 LR levels, 5 Cap levels, and 3-4 EBIT levels\n",
    "ins_rows = [s for s in sc_list if not s[\"noins\"] and np.isfinite(s[\"lr\"])]\n",
    "if not ins_rows:\n",
    "    raise RuntimeError(\"No insurance scenarios with LR available.\")\n",
    "\n",
    "unique_lr = np.unique(np.round([s[\"lr\"] for s in ins_rows], 3))\n",
    "preferred_lr = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "lr_levels = [lvl for lvl in preferred_lr if np.any(np.isclose(unique_lr, lvl, atol=1e-3))]\n",
    "if len(lr_levels) < 3:\n",
    "    for u in unique_lr:\n",
    "        if not any(np.isclose(u, l, atol=1e-3) for l in lr_levels):\n",
    "            lr_levels.append(float(u))\n",
    "        if len(lr_levels) >= 3:\n",
    "            break\n",
    "lr_levels = lr_levels[:3]\n",
    "\n",
    "caps_all = sorted(np.unique([s[\"cap\"] for s in sc_list]))\n",
    "if len(caps_all) <= 5:\n",
    "    cap_levels = caps_all\n",
    "else:\n",
    "    idxs = np.round(np.linspace(0, len(caps_all) - 1, 5)).astype(int)\n",
    "    cap_levels = [caps_all[i] for i in idxs]\n",
    "cap_levels = cap_levels[:5]\n",
    "\n",
    "# EBIT levels: choose 3-4 evenly spaced values\n",
    "ebit_all = sorted(np.unique([s[\"ebit\"] for s in sc_list]))\n",
    "ebit_levels = ebit_all\n",
    "\n",
    "# Deductible levels\n",
    "ded_uni = sorted(np.unique([s[\"ded\"] for s in ins_rows]))\n",
    "ded_levels = ded_uni\n",
    "\n",
    "cmap = plt.cm.viridis\n",
    "ded_colors = cmap(np.linspace(0.1, 0.9, max(1, len(ded_levels))))\n",
    "\n",
    "# Build baseline (NOINS) for each (Cap, EBIT, ATR) combination\n",
    "baseline_map = {}  # baseline_map[(cap, ebit, atr)] = growth_rate\n",
    "for s in sc_list:\n",
    "    if s[\"noins\"]:\n",
    "        key = (s[\"cap\"], round(s[\"ebit\"], 6), round(s[\"atr\"], 6))\n",
    "        baseline_map[key] = s[\"gr\"]\n",
    "\n",
    "# Create 3D grid: nrows (LR) × ncols (Cap) × n_ebit_panels (EBIT)\n",
    "# We'll use a single large figure with subplots arranged as: \n",
    "# Outer grid = LR (rows) × EBIT (columns within each LR)\n",
    "# Inner grid = Cap (columns within each EBIT panel)\n",
    "\n",
    "# Actually, let's make it: nrows=LR, ncols=Cap, and create separate figures for each EBIT\n",
    "# OR: nrows=LR, ncols=Cap*EBIT (nested)\n",
    "\n",
    "# For simplicity, let's create: nrows=LR, ncols=EBIT, and use different Cap as separate line styles or colors\n",
    "# Actually, the cleanest is: create a mega-grid with LR × (Cap × EBIT)\n",
    "\n",
    "# Let's use a simpler approach: LR rows, Cap columns, and create multiple figures (one per EBIT)\n",
    "# OR: LR rows, EBIT super-columns (with Cap sub-columns inside)\n",
    "\n",
    "# Most readable: Create separate figures for each EBIT level\n",
    "for ebit_idx, ebit_val in enumerate(ebit_levels):\n",
    "    nrows = len(lr_levels)\n",
    "    ncols = len(cap_levels)\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(4.0*ncols, 3.0*nrows), \n",
    "                             sharex=True, sharey=True)\n",
    "    if nrows == 1 and ncols == 1:\n",
    "        axes = np.array([[axes]])\n",
    "    elif nrows == 1:\n",
    "        axes = np.array([axes])\n",
    "    elif ncols == 1:\n",
    "        axes = axes[:, np.newaxis]\n",
    "    \n",
    "    for j, cap in enumerate(cap_levels):\n",
    "        axes[0, j].set_title(f\"Cap = {fmt_amount(cap)}\", fontsize=10)\n",
    "    for i, lr in enumerate(lr_levels):\n",
    "        axes[i, 0].set_ylabel(f\"Growth Lift (LR={lr:.2f})\", fontsize=9)\n",
    "    \n",
    "    for i, lr in enumerate(lr_levels):\n",
    "        for j, cap in enumerate(cap_levels):\n",
    "            ax = axes[i, j]\n",
    "            ax.axhline(0.0, color=\"gray\", lw=1.0, ls=\"--\", alpha=0.6, zorder=1)\n",
    "            \n",
    "            for color, dval in zip(ded_colors, ded_levels):\n",
    "                # Collect points for this (lr, cap, ebit, ded)\n",
    "                atrs_list = []\n",
    "                deltas_list = []\n",
    "                \n",
    "                for s in sc_list:\n",
    "                    if (not s[\"noins\"]\n",
    "                        and np.isfinite(s[\"lr\"]) and np.isclose(s[\"lr\"], lr, atol=1e-3)\n",
    "                        and np.isclose(s[\"cap\"], cap, atol=1e-9)\n",
    "                        and np.isclose(s[\"ebit\"], ebit_val, atol=1e-6)\n",
    "                        and np.isclose(s[\"ded\"], dval, atol=1e-9)):\n",
    "                        \n",
    "                        key_atr = round(s[\"atr\"], 6)\n",
    "                        baseline_key = (cap, round(ebit_val, 6), key_atr)\n",
    "                        if baseline_key in baseline_map:\n",
    "                            baseline_gr = baseline_map[baseline_key]\n",
    "                            delta = s[\"gr\"] - baseline_gr\n",
    "                            atrs_list.append(s[\"atr\"])\n",
    "                            deltas_list.append(delta)\n",
    "                \n",
    "                if not atrs_list:\n",
    "                    continue\n",
    "                \n",
    "                # Sort by ATR\n",
    "                atrs_arr = np.array(atrs_list)\n",
    "                deltas_arr = np.array(deltas_list)\n",
    "                order = np.argsort(atrs_arr)\n",
    "                \n",
    "                ax.plot(atrs_arr[order], deltas_arr[order], \n",
    "                       color=color, lw=2.0, alpha=0.95, \n",
    "                       marker='o', ms=3, label=ded_label(dval), zorder=3)\n",
    "            \n",
    "            if i == nrows - 1:\n",
    "                ax.set_xlabel(\"Asset Turnover Ratio\", fontsize=9)\n",
    "            ax.grid(True, alpha=0.25)\n",
    "            ax.tick_params(labelsize=8)\n",
    "    \n",
    "    # Legend\n",
    "    legend_elems = [Line2D([0], [0], color=ded_colors[k], lw=2.0, \n",
    "                           label=ded_label(ded_levels[k]))\n",
    "                    for k in range(len(ded_levels))]\n",
    "    legend_elems.append(Line2D([0], [0], color=\"gray\", lw=1.0, ls=\"--\", \n",
    "                               label=\"Uninsured\"))\n",
    "    fig.legend(legend_elems, [h.get_label() for h in legend_elems],\n",
    "               loc=\"lower center\", ncol=min(6, len(legend_elems)), \n",
    "               frameon=False, fontsize=9)\n",
    "    \n",
    "    fig.suptitle(\"Asset Turnover Ratio vs Time-Average Growth Lift \" + \n",
    "                 f\"(Rows: Loss Ratio, Cols: Capital, Operating Margin = {ebit_val:.1%})\",\n",
    "                 y=0.995, fontsize=14)\n",
    "    fig.tight_layout(rect=[0, 0.06, 1, 0.965])\n",
    "    \n",
    "    output_dir = Path(\"output\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out = output_dir / f\"option3_multiples_ebit_{ebit_val:.3f}.png\"\n",
    "    fig.savefig(out, dpi=300)\n",
    "    print(f\"Saved: {out}\")\n",
    "\n",
    "plt.show()\n",
    "print(f\"Created {len(ebit_levels)} figures for Option 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e912ff",
   "metadata": {},
   "source": [
    "## Plot Growth Lift by Initial Cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d2059b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single line graph: Growth Lift vs Initial Capitalization\n",
    "# Parameters: LR=0.7, ATR=1.0, Operating Margin=10%\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from pathlib import Path\n",
    "\n",
    "# Filter parameters\n",
    "TARGET_LR = 0.7\n",
    "TARGET_ATR = 1.0\n",
    "TARGET_EBIT = 0.125\n",
    "TARGET_CAPS = [5_000_000, 10_000_000, 25_000_000]\n",
    "\n",
    "# Use same color scheme as previous visualizations\n",
    "ded_uni = sorted(np.unique([s[\"ded\"] for s in sc_list if not s[\"noins\"] and np.isfinite(s[\"lr\"])]))\n",
    "cmap = plt.cm.viridis\n",
    "ded_colors = cmap(np.linspace(0.1, 0.9, max(1, len(ded_uni))))\n",
    "\n",
    "# Build baseline map for uninsured scenarios\n",
    "baseline_by_cap = {}  # baseline_by_cap[cap] = growth_rate\n",
    "for s in sc_list:\n",
    "    if (s[\"noins\"] \n",
    "        and np.isclose(s[\"atr\"], TARGET_ATR, atol=1e-3)\n",
    "        and np.isclose(s[\"ebit\"], TARGET_EBIT, atol=1e-6)):\n",
    "        baseline_by_cap[s[\"cap\"]] = s[\"gr\"]\n",
    "\n",
    "# Collect data for each deductible level\n",
    "data_by_ded = {}  # data_by_ded[ded] = [(cap, growth_lift), ...]\n",
    "\n",
    "for ded_val in ded_uni:\n",
    "    caps_list = []\n",
    "    lifts_list = []\n",
    "    \n",
    "    for cap in TARGET_CAPS:\n",
    "        # Find matching scenario\n",
    "        for s in sc_list:\n",
    "            if (not s[\"noins\"]\n",
    "                and np.isfinite(s[\"lr\"]) and np.isclose(s[\"lr\"], TARGET_LR, atol=1e-3)\n",
    "                and np.isclose(s[\"cap\"], cap, atol=1e-3)\n",
    "                and np.isclose(s[\"atr\"], TARGET_ATR, atol=1e-3)\n",
    "                and np.isclose(s[\"ebit\"], TARGET_EBIT, atol=1e-6)\n",
    "                and np.isclose(s[\"ded\"], ded_val, atol=1e-3)):\n",
    "                \n",
    "                # Calculate growth lift\n",
    "                if cap in baseline_by_cap:\n",
    "                    baseline_gr = baseline_by_cap[cap]\n",
    "                    lift = s[\"gr\"] - baseline_gr\n",
    "                    caps_list.append(cap)\n",
    "                    lifts_list.append(lift)\n",
    "                    break\n",
    "    \n",
    "    if caps_list:\n",
    "        data_by_ded[ded_val] = (np.array(caps_list), np.array(lifts_list))\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot uninsured baseline (zero lift)\n",
    "ax.axhline(0.0, color=\"gray\", lw=2.0, ls=\"--\", alpha=0.7, label=\"Uninsured\", zorder=1)\n",
    "\n",
    "# Plot each deductible level\n",
    "for i, (ded_val, (caps, lifts)) in enumerate(sorted(data_by_ded.items())):\n",
    "    color = ded_colors[ded_uni.index(ded_val)]\n",
    "    \n",
    "    # Sort by cap for clean line\n",
    "    order = np.argsort(caps)\n",
    "    caps_sorted = caps[order]\n",
    "    lifts_sorted = lifts[order]\n",
    "    \n",
    "    # Format label\n",
    "    if ded_val >= 1_000_000:\n",
    "        label = f\"${ded_val/1_000_000:.0f}M Deductible\"\n",
    "    elif ded_val >= 1_000:\n",
    "        label = f\"${ded_val/1_000:.0f}K Deductible\"\n",
    "    else:\n",
    "        label = f\"${ded_val:.0f} Deductible\"\n",
    "    \n",
    "    ax.plot(caps_sorted / 1_000_000, lifts_sorted, \n",
    "           color=color, lw=2.5, alpha=0.95, \n",
    "           marker='o', ms=8, label=label, zorder=3)\n",
    "\n",
    "# Formatting\n",
    "ax.set_xlabel(\"Initial Capitalization ($M)\", fontsize=12)\n",
    "ax.set_ylabel(\"Growth Lift (vs Uninsured)\", fontsize=12)\n",
    "ax.set_title(\"Growth Lift by Initial Capitalization\\n\" + \n",
    "            f\"(Loss Ratio = {TARGET_LR:.0%}, Asset Turnover = {TARGET_ATR:.1f}, Operating Margin = {TARGET_EBIT:.0%})\",\n",
    "            fontsize=14, pad=20)\n",
    "\n",
    "# Grid and styling\n",
    "ax.grid(True, alpha=0.3, linestyle='--')\n",
    "ax.tick_params(labelsize=10)\n",
    "\n",
    "# Legend\n",
    "ax.legend(loc='best', frameon=True, fontsize=10, framealpha=0.95)\n",
    "\n",
    "# Format x-axis to show capitalization values\n",
    "ax.set_xticks([cap / 1_000_000 for cap in TARGET_CAPS])\n",
    "ax.set_xticklabels([f\"${cap/1_000_000:.0f}M\" for cap in TARGET_CAPS])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "output_dir = Path(\"output\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "out_path = output_dir / \"growth_lift_by_capitalization.png\"\n",
    "fig.savefig(out_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"Saved: {out_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pzdmfa2tl5",
   "metadata": {},
   "source": [
    "## Build Dashboard Cache\n",
    "\n",
    "Build comprehensive cache for interactive EDA dashboards (Space Explorer and Paired Path Browser).\n",
    "This extracts time-series percentiles, distribution data, interesting simulation IDs, and CRN pairing maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ve99tux9dz",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from time import perf_counter\n",
    "\n",
    "dashboard_cache_file = r\"cache\\dashboard_cache.pkl\"\n",
    "Path(dashboard_cache_file).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PERCENTILE_LEVELS = [5, 10, 25, 50, 75, 90, 95]\n",
    "QUANTILE_LEVELS = np.linspace(0, 1, 1001)\n",
    "\n",
    "try:\n",
    "    with open(dashboard_cache_file, \"rb\") as f:\n",
    "        dashboard_cache = pickle.load(f)\n",
    "    if len(dashboard_cache.get(\"configs\", {})) == 0:\n",
    "        raise ValueError(\"Empty cache, rebuilding.\")\n",
    "    print(f\"Loaded dashboard cache with {len(dashboard_cache['configs'])} configs\")\n",
    "except Exception as e:\n",
    "    print(f\"Building dashboard cache (cache miss: {e})\")\n",
    "\n",
    "    dashboard_cache = {\n",
    "        \"version\": 1,\n",
    "        \"results_dir\": str(results_dir),\n",
    "        \"configs\": {},\n",
    "        \"crn_pairs\": {},\n",
    "        \"param_values\": {},\n",
    "        \"percentile_levels\": PERCENTILE_LEVELS,\n",
    "        \"quantile_levels\": QUANTILE_LEVELS,\n",
    "    }\n",
    "\n",
    "    start = perf_counter()\n",
    "    for idx, path in enumerate(tqdm(pkl_paths, desc=\"Building dashboard cache\")):\n",
    "        key = path.stem\n",
    "        try:\n",
    "            with open(path, \"rb\") as f:\n",
    "                data = pickle.load(f)\n",
    "\n",
    "            params = parse_config_key(key)\n",
    "            n_sims = len(data.growth_rates)\n",
    "            n_years = data.config.n_years\n",
    "            initial_assets = params.get(\"Cap\", 0)\n",
    "\n",
    "            # Time-series percentiles: shape (n_years, n_percentiles)\n",
    "            ts_losses = np.percentile(data.annual_losses, PERCENTILE_LEVELS, axis=0).T\n",
    "            ts_recoveries = np.percentile(data.insurance_recoveries, PERCENTILE_LEVELS, axis=0).T\n",
    "            ts_retained = np.percentile(data.retained_losses, PERCENTILE_LEVELS, axis=0).T\n",
    "\n",
    "            # Fine-grained distribution quantiles for smooth KDE-like plots\n",
    "            gr_qs = np.quantile(data.growth_rates, QUANTILE_LEVELS)\n",
    "            fa_qs = np.quantile(data.final_assets, QUANTILE_LEVELS)\n",
    "\n",
    "            # Identify interesting simulation IDs for the Paired Path Browser\n",
    "            interesting = {}\n",
    "\n",
    "            # Catastrophic: sims with any single-year loss > $2M\n",
    "            max_annual = data.annual_losses.max(axis=1)\n",
    "            catastrophic_mask = max_annual > 2_000_000\n",
    "            catastrophic_ids = np.where(catastrophic_mask)[0]\n",
    "            interesting[\"catastrophic\"] = catastrophic_ids[:100].tolist()\n",
    "\n",
    "            # Near-ruin: bottom 1% of surviving sims by final assets\n",
    "            insolvency_tol = getattr(data.config, \"insolvency_tolerance\", 10_000)\n",
    "            survived_mask = data.final_assets > insolvency_tol\n",
    "            if survived_mask.sum() > 0:\n",
    "                survived_assets = data.final_assets[survived_mask]\n",
    "                threshold_1pct = np.percentile(survived_assets, 1)\n",
    "                near_ruin_mask = survived_mask & (data.final_assets <= threshold_1pct)\n",
    "                interesting[\"near_ruin\"] = np.where(near_ruin_mask)[0][:50].tolist()\n",
    "            else:\n",
    "                interesting[\"near_ruin\"] = []\n",
    "\n",
    "            # Median: sims closest to median final assets\n",
    "            median_fa = np.median(data.final_assets)\n",
    "            dist_to_median = np.abs(data.final_assets - median_fa)\n",
    "            interesting[\"median\"] = np.argsort(dist_to_median)[:20].tolist()\n",
    "\n",
    "            # Best: top 1% by final assets\n",
    "            best_threshold = np.percentile(data.final_assets, 99)\n",
    "            interesting[\"best\"] = np.where(data.final_assets >= best_threshold)[0][:50].tolist()\n",
    "\n",
    "            # Worst surviving: bottom 5% of survivors\n",
    "            if survived_mask.sum() > 0:\n",
    "                threshold_5pct = np.percentile(data.final_assets[survived_mask], 5)\n",
    "                worst_surv_mask = survived_mask & (data.final_assets <= threshold_5pct)\n",
    "                interesting[\"worst_surviving\"] = np.where(worst_surv_mask)[0][:50].tolist()\n",
    "            else:\n",
    "                interesting[\"worst_surviving\"] = []\n",
    "\n",
    "            # Ruined: sims that went insolvent\n",
    "            ruined_mask = ~survived_mask\n",
    "            interesting[\"ruined\"] = np.where(ruined_mask)[0][:100].tolist()\n",
    "\n",
    "            dashboard_cache[\"configs\"][key] = {\n",
    "                \"Cap\": params.get(\"Cap\"),\n",
    "                \"ATR\": params.get(\"ATR\"),\n",
    "                \"EBITABL\": params.get(\"EBITABL\"),\n",
    "                \"Ded\": params.get(\"Ded\"),\n",
    "                \"LR\": params.get(\"LR\"),\n",
    "                \"Vol\": params.get(\"Vol\"),\n",
    "                \"NOINS\": params.get(\"NOINS\", False),\n",
    "                \"XS_Kurt\": params.get(\"XS_Kurt\"),\n",
    "                \"n_sims\": n_sims,\n",
    "                \"n_years\": n_years,\n",
    "                \"initial_assets\": initial_assets,\n",
    "                \"growth_rate_mean\": float(data.growth_rates.mean()),\n",
    "                \"growth_rate_median\": float(np.median(data.growth_rates)),\n",
    "                \"growth_rate_std\": float(data.growth_rates.std()),\n",
    "                \"final_assets_mean\": float(data.final_assets.mean()),\n",
    "                \"final_assets_median\": float(np.median(data.final_assets)),\n",
    "                \"ruin_probability\": {int(k) if str(k).isdigit() else k: float(v)\n",
    "                                     for k, v in data.ruin_probability.items()},\n",
    "                \"ts_annual_losses_pctiles\": ts_losses,\n",
    "                \"ts_insurance_recoveries_pctiles\": ts_recoveries,\n",
    "                \"ts_retained_losses_pctiles\": ts_retained,\n",
    "                \"growth_rate_qs\": gr_qs,\n",
    "                \"final_assets_qs\": fa_qs,\n",
    "                \"interesting_sims\": interesting,\n",
    "            }\n",
    "        except Exception as ex:\n",
    "            print(f\"  Skipping {path.name}: {ex}\")\n",
    "\n",
    "    # Build CRN pairing map: insured_key -> noins_key\n",
    "    insured_cfgs = {k: v for k, v in dashboard_cache[\"configs\"].items() if not v.get(\"NOINS\")}\n",
    "    noins_cfgs = {k: v for k, v in dashboard_cache[\"configs\"].items() if v.get(\"NOINS\")}\n",
    "\n",
    "    for ins_key, ins_cfg in insured_cfgs.items():\n",
    "        for ni_key, ni_cfg in noins_cfgs.items():\n",
    "            if (ins_cfg[\"Cap\"] == ni_cfg[\"Cap\"]\n",
    "                and ins_cfg[\"ATR\"] == ni_cfg[\"ATR\"]\n",
    "                and abs((ins_cfg[\"EBITABL\"] or 0) - (ni_cfg[\"EBITABL\"] or 0)) < 1e-6\n",
    "                and abs((ins_cfg[\"Vol\"] or 0) - (ni_cfg[\"Vol\"] or 0)) < 1e-6):\n",
    "                dashboard_cache[\"crn_pairs\"][ins_key] = ni_key\n",
    "                break\n",
    "\n",
    "    # Collect unique parameter values (insured configs only)\n",
    "    ins_cfgs_list = [v for v in dashboard_cache[\"configs\"].values() if not v.get(\"NOINS\")]\n",
    "    dashboard_cache[\"param_values\"] = {\n",
    "        \"Cap\": sorted(set(c[\"Cap\"] for c in ins_cfgs_list if c[\"Cap\"] is not None)),\n",
    "        \"ATR\": sorted(set(c[\"ATR\"] for c in ins_cfgs_list if c[\"ATR\"] is not None)),\n",
    "        \"Ded\": sorted(set(c[\"Ded\"] for c in ins_cfgs_list if c[\"Ded\"] is not None)),\n",
    "    }\n",
    "\n",
    "    elapsed = perf_counter() - start\n",
    "    print(f\"\\nBuilt dashboard cache in {elapsed:.1f}s:\")\n",
    "    print(f\"  {len(dashboard_cache['configs'])} configs total\")\n",
    "    print(f\"  {len(insured_cfgs)} insured, {len(noins_cfgs)} NOINS\")\n",
    "    print(f\"  {len(dashboard_cache['crn_pairs'])} CRN pairs\")\n",
    "\n",
    "    with open(dashboard_cache_file, \"wb\") as f:\n",
    "        pickle.dump(dashboard_cache, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(f\"  Saved to {dashboard_cache_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kd41ioc9ngc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dashboard cache contents\n",
    "print(f\"Dashboard cache version: {dashboard_cache['version']}\")\n",
    "print(f\"Results dir: {dashboard_cache['results_dir']}\")\n",
    "print(f\"Total configs: {len(dashboard_cache['configs'])}\")\n",
    "print(f\"CRN pairs: {len(dashboard_cache['crn_pairs'])}\")\n",
    "print(f\"\\nParameter values:\")\n",
    "for k, v in dashboard_cache[\"param_values\"].items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "n_insured = sum(1 for c in dashboard_cache[\"configs\"].values() if not c.get(\"NOINS\"))\n",
    "n_noins = sum(1 for c in dashboard_cache[\"configs\"].values() if c.get(\"NOINS\"))\n",
    "print(f\"\\nInsured configs: {n_insured}\")\n",
    "print(f\"NOINS configs: {n_noins}\")\n",
    "\n",
    "# Show sample config structure\n",
    "sample_key = next(k for k, v in dashboard_cache[\"configs\"].items() if not v.get(\"NOINS\"))\n",
    "sample = dashboard_cache[\"configs\"][sample_key]\n",
    "print(f\"\\nSample config: {sample_key}\")\n",
    "print(f\"  Cap={sample['Cap']}, ATR={sample['ATR']}, Ded={sample['Ded']}\")\n",
    "print(f\"  n_sims={sample['n_sims']}, n_years={sample['n_years']}\")\n",
    "print(f\"  growth_rate_mean={sample['growth_rate_mean']:.6f}\")\n",
    "print(f\"  ts_annual_losses shape: {sample['ts_annual_losses_pctiles'].shape}\")\n",
    "print(f\"  growth_rate_qs shape: {sample['growth_rate_qs'].shape}\")\n",
    "print(f\"  Interesting sims: {', '.join(f'{k}={len(v)}' for k, v in sample['interesting_sims'].items())}\")\n",
    "print(f\"\\nCRN pair example: {sample_key[:40]}...\")\n",
    "if sample_key in dashboard_cache[\"crn_pairs\"]:\n",
    "    print(f\"  -> {dashboard_cache['crn_pairs'][sample_key][:40]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0b4855",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
