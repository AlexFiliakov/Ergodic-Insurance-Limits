{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scenario Comparison and Annotation Framework Demo\n",
    "\n",
    "This notebook demonstrates the comprehensive scenario comparison and annotation framework,\n",
    "including:\n",
    "- Side-by-side scenario comparisons\n",
    "- Automatic insight extraction\n",
    "- Smart annotation placement\n",
    "- Natural language generation\n",
    "- Statistical significance testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.insert(0, os.path.abspath('../..'))\n",
    "\n",
    "# Project imports\n",
    "from ergodic_insurance.config_manager import ConfigManager\n",
    "from ergodic_insurance.manufacturer import WidgetManufacturer\n",
    "from ergodic_insurance.simulation import Simulation\n",
    "from ergodic_insurance.scenario_manager import ScenarioManager, ParameterSpec\n",
    "from ergodic_insurance.batch_processor import BatchProcessor\n",
    "\n",
    "# New comparison framework imports\n",
    "from ergodic_insurance.reporting.scenario_comparator import ScenarioComparator\n",
    "from ergodic_insurance.reporting.insight_extractor import InsightExtractor\n",
    "from ergodic_insurance.visualization.annotations import (\n",
    "    SmartAnnotationPlacer,\n",
    "    auto_annotate_peaks_valleys,\n",
    "    create_leader_line\n",
    ")\n",
    "\n",
    "# Set style\n",
    "from ergodic_insurance.visualization.core import set_wsj_style\n",
    "set_wsj_style()\n",
    "\n",
    "print(\"Framework loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Multiple Scenarios for Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scenario manager\n",
    "scenario_mgr = ScenarioManager()\n",
    "\n",
    "# Load base configuration\n",
    "config_mgr = ConfigManager()\n",
    "base_config = config_mgr.load_profile('default')\n",
    "\n",
    "# Define parameter specifications for sensitivity analysis\n",
    "param_specs = [\n",
    "    ParameterSpec(\n",
    "        name='insurance.base_premium_rate',\n",
    "        base_value=0.015,\n",
    "        variation_pct=0.3\n",
    "    ),\n",
    "    ParameterSpec(\n",
    "        name='insurance.limit',\n",
    "        base_value=5000000,\n",
    "        variation_pct=0.5\n",
    "    ),\n",
    "    ParameterSpec(\n",
    "        name='manufacturer.target_margin',\n",
    "        base_value=0.08,\n",
    "        variation_pct=0.25\n",
    "    )\n",
    "]\n",
    "\n",
    "# Create scenarios\n",
    "scenarios = scenario_mgr.create_sensitivity_analysis(\n",
    "    base_name='insurance_optimization',\n",
    "    parameter_specs=param_specs,\n",
    "    base_config=base_config,\n",
    "    tags={'demo', 'comparison'}\n",
    ")\n",
    "\n",
    "print(f\"Created {len(scenarios)} scenarios for comparison:\")\n",
    "for scenario in scenarios:\n",
    "    print(f\"  - {scenario.name}: {scenario.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run Simulations and Collect Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demo purposes, we'll create synthetic results\n",
    "# In practice, you would run actual simulations\n",
    "\n",
    "def create_synthetic_results(scenario_name, param_overrides):\n",
    "    \"\"\"Create synthetic results for demonstration.\"\"\"\n",
    "    np.random.seed(hash(scenario_name) % 2**32)\n",
    "    \n",
    "    # Base metrics\n",
    "    base_growth = 0.06\n",
    "    base_ruin = 0.015\n",
    "    base_assets = 10000000\n",
    "    \n",
    "    # Adjust based on parameters\n",
    "    if 'insurance.base_premium_rate' in param_overrides:\n",
    "        premium_effect = (param_overrides['insurance.base_premium_rate'] - 0.015) * 100\n",
    "        base_growth -= premium_effect * 0.5\n",
    "        base_ruin += premium_effect * 0.3\n",
    "    \n",
    "    if 'insurance.limit' in param_overrides:\n",
    "        limit_effect = (param_overrides['insurance.limit'] - 5000000) / 5000000\n",
    "        base_ruin *= (1 - limit_effect * 0.4)\n",
    "        base_growth += limit_effect * 0.01\n",
    "    \n",
    "    if 'manufacturer.target_margin' in param_overrides:\n",
    "        margin_effect = (param_overrides['manufacturer.target_margin'] - 0.08) / 0.08\n",
    "        base_growth += margin_effect * 0.02\n",
    "        base_assets *= (1 + margin_effect * 0.15)\n",
    "    \n",
    "    # Add some randomness\n",
    "    growth_rate = base_growth + np.random.normal(0, 0.005)\n",
    "    ruin_prob = max(0, base_ruin + np.random.normal(0, 0.002))\n",
    "    final_assets = base_assets * (1 + np.random.normal(0, 0.1))\n",
    "    \n",
    "    # Create time series for trends\n",
    "    n_periods = 50\n",
    "    time_series_growth = np.cumsum(np.random.normal(growth_rate/n_periods, 0.01, n_periods))\n",
    "    time_series_assets = base_assets * np.exp(time_series_growth)\n",
    "    \n",
    "    return {\n",
    "        'summary_statistics': {\n",
    "            'mean_growth_rate': growth_rate,\n",
    "            'ruin_probability': ruin_prob,\n",
    "            'mean_final_assets': final_assets,\n",
    "            'var_95': -np.random.uniform(50000, 200000),\n",
    "            'var_99': -np.random.uniform(100000, 500000),\n",
    "            'sharpe_ratio': growth_rate / 0.15,\n",
    "            'max_drawdown': np.random.uniform(0.05, 0.25)\n",
    "        },\n",
    "        'time_series': {\n",
    "            'growth_rate': time_series_growth,\n",
    "            'assets': time_series_assets\n",
    "        },\n",
    "        'config': param_overrides\n",
    "    }\n",
    "\n",
    "# Generate results for all scenarios\n",
    "results = {}\n",
    "for scenario in scenarios:\n",
    "    results[scenario.name] = create_synthetic_results(\n",
    "        scenario.name,\n",
    "        scenario.parameter_overrides\n",
    "    )\n",
    "\n",
    "print(f\"Generated results for {len(results)} scenarios\")\n",
    "print(\"\\nSample metrics for baseline:\")\n",
    "baseline_name = 'insurance_optimization_baseline'\n",
    "if baseline_name in results:\n",
    "    for metric, value in results[baseline_name]['summary_statistics'].items():\n",
    "        print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compare Scenarios Using the Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize comparator\n",
    "comparator = ScenarioComparator()\n",
    "\n",
    "# Compare scenarios\n",
    "comparison = comparator.compare_scenarios(\n",
    "    results,\n",
    "    baseline='insurance_optimization_baseline',\n",
    "    metrics=['mean_growth_rate', 'ruin_probability', 'mean_final_assets', 'sharpe_ratio']\n",
    ")\n",
    "\n",
    "print(\"Comparison Analysis Complete!\")\n",
    "print(f\"\\nScenarios compared: {', '.join(comparison.scenarios)}\")\n",
    "print(f\"\\nMetrics analyzed: {', '.join(comparison.metrics.keys())}\")\n",
    "\n",
    "# Show top performers\n",
    "print(\"\\n=== Top Performers ===\")\n",
    "for metric in ['mean_growth_rate', 'ruin_probability']:\n",
    "    ascending = 'risk' in metric or 'ruin' in metric\n",
    "    top = comparison.get_top_performers(metric, n=3, ascending=ascending)\n",
    "    print(f\"\\n{metric.replace('_', ' ').title()}:\")\n",
    "    for scenario, value in top:\n",
    "        print(f\"  {scenario}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Comparison Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison grid\n",
    "fig = comparator.create_comparison_grid(\n",
    "    metrics=['mean_growth_rate', 'ruin_probability', 'mean_final_assets', 'sharpe_ratio'],\n",
    "    figsize=(16, 10),\n",
    "    show_diff=True\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Comparison grid created with difference indicators from baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Extract and Display Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize insight extractor\n",
    "extractor = InsightExtractor()\n",
    "\n",
    "# Extract insights from comparison\n",
    "insights = extractor.extract_insights(\n",
    "    comparison,\n",
    "    focus_metrics=['mean_growth_rate', 'ruin_probability'],\n",
    "    threshold_importance=50\n",
    ")\n",
    "\n",
    "print(f\"Extracted {len(insights)} insights:\\n\")\n",
    "\n",
    "# Display top insights\n",
    "for i, insight in enumerate(insights[:5], 1):\n",
    "    print(f\"{i}. [{insight.category.upper()}] {insight.title}\")\n",
    "    print(f\"   {insight.description}\")\n",
    "    print(f\"   Importance: {insight.importance:.0f}/100 | Confidence: {insight.confidence:.1%}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Executive Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate executive summary\n",
    "executive_summary = extractor.generate_executive_summary(\n",
    "    max_points=5,\n",
    "    focus_positive=True\n",
    ")\n",
    "\n",
    "print(executive_summary)\n",
    "\n",
    "# Generate technical notes\n",
    "print(\"\\n## Technical Notes\\n\")\n",
    "technical_notes = extractor.generate_technical_notes()\n",
    "for note in technical_notes:\n",
    "    print(f\"• {note}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Demonstrate Smart Annotation Placement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample plot with smart annotations - IMPROVED VERSION\n",
    "fig, ax = plt.subplots(figsize=(14, 9))\n",
    "\n",
    "# Generate sample time series data\n",
    "np.random.seed(42)\n",
    "x = np.arange(0, 100)\n",
    "y1 = np.cumsum(np.random.randn(100) * 0.5) + 50\n",
    "y2 = np.cumsum(np.random.randn(100) * 0.3) + 45\n",
    "y3 = np.cumsum(np.random.randn(100) * 0.4) + 40\n",
    "\n",
    "# Plot lines with specific colors\n",
    "line1 = ax.plot(x, y1, label='Optimized Strategy', linewidth=2, color='#1f77b4')[0]\n",
    "line2 = ax.plot(x, y2, label='Baseline Strategy', linewidth=2, color='#ff7f0e')[0]\n",
    "line3 = ax.plot(x, y3, label='Conservative Strategy', linewidth=2, color='#2ca02c')[0]\n",
    "\n",
    "# Initialize smart annotation placer ONCE for all annotations\n",
    "placer = SmartAnnotationPlacer(ax)\n",
    "\n",
    "# First, automatically annotate peaks and valleys on the optimized strategy\n",
    "# Pass the placer instance to avoid overlaps with other annotations\n",
    "placer = auto_annotate_peaks_valleys(\n",
    "    ax, x, y1, \n",
    "    n_peaks=2, \n",
    "    n_valleys=1,\n",
    "    peak_color='#006400',  # Dark green for peaks (distinct from line colors)\n",
    "    valley_color='#8B0000',  # Dark red for valleys (distinct from line colors)\n",
    "    fontsize=8,\n",
    "    placer=placer  # Use same placer instance\n",
    ")\n",
    "\n",
    "# Define strategic annotations with better positioning hints\n",
    "annotations = [\n",
    "    {\n",
    "        'text': 'Optimized outperforms',\n",
    "        'point': (60, y1[60]),  # Moved to avoid peak/valley areas\n",
    "        'priority': 85,  # High but lower than peaks/valleys\n",
    "        'color': '#4169E1'  # Royal blue - distinct from lines\n",
    "    },\n",
    "    {\n",
    "        'text': 'Baseline steady',\n",
    "        'point': (35, y2[35]),\n",
    "        'priority': 75,\n",
    "        'color': '#FF8C00'  # Dark orange - distinct variant\n",
    "    },\n",
    "    {\n",
    "        'text': 'Conservative lags',\n",
    "        'point': (75, y3[75]),\n",
    "        'priority': 65,\n",
    "        'color': '#556B2F'  # Dark olive green - distinct variant\n",
    "    },\n",
    "    {\n",
    "        'text': 'Convergence zone',\n",
    "        'point': (20, (y1[20] + y2[20] + y3[20])/3),  # Center of all three\n",
    "        'priority': 70,\n",
    "        'color': '#800080'  # Purple - unique color\n",
    "    },\n",
    "    {\n",
    "        'text': 'Divergence begins',\n",
    "        'point': (45, (y1[45] + y3[45])/2),\n",
    "        'priority': 60,\n",
    "        'color': '#8B4513'  # Saddle brown - distinct\n",
    "    }\n",
    "]\n",
    "\n",
    "# Add strategic annotations using the same placer (ensures no overlaps)\n",
    "placer.add_smart_annotations(annotations, fontsize=9)\n",
    "\n",
    "# Add a subtle leader line example with better positioning\n",
    "leader_start = (10, y1[10])\n",
    "leader_end = (18, y1[10] + 6)\n",
    "create_leader_line(ax, leader_start, leader_end, \n",
    "                   style='curved', color='#A9A9A9', \n",
    "                   linewidth=0.8, alpha=0.5)\n",
    "\n",
    "# Add small text at the leader line end\n",
    "ax.text(leader_end[0], leader_end[1], 'Initial growth phase',\n",
    "        fontsize=8, color='#696969', style='italic',\n",
    "        ha='center', va='bottom')\n",
    "\n",
    "# Format plot with improved styling\n",
    "ax.set_xlabel('Time Period', fontsize=11)\n",
    "ax.set_ylabel('Performance Metric', fontsize=11)\n",
    "ax.set_title('Smart Annotation Demonstration - Improved Placement System', \n",
    "             fontsize=13, fontweight='bold', pad=20)\n",
    "\n",
    "# Position legend to avoid annotation areas\n",
    "ax.legend(loc='lower right', framealpha=0.95, edgecolor='gray', \n",
    "          borderpad=1, columnspacing=1.5)\n",
    "\n",
    "# Refined grid\n",
    "ax.grid(True, alpha=0.2, linestyle='--', linewidth=0.5)\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "# Set reasonable axis limits with padding for annotations\n",
    "ax.set_xlim(-5, 105)\n",
    "y_min = min(y1.min(), y2.min(), y3.min())\n",
    "y_max = max(y1.max(), y2.max(), y3.max())\n",
    "y_padding = (y_max - y_min) * 0.15\n",
    "ax.set_ylim(y_min - y_padding, y_max + y_padding)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Smart annotations placed without overlaps!\")\n",
    "print(\"✓ Colors chosen to avoid conflicts with line colors\")\n",
    "print(\"✓ All annotations kept within visible bounds\")\n",
    "print(\"✓ Arrows properly connected to target points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create Parameter Difference Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create parameter difference tables for key scenarios\n",
    "for scenario_name in comparison.scenarios[1:3]:  # Show first 2 non-baseline scenarios\n",
    "    print(f\"\\n=== Parameter Differences: {scenario_name} ===\")\n",
    "    \n",
    "    diff_table = comparator.create_parameter_diff_table(\n",
    "        scenario_name,\n",
    "        threshold=5.0  # Show changes > 5%\n",
    "    )\n",
    "    \n",
    "    if not diff_table.empty:\n",
    "        print(diff_table.to_string(index=False))\n",
    "    else:\n",
    "        print(\"No significant parameter differences found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. A/B Testing Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create A/B testing visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Select two scenarios for A/B comparison\n",
    "scenario_a = 'insurance_optimization_baseline'\n",
    "scenario_b = list(comparison.scenarios)[1] if len(comparison.scenarios) > 1 else scenario_a\n",
    "\n",
    "metrics_to_compare = ['mean_growth_rate', 'ruin_probability', 'mean_final_assets', 'sharpe_ratio']\n",
    "\n",
    "for idx, metric in enumerate(metrics_to_compare):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Get values\n",
    "    val_a = comparison.metrics[metric].get(scenario_a, 0)\n",
    "    val_b = comparison.metrics[metric].get(scenario_b, 0)\n",
    "    \n",
    "    # Create bar plot\n",
    "    bars = ax.bar(['Scenario A\\n(Baseline)', 'Scenario B\\n(Alternative)'], \n",
    "                   [val_a, val_b],\n",
    "                   color=['#1f77b4', '#ff7f0e'],\n",
    "                   alpha=0.8)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars, [val_a, val_b]):\n",
    "        height = bar.get_height()\n",
    "        if 'probability' in metric or 'rate' in metric:\n",
    "            label = f'{val:.2%}'\n",
    "        elif 'assets' in metric:\n",
    "            label = f'${val/1e6:.1f}M'\n",
    "        else:\n",
    "            label = f'{val:.3f}'\n",
    "        \n",
    "        ax.text(bar.get_x() + bar.get_width()/2, height,\n",
    "               label, ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Calculate and show difference\n",
    "    diff = ((val_b - val_a) / val_a * 100) if val_a != 0 else 0\n",
    "    color = 'green' if diff > 0 else 'red'\n",
    "    \n",
    "    # Determine if improvement based on metric type\n",
    "    if 'risk' in metric or 'ruin' in metric or 'drawdown' in metric:\n",
    "        improvement = diff < 0\n",
    "    else:\n",
    "        improvement = diff > 0\n",
    "    \n",
    "    status = '✓ Better' if improvement else '✗ Worse'\n",
    "    \n",
    "    ax.text(0.5, 0.95, f'{diff:+.1f}% {status}',\n",
    "           transform=ax.transAxes,\n",
    "           ha='center', va='top',\n",
    "           fontsize=12, fontweight='bold',\n",
    "           color=color,\n",
    "           bbox=dict(boxstyle='round', facecolor='white', edgecolor=color, alpha=0.8))\n",
    "    \n",
    "    ax.set_title(metric.replace('_', ' ').title())\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "fig.suptitle(f'A/B Test: {scenario_a} vs {scenario_b}', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"A/B testing visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Comparison Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export comprehensive comparison report\n",
    "import os\n",
    "output_dir = 'comparison_reports'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "output_base = os.path.join(output_dir, 'scenario_comparison')\n",
    "\n",
    "# Export comparison data\n",
    "output_files = comparator.export_comparison_report(\n",
    "    output_base,\n",
    "    include_plots=True\n",
    ")\n",
    "\n",
    "print(\"Comparison report exported:\")\n",
    "for key, path in output_files.items():\n",
    "    print(f\"  - {key}: {path}\")\n",
    "\n",
    "# Export insights with correct parameter name\n",
    "insights_path = os.path.join(output_dir, 'insights.md')\n",
    "extractor.export_insights(insights_path, output_format='markdown')  # Fixed: use output_format instead of format\n",
    "print(f\"  - insights: {insights_path}\")\n",
    "\n",
    "# Display sample of the executive summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Sample Executive Summary:\")\n",
    "print(\"=\"*50)\n",
    "print(executive_summary[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the comprehensive scenario comparison and annotation framework:\n",
    "\n",
    "### Key Features Shown:\n",
    "1. **Scenario Comparison**: Side-by-side analysis of multiple scenarios\n",
    "2. **Parameter Diff Detection**: Automatic identification of parameter changes\n",
    "3. **Statistical Testing**: Significance analysis across scenarios\n",
    "4. **Smart Annotations**: Automatic placement without overlaps\n",
    "5. **Insight Extraction**: Automatic identification of key findings\n",
    "6. **Natural Language Generation**: Executive summaries and technical notes\n",
    "7. **A/B Testing**: Direct comparison visualizations\n",
    "8. **Leader Lines**: Intelligent routing for callouts\n",
    "9. **Peak/Valley Detection**: Automatic annotation of extrema\n",
    "10. **Export Capabilities**: Comprehensive reporting in multiple formats\n",
    "\n",
    "### Use Cases:\n",
    "- **Decision Support**: Compare different insurance strategies\n",
    "- **Sensitivity Analysis**: Understand parameter impacts\n",
    "- **Report Generation**: Automated insights for stakeholders\n",
    "- **Presentation Ready**: Professional visualizations with annotations\n",
    "\n",
    "The framework provides a complete solution for scenario analysis and comparison,\n",
    "making it easy to identify optimal configurations and communicate findings effectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
