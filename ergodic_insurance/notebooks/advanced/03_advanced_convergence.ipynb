{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Convergence Analysis\n",
    "\n",
    "## Overview\n",
    "Deep-dive into Monte Carlo convergence diagnostics: running-mean stability, effective sample size (ESS), MCMC-style chain mixing, batch-means standard errors, and practical guidance on choosing simulation counts.\n",
    "\n",
    "- **Prerequisites**: [core/03_monte_carlo_simulation](../core/03_monte_carlo_simulation.ipynb)\n",
    "- **Estimated runtime**: 1-2 minutes\n",
    "- **Audience**: [Developer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from ergodic_insurance import ManufacturerConfig, InsuranceProgram, EnhancedInsuranceLayer\n",
    "from ergodic_insurance.manufacturer import WidgetManufacturer\n",
    "from ergodic_insurance.loss_distributions import ManufacturingLossGenerator\n",
    "from ergodic_insurance.monte_carlo import MonteCarloEngine\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Baseline Simulation\n",
    "\n",
    "Generate a large Monte Carlo run that serves as our \"ground truth\" for convergence analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "config = ManufacturerConfig(\n    initial_assets=10_000_000,\n    asset_turnover_ratio=1.0,\n    base_operating_margin=0.10,\n    tax_rate=0.25,\n    retention_ratio=0.7,\n)\n\nloss_gen = ManufacturingLossGenerator(\n    attritional_params={\"base_frequency\": 4.0, \"severity_mean\": 30_000, \"severity_cv\": 0.6},\n    large_params={\"base_frequency\": 0.4, \"severity_mean\": 400_000, \"severity_cv\": 0.8},\n    catastrophic_params={\"base_frequency\": 0.02, \"severity_xm\": 3_000_000, \"severity_alpha\": 2.0},\n    seed=SEED,\n)\n\nprogram = InsuranceProgram([\n    EnhancedInsuranceLayer(attachment_point=50_000, limit=2_000_000,\n                           base_premium_rate=0.015),\n    EnhancedInsuranceLayer(attachment_point=2_050_000, limit=5_000_000,\n                           base_premium_rate=0.005),\n])\npremium = float(program.calculate_annual_premium())\n\nN_SIMS = 5_000\nN_YEARS = 10\n\nfinal_assets = np.zeros(N_SIMS)\nfor s in range(N_SIMS):\n    m = WidgetManufacturer(config)\n    for yr in range(N_YEARS):\n        _, st = loss_gen.generate_losses(1.0, float(m.calculate_revenue()))\n        rec = program.process_claim(st[\"total_amount\"])\n        net = st[\"total_amount\"] - rec[\"insurance_recovery\"]\n        if net > 0:\n            m.process_insurance_claim(net)\n        if premium > 0:\n            m.record_insurance_premium(premium)\n        m.step(growth_rate=0.0)\n        if float(m.equity) <= 0:\n            break\n    final_assets[s] = max(float(m.total_assets), 0)\n\nsurviving = final_assets[final_assets > 0]\ngrowth_rates = (surviving / config.initial_assets) ** (1 / N_YEARS) - 1\n\nprint(f\"Simulations : {N_SIMS}\")\nprint(f\"Survivors   : {len(surviving)} ({len(surviving)/N_SIMS:.1%})\")\nprint(f\"Ruin rate   : {1 - len(surviving)/N_SIMS:.2%}\")\nprint(f\"Mean CAGR   : {np.mean(growth_rates):.3%}\")\nprint(f\"Median CAGR : {np.median(growth_rates):.3%}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Running-Mean Convergence\n",
    "\n",
    "Plot the running (cumulative) mean of the growth rate as more simulations are added.\n",
    "A stable running mean indicates convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_mean = np.cumsum(growth_rates) / np.arange(1, len(growth_rates) + 1)\n",
    "running_std  = np.array([growth_rates[:i+1].std() for i in range(len(growth_rates))])\n",
    "se = running_std / np.sqrt(np.arange(1, len(growth_rates) + 1))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax = axes[0]\n",
    "n = np.arange(1, len(running_mean) + 1)\n",
    "ax.plot(n, running_mean * 100, lw=1.5, label=\"Running mean\")\n",
    "ax.fill_between(n, (running_mean - 2 * se) * 100, (running_mean + 2 * se) * 100,\n",
    "                alpha=0.2, label=\"95% CI\")\n",
    "ax.axhline(np.mean(growth_rates) * 100, ls=\"--\", color=\"red\", alpha=0.5,\n",
    "           label=f\"Final mean={np.mean(growth_rates):.2%}\")\n",
    "ax.set_xlabel(\"Number of Simulations\")\n",
    "ax.set_ylabel(\"Mean CAGR (%)\")\n",
    "ax.set_title(\"Running Mean Convergence\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(n, se * 100, lw=1.5, color=\"orange\")\n",
    "ax.set_xlabel(\"Number of Simulations\")\n",
    "ax.set_ylabel(\"Standard Error (%)\")\n",
    "ax.set_title(\"Standard Error Decay\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Report convergence at key sample sizes\n",
    "for k in [100, 500, 1000, 2000, len(growth_rates)]:\n",
    "    idx = min(k, len(growth_rates)) - 1\n",
    "    print(f\"  n={k:>5d}  mean={running_mean[idx]:.3%}  SE={se[idx]:.4%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Effective Sample Size (ESS)\n",
    "\n",
    "Autocorrelation reduces the effective number of independent samples.\n",
    "We estimate ESS using the initial monotone sequence estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ess(x):\n",
    "    \"\"\"Effective sample size via initial positive sequence estimator.\"\"\"\n",
    "    n = len(x)\n",
    "    x_centered = x - x.mean()\n",
    "    acf = np.correlate(x_centered, x_centered, mode=\"full\")[n - 1:]\n",
    "    acf /= acf[0]\n",
    "    # sum positive pairs\n",
    "    tau = 1.0\n",
    "    for k in range(1, n // 2):\n",
    "        pair_sum = acf[2 * k - 1] + acf[2 * k]\n",
    "        if pair_sum < 0:\n",
    "            break\n",
    "        tau += 2 * pair_sum\n",
    "    return n / tau\n",
    "\n",
    "ess = compute_ess(growth_rates)\n",
    "ess_ratio = ess / len(growth_rates)\n",
    "\n",
    "print(f\"Nominal samples : {len(growth_rates)}\")\n",
    "print(f\"Effective samples: {ess:.0f}  ({ess_ratio:.1%} efficiency)\")\n",
    "\n",
    "# Autocorrelation plot\n",
    "max_lag = 50\n",
    "acf_vals = [np.corrcoef(growth_rates[:-lag], growth_rates[lag:])[0, 1]\n",
    "            for lag in range(1, max_lag + 1)]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.bar(range(1, max_lag + 1), acf_vals, color=\"steelblue\", alpha=0.7)\n",
    "ax.axhline(0, color=\"black\", lw=0.5)\n",
    "ax.axhline( 2 / np.sqrt(len(growth_rates)), ls=\"--\", color=\"red\", alpha=0.5)\n",
    "ax.axhline(-2 / np.sqrt(len(growth_rates)), ls=\"--\", color=\"red\", alpha=0.5)\n",
    "ax.set_xlabel(\"Lag\")\n",
    "ax.set_ylabel(\"Autocorrelation\")\n",
    "ax.set_title(\"Sample Autocorrelation of Growth Rates\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Batch-Means Standard Error\n",
    "\n",
    "Divide the sample into batches and use the between-batch variance to estimate the standard error.\n",
    "This is robust to serial correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_means_se(x, n_batches=20):\n",
    "    n = len(x)\n",
    "    batch_size = n // n_batches\n",
    "    batch_means = np.array([x[i * batch_size:(i + 1) * batch_size].mean()\n",
    "                            for i in range(n_batches)])\n",
    "    return batch_means.std() / np.sqrt(n_batches), batch_means\n",
    "\n",
    "bm_se, batch_vals = batch_means_se(growth_rates)\n",
    "naive_se = growth_rates.std() / np.sqrt(len(growth_rates))\n",
    "\n",
    "print(f\"Naive SE        : {naive_se:.5%}\")\n",
    "print(f\"Batch-means SE  : {bm_se:.5%}\")\n",
    "print(f\"Ratio (BM/Naive): {bm_se / naive_se:.2f}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.bar(range(len(batch_vals)), batch_vals * 100, color=\"teal\", alpha=0.7)\n",
    "ax.axhline(np.mean(growth_rates) * 100, ls=\"--\", color=\"red\",\n",
    "           label=f\"Overall mean={np.mean(growth_rates):.2%}\")\n",
    "ax.set_xlabel(\"Batch\")\n",
    "ax.set_ylabel(\"Batch Mean CAGR (%)\")\n",
    "ax.set_title(\"Batch Means\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Convergence by Metric\n",
    "\n",
    "Different quantities converge at different rates.  Compare ruin probability, mean growth, and VaR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sizes = np.arange(50, len(final_assets) + 1, 50)\n",
    "ruin_conv, mean_conv, var95_conv = [], [], []\n",
    "\n",
    "for n in sample_sizes:\n",
    "    fa = final_assets[:n]\n",
    "    surv = fa[fa > 0]\n",
    "    ruin_conv.append(1 - len(surv) / n)\n",
    "    if len(surv) > 1:\n",
    "        gr = (surv / config.initial_assets) ** (1 / N_YEARS) - 1\n",
    "        mean_conv.append(np.mean(gr))\n",
    "        var95_conv.append(np.percentile(gr, 5))\n",
    "    else:\n",
    "        mean_conv.append(np.nan)\n",
    "        var95_conv.append(np.nan)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "for ax, vals, title in zip(axes,\n",
    "    [ruin_conv, mean_conv, var95_conv],\n",
    "    [\"Ruin Probability\", \"Mean CAGR\", \"5th Percentile CAGR (VaR)\"],\n",
    "):\n",
    "    ax.plot(sample_sizes, vals, lw=1.5)\n",
    "    if not np.all(np.isnan(vals)):\n",
    "        ax.axhline(vals[-1], ls=\"--\", color=\"red\", alpha=0.5)\n",
    "    ax.set_xlabel(\"N simulations\")\n",
    "    ax.set_title(title)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"Convergence by Metric\", fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Tail metrics (ruin, VaR) require more simulations than central metrics (mean).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Practical Guidance: How Many Simulations?\n",
    "\n",
    "Use the standard-error budget to choose simulation count for a target precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "target_se = 0.002  # target SE on CAGR\nsigma = growth_rates.std() if len(growth_rates) > 1 else 0.0\n\nif sigma > 0 and np.isfinite(sigma):\n    n_needed_naive = int(np.ceil((sigma / target_se) ** 2))\n    n_needed_ess   = int(np.ceil(n_needed_naive / ess_ratio)) if ess_ratio > 0 else n_needed_naive\nelse:\n    n_needed_naive = 0\n    n_needed_ess = 0\n\nprint(f\"Sample std of CAGR: {sigma:.4%}\")\nprint(f\"Target SE          : {target_se:.4%}\")\nprint(f\"Needed (iid)       : {n_needed_naive:,}\")\nprint(f\"Needed (ESS-adj)   : {n_needed_ess:,}\")\n\np_hat = 1 - len(surviving) / N_SIMS\nprint(f\"\\nFor ruin probability estimation at {p_hat:.2%}:\")\nif p_hat > 0:\n    n_ruin = int(np.ceil(p_hat * (1 - p_hat) / (0.005 ** 2)))  # SE=0.5%\n    print(f\"  Needed for SE < 0.5%: {n_ruin:,}\")\nelse:\n    print(f\"  No ruin events -- need many more simulations to estimate.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- **Running-mean plots** are the simplest convergence diagnostic -- look for a flat tail.\n",
    "- **ESS** quantifies how much autocorrelation inflates uncertainty; low ESS means results are less reliable than the nominal sample count suggests.\n",
    "- **Batch-means SE** is robust to serial correlation and should be preferred over naive SE.\n",
    "- **Tail metrics** (ruin probability, VaR) converge much more slowly than central moments; budget 5-10x more simulations for tails.\n",
    "- Use the SE-budget formula to determine the simulation count **before** running expensive optimisations.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- [core/03_monte_carlo_simulation](../core/03_monte_carlo_simulation.ipynb) -- Monte Carlo engine basics\n",
    "- [advanced/02_walk_forward_validation](02_walk_forward_validation.ipynb) -- out-of-sample strategy validation\n",
    "- [optimization/01_optimization_overview](../optimization/01_optimization_overview.ipynb) -- optimization algorithms"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
