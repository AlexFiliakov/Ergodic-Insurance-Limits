{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scenario Comparison and Annotation Framework Demo\n",
    "\n",
    "This notebook demonstrates the comprehensive scenario comparison and annotation framework,\n",
    "including:\n",
    "- Side-by-side scenario comparisons\n",
    "- Automatic insight extraction\n",
    "- Smart annotation placement\n",
    "- Natural language generation\n",
    "- Statistical significance testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Standard imports\nimport sys\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Add parent directory to path for imports\nsys.path.insert(0, os.path.abspath('../..'))\n\n# Project imports\nfrom ergodic_insurance.src.config_manager import ConfigManager\nfrom ergodic_insurance.src.manufacturer import WidgetManufacturer\nfrom ergodic_insurance.src.simulation import InsuranceSimulation\nfrom ergodic_insurance.src.scenario_manager import ScenarioManager, ParameterSpec\nfrom ergodic_insurance.src.batch_processor import BatchProcessor\n\n# New comparison framework imports\nfrom ergodic_insurance.src.reporting.scenario_comparator import ScenarioComparator\nfrom ergodic_insurance.src.reporting.insight_extractor import InsightExtractor\nfrom ergodic_insurance.src.visualization.annotations import (\n    SmartAnnotationPlacer,\n    auto_annotate_peaks_valleys,\n    create_leader_line\n)\n\n# Set style\nfrom ergodic_insurance.src.visualization.core import set_wsj_style\nset_wsj_style()\n\nprint(\"Framework loaded successfully!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Multiple Scenarios for Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scenario manager\n",
    "scenario_mgr = ScenarioManager()\n",
    "\n",
    "# Load base configuration\n",
    "config_mgr = ConfigManager()\n",
    "base_config = config_mgr.load_profile('default')\n",
    "\n",
    "# Define parameter specifications for sensitivity analysis\n",
    "param_specs = [\n",
    "    ParameterSpec(\n",
    "        name='insurance.premium_rate',\n",
    "        base_value=0.015,\n",
    "        variation_pct=0.3\n",
    "    ),\n",
    "    ParameterSpec(\n",
    "        name='insurance.limit',\n",
    "        base_value=5000000,\n",
    "        variation_pct=0.5\n",
    "    ),\n",
    "    ParameterSpec(\n",
    "        name='manufacturer.target_margin',\n",
    "        base_value=0.08,\n",
    "        variation_pct=0.25\n",
    "    )\n",
    "]\n",
    "\n",
    "# Create scenarios\n",
    "scenarios = scenario_mgr.create_sensitivity_analysis(\n",
    "    base_name='insurance_optimization',\n",
    "    parameter_specs=param_specs,\n",
    "    base_config=base_config,\n",
    "    tags={'demo', 'comparison'}\n",
    ")\n",
    "\n",
    "print(f\"Created {len(scenarios)} scenarios for comparison:\")\n",
    "for scenario in scenarios:\n",
    "    print(f\"  - {scenario.name}: {scenario.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run Simulations and Collect Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demo purposes, we'll create synthetic results\n",
    "# In practice, you would run actual simulations\n",
    "\n",
    "def create_synthetic_results(scenario_name, param_overrides):\n",
    "    \"\"\"Create synthetic results for demonstration.\"\"\"\n",
    "    np.random.seed(hash(scenario_name) % 2**32)\n",
    "    \n",
    "    # Base metrics\n",
    "    base_growth = 0.06\n",
    "    base_ruin = 0.015\n",
    "    base_assets = 10000000\n",
    "    \n",
    "    # Adjust based on parameters\n",
    "    if 'insurance.premium_rate' in param_overrides:\n",
    "        premium_effect = (param_overrides['insurance.premium_rate'] - 0.015) * 100\n",
    "        base_growth -= premium_effect * 0.5\n",
    "        base_ruin += premium_effect * 0.3\n",
    "    \n",
    "    if 'insurance.limit' in param_overrides:\n",
    "        limit_effect = (param_overrides['insurance.limit'] - 5000000) / 5000000\n",
    "        base_ruin *= (1 - limit_effect * 0.4)\n",
    "        base_growth += limit_effect * 0.01\n",
    "    \n",
    "    if 'manufacturer.target_margin' in param_overrides:\n",
    "        margin_effect = (param_overrides['manufacturer.target_margin'] - 0.08) / 0.08\n",
    "        base_growth += margin_effect * 0.02\n",
    "        base_assets *= (1 + margin_effect * 0.15)\n",
    "    \n",
    "    # Add some randomness\n",
    "    growth_rate = base_growth + np.random.normal(0, 0.005)\n",
    "    ruin_prob = max(0, base_ruin + np.random.normal(0, 0.002))\n",
    "    final_assets = base_assets * (1 + np.random.normal(0, 0.1))\n",
    "    \n",
    "    # Create time series for trends\n",
    "    n_periods = 50\n",
    "    time_series_growth = np.cumsum(np.random.normal(growth_rate/n_periods, 0.01, n_periods))\n",
    "    time_series_assets = base_assets * np.exp(time_series_growth)\n",
    "    \n",
    "    return {\n",
    "        'summary_statistics': {\n",
    "            'mean_growth_rate': growth_rate,\n",
    "            'ruin_probability': ruin_prob,\n",
    "            'mean_final_assets': final_assets,\n",
    "            'var_95': -np.random.uniform(50000, 200000),\n",
    "            'var_99': -np.random.uniform(100000, 500000),\n",
    "            'sharpe_ratio': growth_rate / 0.15,\n",
    "            'max_drawdown': np.random.uniform(0.05, 0.25)\n",
    "        },\n",
    "        'time_series': {\n",
    "            'growth_rate': time_series_growth,\n",
    "            'assets': time_series_assets\n",
    "        },\n",
    "        'config': param_overrides\n",
    "    }\n",
    "\n",
    "# Generate results for all scenarios\n",
    "results = {}\n",
    "for scenario in scenarios:\n",
    "    results[scenario.name] = create_synthetic_results(\n",
    "        scenario.name,\n",
    "        scenario.parameter_overrides\n",
    "    )\n",
    "\n",
    "print(f\"Generated results for {len(results)} scenarios\")\n",
    "print(\"\\nSample metrics for baseline:\")\n",
    "baseline_name = 'insurance_optimization_baseline'\n",
    "if baseline_name in results:\n",
    "    for metric, value in results[baseline_name]['summary_statistics'].items():\n",
    "        print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compare Scenarios Using the Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize comparator\n",
    "comparator = ScenarioComparator()\n",
    "\n",
    "# Compare scenarios\n",
    "comparison = comparator.compare_scenarios(\n",
    "    results,\n",
    "    baseline='insurance_optimization_baseline',\n",
    "    metrics=['mean_growth_rate', 'ruin_probability', 'mean_final_assets', 'sharpe_ratio']\n",
    ")\n",
    "\n",
    "print(\"Comparison Analysis Complete!\")\n",
    "print(f\"\\nScenarios compared: {', '.join(comparison.scenarios)}\")\n",
    "print(f\"\\nMetrics analyzed: {', '.join(comparison.metrics.keys())}\")\n",
    "\n",
    "# Show top performers\n",
    "print(\"\\n=== Top Performers ===\")\n",
    "for metric in ['mean_growth_rate', 'ruin_probability']:\n",
    "    ascending = 'risk' in metric or 'ruin' in metric\n",
    "    top = comparison.get_top_performers(metric, n=3, ascending=ascending)\n",
    "    print(f\"\\n{metric.replace('_', ' ').title()}:\")\n",
    "    for scenario, value in top:\n",
    "        print(f\"  {scenario}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Comparison Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison grid\n",
    "fig = comparator.create_comparison_grid(\n",
    "    metrics=['mean_growth_rate', 'ruin_probability', 'mean_final_assets', 'sharpe_ratio'],\n",
    "    figsize=(16, 10),\n",
    "    show_diff=True\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Comparison grid created with difference indicators from baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Extract and Display Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize insight extractor\n",
    "extractor = InsightExtractor()\n",
    "\n",
    "# Extract insights from comparison\n",
    "insights = extractor.extract_insights(\n",
    "    comparison,\n",
    "    focus_metrics=['mean_growth_rate', 'ruin_probability'],\n",
    "    threshold_importance=50\n",
    ")\n",
    "\n",
    "print(f\"Extracted {len(insights)} insights:\\n\")\n",
    "\n",
    "# Display top insights\n",
    "for i, insight in enumerate(insights[:5], 1):\n",
    "    print(f\"{i}. [{insight.category.upper()}] {insight.title}\")\n",
    "    print(f\"   {insight.description}\")\n",
    "    print(f\"   Importance: {insight.importance:.0f}/100 | Confidence: {insight.confidence:.1%}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Executive Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate executive summary\n",
    "executive_summary = extractor.generate_executive_summary(\n",
    "    max_points=5,\n",
    "    focus_positive=True\n",
    ")\n",
    "\n",
    "print(executive_summary)\n",
    "\n",
    "# Generate technical notes\n",
    "print(\"\\n## Technical Notes\\n\")\n",
    "technical_notes = extractor.generate_technical_notes()\n",
    "for note in technical_notes:\n",
    "    print(f\"• {note}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Demonstrate Smart Annotation Placement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample plot with smart annotations\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Generate sample time series data\n",
    "np.random.seed(42)\n",
    "x = np.arange(0, 100)\n",
    "y1 = np.cumsum(np.random.randn(100) * 0.5) + 50\n",
    "y2 = np.cumsum(np.random.randn(100) * 0.3) + 45\n",
    "y3 = np.cumsum(np.random.randn(100) * 0.4) + 40\n",
    "\n",
    "# Plot lines\n",
    "ax.plot(x, y1, label='Optimized Strategy', linewidth=2)\n",
    "ax.plot(x, y2, label='Baseline Strategy', linewidth=2)\n",
    "ax.plot(x, y3, label='Conservative Strategy', linewidth=2)\n",
    "\n",
    "# Initialize smart annotation placer\n",
    "placer = SmartAnnotationPlacer(ax)\n",
    "\n",
    "# Find key points to annotate\n",
    "annotations = [\n",
    "    {\n",
    "        'text': 'Optimized outperforms',\n",
    "        'point': (50, y1[50]),\n",
    "        'priority': 90,\n",
    "        'color': 'green'\n",
    "    },\n",
    "    {\n",
    "        'text': 'Baseline steady',\n",
    "        'point': (30, y2[30]),\n",
    "        'priority': 70,\n",
    "        'color': 'blue'\n",
    "    },\n",
    "    {\n",
    "        'text': 'Conservative lags',\n",
    "        'point': (70, y3[70]),\n",
    "        'priority': 60,\n",
    "        'color': 'orange'\n",
    "    },\n",
    "    {\n",
    "        'text': 'Convergence point',\n",
    "        'point': (20, (y1[20] + y2[20])/2),\n",
    "        'priority': 80,\n",
    "        'color': 'red'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Add annotations with smart placement\n",
    "placer.add_smart_annotations(annotations)\n",
    "\n",
    "# Automatically annotate peaks and valleys\n",
    "auto_annotate_peaks_valleys(ax, x, y1, n_peaks=2, n_valleys=1)\n",
    "\n",
    "# Add leader lines for emphasis\n",
    "create_leader_line(ax, (10, y1[10]), (15, y1[10]+5), style='curved', color='gray')\n",
    "\n",
    "# Format plot\n",
    "ax.set_xlabel('Time Period')\n",
    "ax.set_ylabel('Performance Metric')\n",
    "ax.set_title('Smart Annotation Demonstration with Automatic Placement')\n",
    "ax.legend(loc='best')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Smart annotations placed without overlaps!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create Parameter Difference Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create parameter difference tables for key scenarios\n",
    "for scenario_name in comparison.scenarios[1:3]:  # Show first 2 non-baseline scenarios\n",
    "    print(f\"\\n=== Parameter Differences: {scenario_name} ===\")\n",
    "    \n",
    "    diff_table = comparator.create_parameter_diff_table(\n",
    "        scenario_name,\n",
    "        threshold=5.0  # Show changes > 5%\n",
    "    )\n",
    "    \n",
    "    if not diff_table.empty:\n",
    "        print(diff_table.to_string(index=False))\n",
    "    else:\n",
    "        print(\"No significant parameter differences found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. A/B Testing Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create A/B testing visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Select two scenarios for A/B comparison\n",
    "scenario_a = 'insurance_optimization_baseline'\n",
    "scenario_b = list(comparison.scenarios)[1] if len(comparison.scenarios) > 1 else scenario_a\n",
    "\n",
    "metrics_to_compare = ['mean_growth_rate', 'ruin_probability', 'mean_final_assets', 'sharpe_ratio']\n",
    "\n",
    "for idx, metric in enumerate(metrics_to_compare):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Get values\n",
    "    val_a = comparison.metrics[metric].get(scenario_a, 0)\n",
    "    val_b = comparison.metrics[metric].get(scenario_b, 0)\n",
    "    \n",
    "    # Create bar plot\n",
    "    bars = ax.bar(['Scenario A\\n(Baseline)', 'Scenario B\\n(Alternative)'], \n",
    "                   [val_a, val_b],\n",
    "                   color=['#1f77b4', '#ff7f0e'],\n",
    "                   alpha=0.8)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars, [val_a, val_b]):\n",
    "        height = bar.get_height()\n",
    "        if 'probability' in metric or 'rate' in metric:\n",
    "            label = f'{val:.2%}'\n",
    "        elif 'assets' in metric:\n",
    "            label = f'${val/1e6:.1f}M'\n",
    "        else:\n",
    "            label = f'{val:.3f}'\n",
    "        \n",
    "        ax.text(bar.get_x() + bar.get_width()/2, height,\n",
    "               label, ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Calculate and show difference\n",
    "    diff = ((val_b - val_a) / val_a * 100) if val_a != 0 else 0\n",
    "    color = 'green' if diff > 0 else 'red'\n",
    "    \n",
    "    # Determine if improvement based on metric type\n",
    "    if 'risk' in metric or 'ruin' in metric or 'drawdown' in metric:\n",
    "        improvement = diff < 0\n",
    "    else:\n",
    "        improvement = diff > 0\n",
    "    \n",
    "    status = '✓ Better' if improvement else '✗ Worse'\n",
    "    \n",
    "    ax.text(0.5, 0.95, f'{diff:+.1f}% {status}',\n",
    "           transform=ax.transAxes,\n",
    "           ha='center', va='top',\n",
    "           fontsize=12, fontweight='bold',\n",
    "           color=color,\n",
    "           bbox=dict(boxstyle='round', facecolor='white', edgecolor=color, alpha=0.8))\n",
    "    \n",
    "    ax.set_title(metric.replace('_', ' ').title())\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "fig.suptitle(f'A/B Test: {scenario_a} vs {scenario_b}', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"A/B testing visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Comparison Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export comprehensive comparison report\n",
    "import os\n",
    "output_dir = 'comparison_reports'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "output_base = os.path.join(output_dir, 'scenario_comparison')\n",
    "\n",
    "# Export comparison data\n",
    "output_files = comparator.export_comparison_report(\n",
    "    output_base,\n",
    "    include_plots=True\n",
    ")\n",
    "\n",
    "print(\"Comparison report exported:\")\n",
    "for key, path in output_files.items():\n",
    "    print(f\"  - {key}: {path}\")\n",
    "\n",
    "# Export insights\n",
    "insights_path = os.path.join(output_dir, 'insights.md')\n",
    "extractor.export_insights(insights_path, format='markdown')\n",
    "print(f\"  - insights: {insights_path}\")\n",
    "\n",
    "# Display sample of the executive summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Sample Executive Summary:\")\n",
    "print(\"=\"*50)\n",
    "print(executive_summary[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the comprehensive scenario comparison and annotation framework:\n",
    "\n",
    "### Key Features Shown:\n",
    "1. **Scenario Comparison**: Side-by-side analysis of multiple scenarios\n",
    "2. **Parameter Diff Detection**: Automatic identification of parameter changes\n",
    "3. **Statistical Testing**: Significance analysis across scenarios\n",
    "4. **Smart Annotations**: Automatic placement without overlaps\n",
    "5. **Insight Extraction**: Automatic identification of key findings\n",
    "6. **Natural Language Generation**: Executive summaries and technical notes\n",
    "7. **A/B Testing**: Direct comparison visualizations\n",
    "8. **Leader Lines**: Intelligent routing for callouts\n",
    "9. **Peak/Valley Detection**: Automatic annotation of extrema\n",
    "10. **Export Capabilities**: Comprehensive reporting in multiple formats\n",
    "\n",
    "### Use Cases:\n",
    "- **Decision Support**: Compare different insurance strategies\n",
    "- **Sensitivity Analysis**: Understand parameter impacts\n",
    "- **Report Generation**: Automated insights for stakeholders\n",
    "- **Presentation Ready**: Professional visualizations with annotations\n",
    "\n",
    "The framework provides a complete solution for scenario analysis and comparison,\n",
    "making it easy to identify optimal configurations and communicate findings effectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
