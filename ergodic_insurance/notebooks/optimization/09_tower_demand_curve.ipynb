{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6aqk80hy_1x"
   },
   "source": [
    "# Tower Demand Curve for One Renewal Period\n",
    "\n",
    "## Overview\n",
    "Find the optimal insurance tower structure — deductible (retention) and maximum limit — across\n",
    "a range of loss ratios to formulate a demand curve for insurance.  The optimizer maximizes a 75/25 blend of ergodic growth rate and low log-growth volatility, subject to a hard constraint of no more than 0.5% probability of ruin over one renewal year.\n",
    "\n",
    "The tower is built from fixed layer breakpoints but the optimizer can \"squish\" it from both ends by choosing where to start (deductible) and where to stop (max limit).\n",
    "\n",
    "- **Prerequisites**: [optimization/01_optimization_overview](01_optimization_overview.ipynb)\n",
    "- **Audience**: [Practitioner] / [Developer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2327,
     "status": "ok",
     "timestamp": 1771575354618,
     "user": {
      "displayName": "Alex Filiakov",
      "userId": "03712643796933532488"
     },
     "user_tz": 300
    },
    "id": "n1opuX5vy_12",
    "outputId": "38347704-db81-4d85-dcf4-d7241d1f953d"
   },
   "outputs": [],
   "source": [
    "\"\"\"Google Colab setup: mount Drive and install package dependencies.\n",
    "\n",
    "Run this cell first. If prompted to restart the runtime, do so, then re-run all cells.\n",
    "This cell is a no-op when running locally.\n",
    "\"\"\"\n",
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    NOTEBOOK_DIR = '/content/drive/My Drive/Colab Notebooks/ei_notebooks/optimization'\n",
    "\n",
    "    os.chdir(NOTEBOOK_DIR)\n",
    "    if NOTEBOOK_DIR not in sys.path:\n",
    "        sys.path.append(NOTEBOOK_DIR)\n",
    "\n",
    "    !pip install ergodic-insurance -q 2>&1 | tail -3\n",
    "    print('\\nSetup complete. If you see numpy/scipy import errors below,')\n",
    "    print('restart the runtime (Runtime > Restart runtime) and re-run all cells.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geeprRCGy_14"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 721,
     "status": "ok",
     "timestamp": 1771575355345,
     "user": {
      "displayName": "Alex Filiakov",
      "userId": "03712643796933532488"
     },
     "user_tz": 300
    },
    "id": "wqNTpoVZy_15",
    "outputId": "9b857ae2-6920-4d90-fe2b-704bb6bbb0e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPU cores: 16\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "import multiprocessing\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from ergodic_insurance.pareto_frontier import (\n",
    "    Objective, ObjectiveType, ParetoFrontier, ParetoPoint,\n",
    ")\n",
    "from ergodic_insurance.config import ManufacturerConfig\n",
    "from ergodic_insurance.manufacturer import WidgetManufacturer\n",
    "from ergodic_insurance.insurance_program import (\n",
    "    EnhancedInsuranceLayer, InsuranceProgram,\n",
    ")\n",
    "from ergodic_insurance.loss_distributions import ManufacturingLossGenerator\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "N_CORES = multiprocessing.cpu_count()\n",
    "print(f\"Number of CPU cores: {N_CORES}\")  # Available parallel cores for sensitivity sweep\n",
    "CI = False      # Set True to skip heavy computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fhE0J3eM2MIL"
   },
   "source": [
    "## Part I: Parameter Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52krAvZH0IP2"
   },
   "source": [
    "### Manufacturing Company Configuration\n",
    "\n",
    "Baseline company parameters (the experiment varies revenue via `revenue_grid`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1771575355387,
     "user": {
      "displayName": "Alex Filiakov",
      "userId": "03712643796933532488"
     },
     "user_tz": 300
    },
    "id": "JVqpuH1e0amx",
    "outputId": "3a2860d3-4290-49c1-bbb4-69e5dc10148e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MANUFACTURING COMPANY PROFILE\n",
      "============================================================\n",
      "Total Assets:          $     5,000,000\n",
      "Annual Revenue:        $    10,000,000\n",
      "Operating Income:      $     1,500,000\n",
      "Operating Margin:               15.0%\n",
      "Asset Turnover:                   2.0x\n",
      "Revenue Volatility:               0.5\n",
      "Tax Rate:                      25.0%\n",
      "Retention Ratio:               70.0%\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# --- Economic Parameters\n",
    "ATR = 2.0                # Asset turnover ratio\n",
    "OPERATING_MARGIN = 0.15  # 12% EBIT margin before Insurable Losses\n",
    "REV_VOL = 0.50           # Revenue volatility (annualized)\n",
    "INITIAL_ASSETS = 5_000_000\n",
    "\n",
    "# --- Company Configuration ---\n",
    "mfg_config = ManufacturerConfig(\n",
    "    initial_assets=INITIAL_ASSETS,          # $15M total assets\n",
    "    asset_turnover_ratio=ATR,               # Revenue = Assets Ãƒâ€” turnover = $22.5M\n",
    "    base_operating_margin=OPERATING_MARGIN, # 12% EBIT margin -> $2.7M/yr operating income\n",
    "    tax_rate=0.25,                          # 25% corporate tax\n",
    "    retention_ratio=0.70,                   # 70% earnings retained for growth\n",
    ")\n",
    "\n",
    "# Display company profile\n",
    "revenue = mfg_config.initial_assets * mfg_config.asset_turnover_ratio\n",
    "ebit = revenue * mfg_config.base_operating_margin\n",
    "print(\"=\" * 60)\n",
    "print(\"MANUFACTURING COMPANY PROFILE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total Assets:          ${mfg_config.initial_assets:>14,.0f}\")\n",
    "print(f\"Annual Revenue:        ${revenue:>14,.0f}\")\n",
    "print(f\"Operating Income:      ${ebit:>14,.0f}\")\n",
    "print(f\"Operating Margin:      {mfg_config.base_operating_margin:>14.1%}\")\n",
    "print(f\"Asset Turnover:        {mfg_config.asset_turnover_ratio:>14.1f}x\")\n",
    "print(f\"Revenue Volatility:    {REV_VOL:>14}\")\n",
    "print(f\"Tax Rate:              {mfg_config.tax_rate:>13.1%}\")\n",
    "print(f\"Retention Ratio:       {mfg_config.retention_ratio:>13.1%}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5kXbr46X1SEs"
   },
   "source": [
    "### Shared Simulation Infrastructure\n",
    "\n",
    "Loss model, analytical LEV-based layer pricing, CRN scenario generation, and simulation engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5874,
     "status": "ok",
     "timestamp": 1771575361267,
     "user": {
      "displayName": "Alex Filiakov",
      "userId": "03712643796933532488"
     },
     "user_tz": 300
    },
    "id": "sFTJeZfb1UJN",
    "outputId": "b15490e2-1e33-490b-fe51-46101b4e57aa"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import warnings\n",
    "import logging\n",
    "import time\n",
    "\n",
    "# Suppress all warnings and verbose solver logging\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger(\"ergodic_insurance\").setLevel(logging.ERROR)\n",
    "\n",
    "from ergodic_insurance.hjb_solver import (\n",
    "    StateVariable, ControlVariable, StateSpace,\n",
    "    LogUtility, PowerUtility, ExpectedWealth,\n",
    "    HJBProblem, HJBSolver, HJBSolverConfig,\n",
    ")\n",
    "from ergodic_insurance.optimal_control import (\n",
    "    ControlSpace, StaticControl, HJBFeedbackControl,\n",
    "    TimeVaryingControl, OptimalController,\n",
    ")\n",
    "from ergodic_insurance.config import ManufacturerConfig\n",
    "from ergodic_insurance.manufacturer import WidgetManufacturer\n",
    "from ergodic_insurance.insurance_program import (\n",
    "    EnhancedInsuranceLayer, InsuranceProgram,\n",
    ")\n",
    "from ergodic_insurance.loss_distributions import (\n",
    "    ManufacturingLossGenerator, LognormalLoss, ParetoLoss,\n",
    ")\n",
    "from ergodic_insurance.insurance_pricing import LayerPricer\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "N_CORES = 40   # Available parallel cores for sensitivity sweep\n",
    "CI = False      # Set True to skip heavy computations\n",
    "\n",
    "# =====================================================\n",
    "# SHARED SIMULATION INFRASTRUCTURE\n",
    "# =====================================================\n",
    "# Used by Parts 5, 8, 9, 10, and 11.\n",
    "\n",
    "# --- Economic Parameters ---\n",
    "REFERENCE_REVENUE = ATR * INITIAL_ASSETS  # Fixed reference for loss calibration\n",
    "\n",
    "# --- Loss Scaling ---\n",
    "# Loss frequency (and CRN loss amounts) scale with the square root of\n",
    "# revenue.  This keeps the loss drag proportional to the company's\n",
    "# actual size for both insured and uninsured strategies.\n",
    "FREQ_SCALING_EXPONENT = 0.75\n",
    "\n",
    "# --- Amplified Loss Model ---\n",
    "ATTR_BASE_FREQ = 5\n",
    "ATTR_SEV_MEAN = 10_000\n",
    "ATTR_SEV_CV = 10\n",
    "\n",
    "LG_BASE_FREQ = 1.75\n",
    "LG_SEV_MEAN = 400_000\n",
    "LG_SEV_CV = 5\n",
    "\n",
    "CAT_BASE_FREQ = 0.25\n",
    "CAT_SEV_ALPHA = 2.01\n",
    "CAT_SEV_XM = 800_000\n",
    "\n",
    "LOSS_PARAMS = dict(\n",
    "    attritional_params={'base_frequency': ATTR_BASE_FREQ,\n",
    "                        'severity_mean': ATTR_SEV_MEAN,\n",
    "                        'severity_cv': ATTR_SEV_CV,\n",
    "                        'revenue_scaling_exponent': FREQ_SCALING_EXPONENT,\n",
    "                        'reference_revenue': REFERENCE_REVENUE},\n",
    "    large_params={'base_frequency': LG_BASE_FREQ,\n",
    "                  'severity_mean': LG_SEV_MEAN,\n",
    "                  'severity_cv': LG_SEV_CV,\n",
    "                  'revenue_scaling_exponent': FREQ_SCALING_EXPONENT,\n",
    "                  'reference_revenue': REFERENCE_REVENUE},\n",
    "    catastrophic_params={'base_frequency': CAT_BASE_FREQ,\n",
    "                         'severity_alpha': CAT_SEV_ALPHA,\n",
    "                         'severity_xm': CAT_SEV_XM,\n",
    "                         'revenue_scaling_exponent': FREQ_SCALING_EXPONENT,\n",
    "                         'reference_revenue': REFERENCE_REVENUE},\n",
    ")\n",
    "\n",
    "# Quick validation of the loss model\n",
    "_val_gen = ManufacturingLossGenerator(**LOSS_PARAMS, seed=99)\n",
    "_val_totals = []\n",
    "SCENARIOS = 10_000\n",
    "for _ in range(SCENARIOS):\n",
    "    _events, _stats = _val_gen.generate_losses(duration=1.0, revenue=REFERENCE_REVENUE)\n",
    "    _val_totals.append(_stats['total_amount'])\n",
    "_expected_annual_loss = np.mean(_val_totals)\n",
    "_operating_income = INITIAL_ASSETS * ATR * OPERATING_MARGIN\n",
    "print(f\"Loss model validation ({SCENARIOS:,.0f} one-year samples):\")\n",
    "print(f\"  Expected annual loss:  ${_expected_annual_loss:>12,.0f}\")\n",
    "print(f\"  Operating income:      ${_operating_income:>12,.0f}\")\n",
    "print(f\"  Loss / Income ratio:   {_expected_annual_loss / _operating_income:.0%}\")\n",
    "print(f\"  Std dev annual loss:   ${np.std(_val_totals):>12,.0f}\")\n",
    "print(f\"  Max annual loss:       ${np.max(_val_totals):>12,.0f}\")\n",
    "del _val_gen, _val_totals, _events, _stats\n",
    "\n",
    "\n",
    "# --- Analytical Layer Pricing via LEV ---\n",
    "# Instead of hardcoded rate-on-line values, we compute actuarially sound\n",
    "# premiums from the known severity distributions using limited expected\n",
    "# values (LEVs).  For each layer (attachment a, limit l):\n",
    "#\n",
    "#   E[layer loss] = sum_i  freq_i * [LEV_i(a+l) - LEV_i(a)]\n",
    "#   premium       = E[layer loss] / target_loss_ratio\n",
    "#   rate_on_line  = premium / limit\n",
    "#\n",
    "# This ensures the primary-layer ROL decreases naturally as the Ded\n",
    "# (retention) rises, producing the genuine cost-vs-variance tradeoff\n",
    "# that the HJB solver needs.\n",
    "#\n",
    "# The pricers are parameterized so that the sensitivity analysis (Part 9)\n",
    "# can adapt premiums to match the modified loss assumptions being tested.\n",
    "\n",
    "TARGET_LOSS_RATIO = 0.7  # Normal-market loss ratio\n",
    "LOSS_RATIO_INFLECTION = 1  # Factor by which the max layer loss ratio differs from the base\n",
    "\n",
    "def make_layer_pricers(large_freq=LG_BASE_FREQ,\n",
    "                        large_sev_mean=LG_SEV_MEAN,\n",
    "                        cur_revenue=REFERENCE_REVENUE) -> tuple:\n",
    "    \"\"\"Create a tuple of LayerPricers for a given loss parameterization.\n",
    "\n",
    "    Frequency scales as (revenue / reference)^0.5, matching the loss\n",
    "    model's sub-linear revenue scaling.  This keeps premium and loss\n",
    "    scaling consistent so that insured and uninsured strategies face\n",
    "    the same proportional cost growth.\n",
    "\n",
    "    Args:\n",
    "        large_freq: Large-loss annual frequency (default 1.0).\n",
    "        large_sev_mean: Large-loss mean severity (default $1M).\n",
    "        cur_revenue: Current revenue for frequency scaling.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (attritional, large, catastrophic) LayerPricers.\n",
    "    \"\"\"\n",
    "    scale = (cur_revenue / REFERENCE_REVENUE) ** FREQ_SCALING_EXPONENT\n",
    "    return (\n",
    "        LayerPricer(LognormalLoss(mean=ATTR_SEV_MEAN, cv=ATTR_SEV_CV),\n",
    "                    frequency=ATTR_BASE_FREQ * scale),\n",
    "        LayerPricer(LognormalLoss(mean=large_sev_mean, cv=LG_SEV_CV),\n",
    "                    frequency=large_freq * scale),\n",
    "        LayerPricer(ParetoLoss(alpha=CAT_SEV_ALPHA, xm=CAT_SEV_XM),\n",
    "                    frequency=CAT_BASE_FREQ * scale),\n",
    "    )\n",
    "\n",
    "\n",
    "# Default pricers for baseline loss model\n",
    "DEFAULT_PRICERS = make_layer_pricers()\n",
    "\n",
    "MIN_LAYER_MIDPOINT = np.mean([0, 5_000_000])\n",
    "MAX_LAYER_MIDPOINT = np.mean([450_000_000, 500_000_000])\n",
    "\n",
    "def analytical_layer_premium(attachment: float, limit: float,\n",
    "                            base_loss_ratio: float,\n",
    "                            loss_ratio_inflection: float,\n",
    "                            pricers=None) -> float:\n",
    "    \"\"\"Compute actuarial premium for a layer using LEV-based expected losses.\n",
    "\n",
    "    Premium = E[layer loss] / layer_loss_ratio, where:\n",
    "      E[layer loss] = sum over components of freq_i * (LEV_i(a+l) - LEV_i(a))\n",
    "\n",
    "    Args:\n",
    "        attachment: Layer attachment point.\n",
    "        limit: Layer limit (width of coverage).\n",
    "        pricers: Tuple of LayerPricers. Uses DEFAULT_PRICERS if None.\n",
    "        base_loss_ratio: Desired loss ratio for the layer.\n",
    "        loss_ratio_inflection: Factor by which the max layer loss ratio  differs from the base.\n",
    "    \"\"\"\n",
    "    \n",
    "    pricers = pricers or DEFAULT_PRICERS\n",
    "    expected_loss = sum(p.expected_layer_loss(attachment, limit) for p in pricers)\n",
    "    cur_layer_midpoint = np.mean([attachment, attachment + limit])\n",
    "    layer_loss_ratio = base_loss_ratio + \\\n",
    "                        (1.0 / loss_ratio_inflection - 1.0) * base_loss_ratio \\\n",
    "                        * (cur_layer_midpoint - MIN_LAYER_MIDPOINT) \\\n",
    "                        / (MAX_LAYER_MIDPOINT - MIN_LAYER_MIDPOINT)\n",
    "    return expected_loss / layer_loss_ratio\n",
    "\n",
    "\n",
    "def analytical_rate_on_line(attachment: float, limit: float,\n",
    "                            base_loss_ratio: float,\n",
    "                            loss_ratio_inflection: float,\n",
    "                            pricers=None) -> float:\n",
    "    \"\"\"Compute rate-on-line for a layer: premium / limit.\"\"\"\n",
    "    if limit <= 0:\n",
    "        return 0.0\n",
    "    return analytical_layer_premium(attachment, \n",
    "                                    limit, \n",
    "                                    base_loss_ratio, \n",
    "                                    loss_ratio_inflection, \n",
    "                                    pricers) / limit\n",
    "\n",
    "\n",
    "# Validate: show how ROL varies across sample attachment points\n",
    "print(f\"\\nAnalytical layer pricing (target LR = {TARGET_LOSS_RATIO:.0%}):\")\n",
    "print(f\"  {'Attachment':>12s}  {'Limit':>12s}  {'E[Loss]':>12s}  {'Premium':>12s}  {'ROL':>8s}\")\n",
    "print(f\"  {'-'*12}  {'-'*12}  {'-'*12}  {'-'*12}  {'-'*8}\")\n",
    "for _a, _l in [(10_000, 4_990_000), (25_000, 4_975_000), (50_000, 4_950_000),\n",
    "                (250_000, 4_750_000), (1_000_000, 4_000_000),\n",
    "                (2_000_000, 3_000_000), (4_000_000, 1_000_000),\n",
    "                (5_000_000, 20_000_000), (25_000_000, 25_000_000), (50_000_000, 50_000_000)]:\n",
    "    _el = analytical_layer_premium(_a, _l, TARGET_LOSS_RATIO, 1.0) * TARGET_LOSS_RATIO\n",
    "    _p = analytical_layer_premium(_a, _l, TARGET_LOSS_RATIO, 1.0)\n",
    "    _r = analytical_rate_on_line(_a, _l, TARGET_LOSS_RATIO, 1.0)\n",
    "    print(f\"  ${_a:>11,.0f}  ${_l:>11,.0f}  ${_el:>11,.0f}  ${_p:>11,.0f}  {_r:>7.2%}\")\n",
    "\n",
    "\n",
    "# --- Insurance Tower Factory ---\n",
    "# Premium rates are computed analytically from the loss distribution,\n",
    "# ensuring that the primary-layer ROL decreases with higher retention.\n",
    "# The optional `pricers` argument lets the sensitivity analysis pass\n",
    "# in LayerPricers built from alternative loss assumptions, so that\n",
    "# premiums stay consistent with the loss environment being tested.\n",
    "\n",
    "def make_program(ded: float,\n",
    "                base_loss_ratio: float,\n",
    "                loss_ratio_inflection: float,\n",
    "                max_limit: float,\n",
    "                pricers=None) -> InsuranceProgram:\n",
    "    \"\"\"Create 4-layer tower with analytically priced premiums.\n",
    "\n",
    "    Uses LEV-based layer pricing from severity distributions so that\n",
    "    rate-on-line adjusts naturally with the retention level.\n",
    "\n",
    "    Args:\n",
    "        ded: Deductible.\n",
    "        max_limit: Maximum coverage limit (top of tower).\n",
    "        pricers: Tuple of LayerPricers. Uses DEFAULT_PRICERS if None.\n",
    "\n",
    "    Returns:\n",
    "        InsuranceProgram with actuarially sound premium loading.\n",
    "    \"\"\"\n",
    "    layer_defs = [\n",
    "        # (attachment, ceiling, reinstatements)\n",
    "        (0, 5_000_000, 0),\n",
    "        (5_000_000, 10_000_000, 0),\n",
    "        (10_000_000, 25_000_000, 0),\n",
    "        (25_000_000, 50_000_000, 0),\n",
    "        (50_000_000, 100_000_000, 0),\n",
    "        (100_000_000, 150_000_000, 0),\n",
    "        (150_000_000, 200_000_000, 0),\n",
    "        (200_000_000, 250_000_000, 0),\n",
    "        (250_000_000, 300_000_000, 0),\n",
    "        (300_000_000, 350_000_000, 0),\n",
    "        (350_000_000, 400_000_000, 0),\n",
    "        (400_000_000, 450_000_000, 0),\n",
    "        (450_000_000, 500_000_000, 0),\n",
    "    ]\n",
    "    layers = []\n",
    "    for attach, ceiling, reinst in layer_defs:\n",
    "        if ded >= ceiling:\n",
    "            continue  # Skip layers that are fully below the deductible\n",
    "        if max_limit is not None and ceiling > max_limit:\n",
    "            continue  # Skip layers that exceed the max limit constraint\n",
    "        # Now we're within the working layer\n",
    "        # The deductible is below, the max limit is above\n",
    "        effective_attach = max(attach, ded)\n",
    "        limit = ceiling - effective_attach\n",
    "        if limit <= 0:\n",
    "            continue\n",
    "        rol = analytical_rate_on_line(effective_attach, limit, base_loss_ratio, loss_ratio_inflection, pricers)\n",
    "        layers.append(EnhancedInsuranceLayer(\n",
    "            attachment_point=effective_attach,\n",
    "            limit=limit,\n",
    "            base_premium_rate=rol,\n",
    "            reinstatements=reinst,\n",
    "        ))\n",
    "    return InsuranceProgram(\n",
    "        layers=layers,\n",
    "        deductible=ded,\n",
    "        name=f\"Manufacturing Tower (Ded=${ded:,.0f})\",\n",
    "    )\n",
    "\n",
    "\n",
    "# --- CRN: Pre-generate Loss Scenarios ---\n",
    "def generate_loss_pool(n_paths, n_years, reference_revenue=REFERENCE_REVENUE, seed=SEED, specific_loss_params=None):\n",
    "    \"\"\"Pre-generate loss scenarios for Common Random Number comparison.\n",
    "    All strategies will face the exact same loss events and revenue shocks.\n",
    "    Losses are generated at a fixed reference revenue; the simulation\n",
    "    engine then scales event amounts by (actual_revenue / reference)^0.5\n",
    "    so that loss burden grows proportionally with the company.\n",
    "\n",
    "    Args:\n",
    "        n_paths: Number of simulation paths.\n",
    "        n_years: Number of years to simulate.\n",
    "        reference_revenue: Reference revenue for loss calibration.\n",
    "        seed: Base seed for random number generation.\n",
    "        specific_loss_params: A dictionary of loss parameters to use, overriding\n",
    "                              the global LOSS_PARAMS for this call.\n",
    "    \"\"\"\n",
    "    loss_params_to_use = specific_loss_params if specific_loss_params is not None else LOSS_PARAMS\n",
    "\n",
    "    ss = np.random.SeedSequence(seed)\n",
    "    children = ss.spawn(n_paths + 1)\n",
    "\n",
    "    # Shared revenue shocks\n",
    "    rev_rng = np.random.default_rng(children[0])\n",
    "    revenue_shocks = rev_rng.standard_normal((n_paths, n_years))\n",
    "\n",
    "    # Per-path loss event sequences\n",
    "    all_losses = []  # [path][year] -> List[LossEvent]\n",
    "    for i in range(n_paths):\n",
    "        gen = ManufacturingLossGenerator(\n",
    "            **loss_params_to_use, # Use the potentially overridden loss params\n",
    "            seed=int(children[i + 1].generate_state(1)[0] % (2**31)),\n",
    "        )\n",
    "        path_losses = []\n",
    "        for t in range(n_years):\n",
    "            events, _ = gen.generate_losses(duration=1.0, revenue=reference_revenue)\n",
    "            path_losses.append(events)\n",
    "        all_losses.append(path_losses)\n",
    "\n",
    "    return revenue_shocks, all_losses\n",
    "\n",
    "\n",
    "# --- CRN Simulation Engine ---\n",
    "def simulate_with_crn(ded, \n",
    "                    base_loss_ratio: float,\n",
    "                    loss_ratio_inflection: float,\n",
    "                    max_limit: float,\n",
    "                    revenue_shocks, loss_pool, n_years=1,\n",
    "                    initial_assets=INITIAL_ASSETS, pricers=None):\n",
    "    \"\"\"Simulate one static-Ded strategy across all CRN paths.\n",
    "\n",
    "    Uses the library's InsuranceProgram.process_claim() to correctly\n",
    "    allocate each loss through the insurance tower.\n",
    "\n",
    "    Loss amounts from the CRN pool are scaled by\n",
    "    (actual_revenue / REFERENCE_REVENUE)^FREQ_SCALING_EXPONENT so that\n",
    "    the loss burden grows proportionally with the company.  Premium is\n",
    "    repriced at actual revenue with the same exponent, keeping the\n",
    "    cost-of-risk consistent between insured and uninsured strategies.\n",
    "\n",
    "    Args:\n",
    "        ded: Deductible.\n",
    "        revenue_shocks: Pre-generated revenue shocks (n_paths x n_years).\n",
    "        loss_pool: Pre-generated loss events [path][year] -> List[LossEvent].\n",
    "        n_years: Simulation horizon.\n",
    "        initial_assets: Starting wealth.\n",
    "        pricers: Tuple of LayerPricers for premium calculation.\n",
    "            Uses DEFAULT_PRICERS if None (baseline loss assumptions).\n",
    "\n",
    "    Returns:\n",
    "        paths: array of shape (n_paths, n_years + 1) with asset values.\n",
    "    \"\"\"\n",
    "    n_paths = len(loss_pool)\n",
    "    paths = np.zeros((n_paths, n_years + 1))\n",
    "    paths[:, 0] = initial_assets\n",
    "\n",
    "    # Build program template and get fixed annual premium\n",
    "    if ded >= 100_000_000:\n",
    "        # \"No insurance\" -- skip tower entirely\n",
    "        annual_premium = 0.0\n",
    "        use_insurance = False\n",
    "        program_template = None\n",
    "    else:\n",
    "        program_template = make_program(ded, \n",
    "                                        base_loss_ratio, \n",
    "                                        loss_ratio_inflection, \n",
    "                                        max_limit,\n",
    "                                        pricers=pricers)\n",
    "        annual_premium = program_template.calculate_premium()\n",
    "        use_insurance = True\n",
    "\n",
    "    for i in range(n_paths):\n",
    "        assets = initial_assets\n",
    "        for t in range(n_years):\n",
    "            # Operating income with shared revenue shock\n",
    "            revenue = assets * ATR * np.exp(\n",
    "                REV_VOL * revenue_shocks[i, t] - 0.5 * REV_VOL**2\n",
    "            )\n",
    "            operating_income = revenue * OPERATING_MARGIN\n",
    "\n",
    "            # Scale CRN losses to current revenue (sqrt scaling)\n",
    "            loss_scale = (revenue / REFERENCE_REVENUE) ** FREQ_SCALING_EXPONENT\n",
    "\n",
    "            # Process losses through insurance tower\n",
    "            total_retained = 0.0\n",
    "            if use_insurance:\n",
    "                new_pricers = make_layer_pricers(cur_revenue=revenue)\n",
    "                program_update = make_program(ded, \n",
    "                                                base_loss_ratio, \n",
    "                                                loss_ratio_inflection, \n",
    "                                                max_limit,\n",
    "                                                pricers=new_pricers)\n",
    "                annual_premium = program_update.calculate_premium()\n",
    "                program = InsuranceProgram.create_fresh(program_update)\n",
    "                for event in loss_pool[i][t]:\n",
    "                    scaled_amount = event.amount * loss_scale\n",
    "                    result = program.process_claim(scaled_amount)\n",
    "                    total_retained += result.deductible_paid + result.uncovered_loss\n",
    "            else:\n",
    "                for event in loss_pool[i][t]:\n",
    "                    total_retained += event.amount * loss_scale\n",
    "\n",
    "            # Net income and asset update\n",
    "            assets = assets + operating_income - total_retained - annual_premium\n",
    "            assets = max(assets, 0.0)\n",
    "            paths[i, t + 1] = assets\n",
    "\n",
    "    return paths\n",
    "\n",
    "\n",
    "# Pre-generate the main CRN pool\n",
    "N_PATHS = 500\n",
    "N_YEARS = 1\n",
    "print(f\"\\nPre-generating CRN loss pool ({N_PATHS:,.0f} paths x {N_YEARS} years)...\")\n",
    "t0 = time.time()\n",
    "CRN_SHOCKS, CRN_LOSSES = generate_loss_pool(n_paths=N_PATHS, n_years=N_YEARS)\n",
    "print(f\"  Done in {time.time() - t0:.1f}s\")\n",
    "print(f\"  Shape: {CRN_SHOCKS.shape[0]:,} paths x {CRN_SHOCKS.shape[1]} years\")\n",
    "\n",
    "# Quick sanity: total losses per path-year\n",
    "_annual_totals = [\n",
    "    sum(e.amount for e in CRN_LOSSES[i][t])\n",
    "    for i in range(N_PATHS) for t in range(N_YEARS)\n",
    "]\n",
    "print(f\"  Mean annual loss: ${np.mean(_annual_totals):,.0f}\")\n",
    "del _annual_totals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4u_vNHU8r2w"
   },
   "source": [
    "## Part II: Insurance Demand Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pg-aY2jL8r2w"
   },
   "source": [
    "For each revenue level in `revenue_grid`, we sweep all valid `(ded, max_limit)` pairs\n",
    "from `ded_grid` × `max_limit_grid` and simulate 10,000 one-year CRN paths.\n",
    "\n",
    "The optimal tower at each revenue is selected by maximizing a **weighted objective**:\n",
    "- **75% weight** on annual ergodic growth rate\n",
    "- **25% weight** on reducing result volatility (std of log-growth)\n",
    "\n",
    "subject to the **hard constraint** that the probability of ruin does not exceed **1%**.\n",
    "\n",
    "Losses scale with revenue via a **0.75 exponent**, and the tower is \"squished\" from both\n",
    "the deductible (bottom) and the max limit (top) by the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1771575361285,
     "user": {
      "displayName": "Alex Filiakov",
      "userId": "03712643796933532488"
     },
     "user_tz": 300
    },
    "id": "rd7LR-Nhy_17",
    "outputId": "bcb59cc9-e565-4e8c-ab9f-c38dda225dca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deductible grid:  68 values ($0 -- $250,000,000)\n",
      "Max limit grid:   72 values ($500,000 -- $500,000,000)\n",
      "Revenue grid:     24 values ($2,000,000 -- $100,000,000)\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "ded_grid = np.unique(np.concatenate([\n",
    "    [0, 10_000, 15_000, 25_000, 50_000, 75_000,\n",
    "    100_000, 125_000, 150_000, 175_000, 200_000, 250_000,\n",
    "    300_000, 350_000, 400_000, 450_000, 500_000, 750_000, 1_000_000,\n",
    "    1_250_000, 1_500_000, 1_750_000, 2_000_000, 2_500_000,\n",
    "    3_000_000, 3_500_000, 4_000_000, 4_500_000,\n",
    "    5_000_000, 7_500_000, 10_000_000, 15_000_000,\n",
    "    20_000_000, 25_000_000, 30_000_000, 35_000_000,\n",
    "    40_000_000, 45_000_000, 50_000_000],  # landmarks\n",
    "    np.geomspace(50_000_000, 250_000_000, 30)[1:], # excess range\n",
    "]))\n",
    "\n",
    "max_limit_grid = np.unique(np.concatenate([\n",
    "    [500_000, 750_000, 1_000_000,\n",
    "    1_250_000, 1_500_000, 1_750_000, 2_000_000, 2_500_000,\n",
    "    3_000_000, 3_500_000, 4_000_000, 4_500_000,\n",
    "    5_000_000, 7_500_000, 10_000_000, 15_000_000,\n",
    "    20_000_000, 25_000_000, 30_000_000, 35_000_000,\n",
    "    40_000_000, 45_000_000, 50_000_000],  # landmarks\n",
    "    np.geomspace(50_000_000, 500_000_000, 50)[1:], # excess range\n",
    "]))\n",
    "\n",
    "### 3 parameters to sweep: revenue, base loss ratio, and loss ratio inflection factor\n",
    "# This parameter forms the base demand curve\n",
    "base_loss_ratio_grid = np.linspace(0.4, 0.9, 11)\n",
    "\n",
    "# This parameter tests demand sensitivity to upper layer loss ratios\n",
    "inflection_factor_grid = np.array([\n",
    "    1.0,\n",
    "    2.0,\n",
    "    3.0,\n",
    "])\n",
    "\n",
    "# This parameter tests demand sensitivity to company revenue\n",
    "revenue_grid = np.array([\n",
    "    5_000_000,\n",
    "    25_000_000,\n",
    "    50_000_000,\n",
    "])\n",
    "\n",
    "# Grid for parallel sweeping to determine demand curves\n",
    "scan_grid = list(itertools.product(revenue_grid, base_loss_ratio_grid, inflection_factor_grid))\n",
    "\n",
    "print(f\"Deductible grid:  {len(ded_grid)} values \"\n",
    "        f\"(${ded_grid[0]:,.0f} -- ${ded_grid[-1]:,.0f})\")\n",
    "print(f\"Max limit grid:   {len(max_limit_grid)} values \"\n",
    "        f\"(${max_limit_grid[0]:,.0f} -- ${max_limit_grid[-1]:,.0f})\")\n",
    "print(f\"Revenue grid:     {len(revenue_grid)} values \"\n",
    "        f\"(${revenue_grid[0]:,.0f} -- ${revenue_grid[-1]:,.0f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10893747,
     "status": "ok",
     "timestamp": 1771586255037,
     "user": {
      "displayName": "Alex Filiakov",
      "userId": "03712643796933532488"
     },
     "user_tz": 300
    },
    "id": "VPJbzkn28r2w",
    "outputId": "4f670602-c7a1-41f1-f46c-3338626955aa"
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Use correct N_CORES (cell 3 value, not the hardcoded 40 in cell 9)\n",
    "N_CORES = multiprocessing.cpu_count()\n",
    "\n",
    "# --- Optimization criteria ---\n",
    "GROWTH_WEIGHT = 0.75       # weight on annual ergodic growth rate\n",
    "VOL_WEIGHT    = 0.25       # weight on reducing log-growth volatility\n",
    "RUIN_LIMIT    = 0.005       # hard constraint: P(ruin) <= 1%\n",
    "SENS_N_PATHS  = 500\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger(\"ergodic_insurance\").setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "# --- Fixed make_program: truncates layers at max_limit instead of skipping ---\n",
    "def make_program(ded, \n",
    "                base_loss_ratio: float,\n",
    "                loss_ratio_inflection: float,\n",
    "                max_limit=None, \n",
    "                pricers=None,):\n",
    "    \"\"\"Build multi-layer tower truncated by ded (bottom) and max_limit (top).\n",
    "\n",
    "    The tower uses fixed layer breakpoints at $5M / $25M / $50M / $100M.\n",
    "    Layers that straddle the deductible or max_limit are truncated (not dropped),\n",
    "    so the optimizer can smoothly squish the tower from both ends.\n",
    "    \"\"\"\n",
    "    layer_defs = [\n",
    "        (0, 5_000_000, 0),\n",
    "        (5_000_000, 10_000_000, 0),\n",
    "        (10_000_000, 25_000_000, 0),\n",
    "        (25_000_000, 50_000_000, 0),\n",
    "        (50_000_000, 100_000_000, 0),\n",
    "        (100_000_000, 150_000_000, 0),\n",
    "        (150_000_000, 200_000_000, 0),\n",
    "        (200_000_000, 250_000_000, 0),\n",
    "        (250_000_000, 300_000_000, 0),\n",
    "        (300_000_000, 350_000_000, 0),\n",
    "        (350_000_000, 400_000_000, 0),\n",
    "        (400_000_000, 450_000_000, 0),\n",
    "        (450_000_000, 500_000_000, 0),\n",
    "    ]\n",
    "    layers = []\n",
    "    for attach, ceiling, reinst in layer_defs:\n",
    "        if ded >= ceiling:\n",
    "            continue\n",
    "        if max_limit is not None and max_limit <= attach:\n",
    "            continue\n",
    "        effective_attach = max(attach, ded)\n",
    "        effective_ceiling = min(ceiling, max_limit) if max_limit is not None else ceiling\n",
    "        limit = effective_ceiling - effective_attach\n",
    "        if limit <= 0:\n",
    "            continue\n",
    "        rol = analytical_rate_on_line(effective_attach,\n",
    "                                        limit,\n",
    "                                        base_loss_ratio,\n",
    "                                        loss_ratio_inflection,\n",
    "                                        pricers)\n",
    "        layers.append(EnhancedInsuranceLayer(\n",
    "            attachment_point=effective_attach,\n",
    "            limit=limit,\n",
    "            base_premium_rate=rol,\n",
    "            reinstatements=reinst,\n",
    "        ))\n",
    "    name = f\"Tower Ded=${ded:,.0f}\"\n",
    "    if max_limit is not None:\n",
    "        name += f\" Lim=${max_limit:,.0f}\"\n",
    "    return InsuranceProgram(layers=layers, deductible=ded, name=name)\n",
    "\n",
    "\n",
    "def _optimize_tower_for_revenue(args):\n",
    "    \"\"\"Sweep all valid (ded, max_limit) combos for one revenue level.\n",
    "\n",
    "    Generates a CRN loss pool at this revenue, then simulates each combo\n",
    "    over SENS_N_PATHS one-year paths.  Returns all records for post-hoc\n",
    "    optimal selection.\n",
    "    \"\"\"\n",
    "    revenue, base_lr, inf_lr, ded_arr, ml_arr, seed_base = args\n",
    "    initial_assets = revenue / ATR\n",
    "\n",
    "    shocks, losses = generate_loss_pool(\n",
    "        n_paths=SENS_N_PATHS, n_years=1,\n",
    "        reference_revenue=revenue, seed=seed_base,\n",
    "    )\n",
    "    pricers = make_layer_pricers(cur_revenue=revenue)\n",
    "\n",
    "    records = []\n",
    "    for ded in ded_arr:\n",
    "        for ml in ml_arr:\n",
    "            if ded >= ml:\n",
    "                continue\n",
    "\n",
    "            program = make_program(ded, base_lr, inf_lr, max_limit=ml, pricers=pricers)\n",
    "            premium = program.calculate_premium()\n",
    "            has_layers = len(program.layers) > 0\n",
    "\n",
    "            W_T = np.empty(SENS_N_PATHS)\n",
    "            for i in range(SENS_N_PATHS):\n",
    "                assets = initial_assets\n",
    "                rev = assets * ATR * np.exp(\n",
    "                    REV_VOL * shocks[i, 0] - 0.5 * REV_VOL ** 2\n",
    "                )\n",
    "                op_income = rev * OPERATING_MARGIN\n",
    "                loss_scale = (rev / revenue) ** FREQ_SCALING_EXPONENT\n",
    "\n",
    "                total_retained = 0.0\n",
    "                if has_layers:\n",
    "                    prog = InsuranceProgram.create_fresh(program)\n",
    "                    for event in losses[i][0]:\n",
    "                        scaled = event.amount * loss_scale\n",
    "                        result = prog.process_claim(scaled)\n",
    "                        total_retained += result.deductible_paid + result.uncovered_loss\n",
    "                else:\n",
    "                    for event in losses[i][0]:\n",
    "                        total_retained += event.amount * loss_scale\n",
    "\n",
    "                assets += op_income - total_retained - premium\n",
    "                W_T[i] = max(assets, 0.0)\n",
    "\n",
    "            log_g = np.log(np.maximum(W_T, 1.0) / initial_assets)\n",
    "            records.append({\n",
    "                'revenue': revenue,\n",
    "                'base_loss_ratio': base_lr,\n",
    "                'loss_ratio_inflation_factor': inf_lr,\n",
    "                'initial_assets': initial_assets,\n",
    "                'ded': ded,\n",
    "                'max_limit': ml,\n",
    "                'growth_rate': np.mean(log_g),\n",
    "                'growth_vol': np.std(log_g),\n",
    "                'ruin_prob': np.mean(W_T <= 0),\n",
    "                'premium': premium,\n",
    "                'mean_wealth': np.mean(W_T),\n",
    "            })\n",
    "\n",
    "    return records\n",
    "\n",
    "\n",
    "# --- Run the experiment ---\n",
    "inputs = [(rev, base_lr, inf_lr, ded_grid, max_limit_grid, SEED + i)\n",
    "          for i, (rev, base_lr, inf_lr) in enumerate(scan_grid)]\n",
    "n_valid = sum(1 for d in ded_grid for m in max_limit_grid if d < m)\n",
    "print(f\"Tower optimization: {len(scan_grid)} combinations x \"\n",
    "      f\"~{n_valid} valid (ded, limit) combos x {SENS_N_PATHS:,} paths\")\n",
    "print(f\"Parallel on {N_CORES} cores\\n\")\n",
    "\n",
    "t0 = time.time()\n",
    "try:\n",
    "    from joblib import Parallel, delayed\n",
    "    results = Parallel(n_jobs=min(N_CORES, len(revenue_grid)), verbose=10)(\n",
    "        delayed(_optimize_tower_for_revenue)(inp) for inp in inputs\n",
    "    )\n",
    "except ImportError:\n",
    "    print(\"joblib not available, trying ProcessPoolExecutor...\")\n",
    "    try:\n",
    "        with ProcessPoolExecutor(max_workers=min(N_CORES, len(revenue_grid))) as ex:\n",
    "            results = list(tqdm(\n",
    "                ex.map(_optimize_tower_for_revenue, inputs),\n",
    "                total=len(inputs), desc=\"Tower optimization\",\n",
    "            ))\n",
    "    except Exception:\n",
    "        print(\"ProcessPoolExecutor failed, running serially...\")\n",
    "        results = [_optimize_tower_for_revenue(inp)\n",
    "                   for inp in tqdm(inputs, desc=\"Tower optimization (serial)\")]\n",
    "except Exception as e:\n",
    "    print(f\"Parallel failed ({e}), running serially...\")\n",
    "    results = [_optimize_tower_for_revenue(inp)\n",
    "               for inp in tqdm(inputs, desc=\"Tower optimization (serial)\")]\n",
    "\n",
    "all_records = []\n",
    "for r in results:\n",
    "    all_records.extend(r)\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\nDone in {elapsed:.1f}s ({len(all_records):,} evaluations)\")\n",
    "\n",
    "df_tower = pd.DataFrame(all_records)\n",
    "\n",
    "# --- Select optimal tower per (revenue, base_loss_ratio, inflection_factor) ---\n",
    "optimal_towers = []\n",
    "for rev, base_lr, inf_lr in scan_grid:\n",
    "    mask = (df_tower['revenue'] == rev) & \\\n",
    "           (np.isclose(df_tower['base_loss_ratio'], base_lr)) & \\\n",
    "           (np.isclose(df_tower['loss_ratio_inflation_factor'], inf_lr))\n",
    "    df_combo = df_tower[mask]\n",
    "    feasible = df_combo[df_combo['ruin_prob'] <= RUIN_LIMIT]\n",
    "\n",
    "    if len(feasible) > 0:\n",
    "        g = feasible['growth_rate'].values\n",
    "        v = feasible['growth_vol'].values\n",
    "        g_rng = max(g.max() - g.min(), 1e-12)\n",
    "        v_rng = max(v.max() - v.min(), 1e-12)\n",
    "        scores = (GROWTH_WEIGHT * (g - g.min()) / g_rng\n",
    "                  + VOL_WEIGHT  * (v.max() - v) / v_rng)\n",
    "        best_idx = feasible.index[np.argmax(scores)]\n",
    "    else:\n",
    "        best_idx = df_combo['ruin_prob'].idxmin()\n",
    "\n",
    "    row = df_tower.loc[best_idx]\n",
    "    optimal_towers.append({\n",
    "        'revenue': rev,\n",
    "        'base_loss_ratio': base_lr,\n",
    "        'loss_ratio_inflation_factor': inf_lr,\n",
    "        'initial_assets': row['initial_assets'],\n",
    "        'optimal_ded': row['ded'],\n",
    "        'optimal_max_limit': row['max_limit'],\n",
    "        'growth_rate': row['growth_rate'],\n",
    "        'growth_vol': row['growth_vol'],\n",
    "        'ruin_prob': row['ruin_prob'],\n",
    "        'premium': row['premium'],\n",
    "        'tower_width': row['max_limit'] - row['ded'],\n",
    "    })\n",
    "\n",
    "df_optimal = pd.DataFrame(optimal_towers)\n",
    "\n",
    "print(f\"\\nOptimal towers: {len(df_optimal)} combos \"\n",
    "      f\"({len(revenue_grid)} revenues x {len(base_loss_ratio_grid)} LRs \"\n",
    "      f\"x {len(inflection_factor_grid)} inflections)\")\n",
    "print(df_optimal[['revenue', 'base_loss_ratio', 'loss_ratio_inflation_factor',\n",
    "                  'optimal_ded', 'optimal_max_limit', 'growth_rate', 'ruin_prob']].head(20).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1771586255052,
     "user": {
      "displayName": "Alex Filiakov",
      "userId": "03712643796933532488"
     },
     "user_tz": 300
    },
    "id": "b4663a54",
    "outputId": "1d45392d-1ff6-41d9-fb34-e8e33a2753ec"
   },
   "outputs": [],
   "source": "import os\nimport pickle\n\nCACHE_DIR = 'cache'\nos.makedirs(CACHE_DIR, exist_ok=True)\n\n# Save full experiment grid (parquet + pickle)\nrecords_base = os.path.join(CACHE_DIR, 'tower_experiment')\ntry:\n    df_tower.to_parquet(records_base + '.parquet', compression='zstd')\n    sz = os.path.getsize(records_base + '.parquet') / 1024\n    print(f\"Saved: {records_base}.parquet ({len(df_tower):,} rows, {sz:.0f} KB)\")\nexcept Exception as e:\n    print(f\"Parquet save failed: {e}\")\n\ndf_tower.to_pickle(records_base + '.pkl')\nprint(f\"Saved: {records_base}.pkl ({len(df_tower):,} rows)\")\n\n# Save optimal tower summary (parquet + pickle)\noptimal_base = os.path.join(CACHE_DIR, 'optimal_towers')\ntry:\n    df_optimal.to_parquet(optimal_base + '.parquet', compression='zstd')\n    print(f\"Saved: {optimal_base}.parquet ({len(df_optimal)} rows)\")\nexcept Exception as e:\n    print(f\"Parquet save failed: {e}\")\n\ndf_optimal.to_pickle(optimal_base + '.pkl')\nprint(f\"Saved: {optimal_base}.pkl ({len(df_optimal)} rows)\")\n\nprint(f\"\\nCache directory: {os.path.abspath(CACHE_DIR)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g76F0o86DRpI"
   },
   "source": "### Demand Curve Visualization (3x3 Facet Plot)\n\nEach facet shows how the **optimal deductible** and **optimal max limit** vary with the\nbase loss ratio (x-axis):\n\n- **Columns** = starting revenue ($5M, $25M, $50M) — revenue increases left → right\n- **Rows** = excess-layer inflection factor (1×, 2×, 3×) — excess pricing increases bottom → top\n- **Orange line** = optimal deductible (retention)\n- **Blue line** = optimal max limit (top of tower)\n\nA higher inflection factor divides the upper-layer loss ratio, making excess coverage\nmore expensive.  As the base loss ratio rises (insurance becomes cheaper), both\nthe retention and max limit should respond — this is the demand curve."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 707
    },
    "executionInfo": {
     "elapsed": 871,
     "status": "ok",
     "timestamp": 1771586256199,
     "user": {
      "displayName": "Alex Filiakov",
      "userId": "03712643796933532488"
     },
     "user_tz": 300
    },
    "id": "DfSw5ES-DRpI",
    "outputId": "ec18809f-36e5-45bf-d4d4-ae2f080205be"
   },
   "outputs": [],
   "source": "from scipy.interpolate import PchipInterpolator\nimport matplotlib.ticker as mticker\n\n# Sort grids: columns = revenue ascending, rows = inflection descending\n# Top row = highest inflection (most expensive excess), bottom = lowest (cheapest)\nrev_sorted = sorted(revenue_grid)\ninf_sorted = sorted(inflection_factor_grid, reverse=True)\n\nn_rows = len(inf_sorted)\nn_cols = len(rev_sorted)\n\nfig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 4.5 * n_rows),\n                         dpi=150, sharex=True, sharey=True)\n\n# Ensure axes is always 2D\nif n_rows == 1 and n_cols == 1:\n    axes = np.array([[axes]])\nelif n_rows == 1:\n    axes = axes[np.newaxis, :]\nelif n_cols == 1:\n    axes = axes[:, np.newaxis]\n\n# Common y-axis bounds across all facets\nall_ded = df_optimal['optimal_ded'].values\nall_lim = df_optimal['optimal_max_limit'].values\npos_ded = all_ded[all_ded > 0]\ny_min = max(pos_ded.min() * 0.5, 1_000) if len(pos_ded) > 0 else 1_000\ny_max = all_lim.max() * 2.0\n\ndef dollar_fmt(x, pos):\n    if x >= 1e9:\n        return f'${x/1e9:.0f}B'\n    elif x >= 1e6:\n        return f'${x/1e6:.0f}M'\n    elif x >= 1e3:\n        return f'${x/1e3:.0f}K'\n    return f'${x:.0f}'\n\nfor row_idx, inf_f in enumerate(inf_sorted):\n    for col_idx, rev in enumerate(rev_sorted):\n        ax = axes[row_idx, col_idx]\n\n        # Filter data for this facet\n        mask = (np.isclose(df_optimal['loss_ratio_inflation_factor'], inf_f)) & \\\n               (df_optimal['revenue'] == rev)\n        df_f = df_optimal[mask].sort_values('base_loss_ratio')\n\n        if len(df_f) < 2:\n            ax.text(0.5, 0.5, 'Insufficient data', transform=ax.transAxes,\n                    ha='center', va='center', fontsize=9, color='gray')\n            continue\n\n        x = df_f['base_loss_ratio'].values\n        y_ded = np.maximum(df_f['optimal_ded'].values, 1_000)  # floor for log scale\n        y_lim = df_f['optimal_max_limit'].values\n\n        # PCHIP interpolation in log-space for smooth curves\n        x_fine = np.linspace(x.min(), x.max(), 200)\n\n        if len(x) >= 4:\n            ded_interp = PchipInterpolator(x, np.log10(y_ded))\n            lim_interp = PchipInterpolator(x, np.log10(y_lim))\n            ded_smooth = 10 ** ded_interp(x_fine)\n            lim_smooth = 10 ** lim_interp(x_fine)\n        else:\n            ded_smooth = 10 ** np.interp(x_fine, x, np.log10(y_ded))\n            lim_smooth = 10 ** np.interp(x_fine, x, np.log10(y_lim))\n\n        # Plot smooth lines\n        ax.plot(x_fine, lim_smooth, color='steelblue', linewidth=2, label='Max Limit')\n        ax.plot(x_fine, ded_smooth, color='darkorange', linewidth=2, label='Deductible')\n\n        # Plot actual data points\n        ax.scatter(x, y_lim, color='steelblue', s=20, zorder=5, alpha=0.7)\n        ax.scatter(x, y_ded, color='darkorange', s=20, zorder=5, alpha=0.7)\n\n        ax.set_yscale('log')\n        ax.set_ylim(y_min, y_max)\n        ax.grid(True, alpha=0.3, which='both')\n\n        # Column titles (top row only)\n        if row_idx == 0:\n            ax.set_title(f'Revenue = ${rev/1e6:.0f}M', fontsize=12, fontweight='bold')\n\n        # Row labels (left column only)\n        if col_idx == 0:\n            ax.set_ylabel(f'Inflection = {inf_f:.0f}x\\nDemanded Amount ($)', fontsize=10)\n\n        # X-axis label (bottom row only)\n        if row_idx == n_rows - 1:\n            ax.set_xlabel('Base Loss Ratio', fontsize=10)\n\n        ax.yaxis.set_major_formatter(mticker.FuncFormatter(dollar_fmt))\n\n# Shared legend from first facet\nhandles, labels = axes[0, 0].get_legend_handles_labels()\nif handles:\n    fig.legend(handles, labels, loc='upper center', ncol=2, fontsize=12,\n               bbox_to_anchor=(0.5, 0.99), frameon=True, edgecolor='black')\n\nfig.suptitle('Insurance Demand Curves: Optimal Retention & Limit vs. Base Loss Ratio\\n'\n             f'75/25 growth/volatility blend, \\u22640.5% ruin constraint, {SENS_N_PATHS} paths',\n             fontsize=14, fontweight='bold', y=1.03)\n\nfig.patch.set_facecolor('white')\nfor row in axes:\n    for ax in row:\n        ax.set_facecolor('white')\n        for spine in ax.spines.values():\n            spine.set_color('black')\n        ax.tick_params(colors='black')\n\nplt.tight_layout(rect=[0, 0, 1, 0.96])\nplt.savefig(os.path.join(CACHE_DIR, 'demand_curve_3x3.png'),\n            dpi=150, bbox_inches='tight', facecolor='white')\nplt.show()\n\nprint(f\"\\nChart saved to: {os.path.join(CACHE_DIR, 'demand_curve_3x3.png')}\")"
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V6E1",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
