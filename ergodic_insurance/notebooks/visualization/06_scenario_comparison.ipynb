{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scenario Comparison\n\n## Overview\n- **What**: Side-by-side scenario comparison framework with automatic insight extraction, smart annotation placement, A/B testing visualizations, and natural-language executive summaries.\n- **Prerequisites**: [01_visualization_factory](01_visualization_factory.ipynb), [../core/04_monte_carlo_engine](../core/04_monte_carlo_engine.ipynb)\n- **Estimated runtime**: < 1 minute\n- **Audience**: [Practitioner] / [Developer]\n\n## Topics Covered\n1. Creating scenarios via `ScenarioManager`\n2. Comparing scenarios with `ScenarioComparator`\n3. Automatic insight extraction\n4. Smart annotation placement\n5. A/B testing visualization\n6. Exporting comparison reports"
   ],
   "id": "cell-0"
  },
  {
   "cell_type": "code",
   "id": "6os2i9fe6w2",
   "source": "\"\"\"Google Colab setup: mount Drive and install package dependencies.\n\nRun this cell first. If prompted to restart the runtime, do so, then re-run all cells.\nThis cell is a no-op when running locally.\n\"\"\"\nimport sys, os\nif 'google.colab' in sys.modules:\n    from google.colab import drive\n    drive.mount('/content/drive')\n\n    NOTEBOOK_DIR = '/content/drive/My Drive/Colab Notebooks/ei_notebooks/visualization'\n\n    os.chdir(NOTEBOOK_DIR)\n    if NOTEBOOK_DIR not in sys.path:\n        sys.path.append(NOTEBOOK_DIR)\n\n    !pip install ergodic-insurance -q 2>&1 | tail -3\n    print('\\nSetup complete. If you see numpy/scipy import errors below,')\n    print('restart the runtime (Runtime > Restart runtime) and re-run all cells.')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Setup\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nfrom ergodic_insurance.config_manager import ConfigManager\nfrom ergodic_insurance.scenario_manager import ScenarioManager, ParameterSpec\nfrom ergodic_insurance.reporting.scenario_comparator import ScenarioComparator\nfrom ergodic_insurance.reporting.insight_extractor import InsightExtractor\nfrom ergodic_insurance.visualization.annotations import (\n    SmartAnnotationPlacer,\n    auto_annotate_peaks_valleys,\n    create_leader_line,\n)\nfrom ergodic_insurance.visualization.core import set_wsj_style\n\nset_wsj_style()\nnp.random.seed(42)\n\nprint(\"Scenario comparison framework loaded.\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cell-1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Scenarios\n\nDefine parameter specifications and generate a sensitivity analysis grid from a base configuration."
   ],
   "id": "cell-2"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "scenario_mgr = ScenarioManager()\nconfig_mgr = ConfigManager()\nbase_config = config_mgr.load_profile(\"default\")\n\nparam_specs = [\n    ParameterSpec(name=\"insurance.base_premium_rate\", base_value=0.015, variation_pct=0.3),\n    ParameterSpec(name=\"insurance.limit\", base_value=5_000_000, variation_pct=0.5),\n    ParameterSpec(name=\"manufacturer.target_margin\", base_value=0.08, variation_pct=0.25),\n]\n\nscenarios = scenario_mgr.create_sensitivity_analysis(\n    base_name=\"insurance_optimization\",\n    parameter_specs=param_specs,\n    base_config=base_config,\n    tags={\"demo\", \"comparison\"},\n)\n\nprint(f\"Created {len(scenarios)} scenarios:\")\nfor s in scenarios:\n    print(f\"  - {s.name}: {s.description}\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cell-3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Results\n\nIn production these would come from actual simulations. Here we create synthetic results that respond realistically to parameter changes."
   ],
   "id": "cell-4"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def create_synthetic_results(scenario_name, param_overrides):\n    np.random.seed(hash(scenario_name) % 2**32)\n    base_growth, base_ruin, base_assets = 0.06, 0.015, 10_000_000\n\n    if \"insurance.base_premium_rate\" in param_overrides:\n        eff = (param_overrides[\"insurance.base_premium_rate\"] - 0.015) * 100\n        base_growth -= eff * 0.5\n        base_ruin += eff * 0.3\n    if \"insurance.limit\" in param_overrides:\n        eff = (param_overrides[\"insurance.limit\"] - 5_000_000) / 5_000_000\n        base_ruin *= 1 - eff * 0.4\n        base_growth += eff * 0.01\n    if \"manufacturer.target_margin\" in param_overrides:\n        eff = (param_overrides[\"manufacturer.target_margin\"] - 0.08) / 0.08\n        base_growth += eff * 0.02\n        base_assets *= 1 + eff * 0.15\n\n    growth = base_growth + np.random.normal(0, 0.005)\n    ruin = max(0, base_ruin + np.random.normal(0, 0.002))\n    assets = base_assets * (1 + np.random.normal(0, 0.1))\n\n    return {\n        \"summary_statistics\": {\n            \"mean_growth_rate\": growth,\n            \"ruin_probability\": ruin,\n            \"mean_final_assets\": assets,\n            \"var_95\": -np.random.uniform(50_000, 200_000),\n            \"var_99\": -np.random.uniform(100_000, 500_000),\n            \"sharpe_ratio\": growth / 0.15,\n            \"max_drawdown\": np.random.uniform(0.05, 0.25),\n        },\n        \"config\": param_overrides,\n    }\n\n\nresults = {\n    s.name: create_synthetic_results(s.name, s.parameter_overrides)\n    for s in scenarios\n}\nprint(f\"Generated results for {len(results)} scenarios\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cell-5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compare Scenarios\n\nThe `ScenarioComparator` ranks scenarios against a baseline and identifies top performers."
   ],
   "id": "cell-6"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "comparator = ScenarioComparator()\n\ncomparison = comparator.compare_scenarios(\n    results,\n    baseline=\"insurance_optimization_baseline\",\n    metrics=[\"mean_growth_rate\", \"ruin_probability\", \"mean_final_assets\", \"sharpe_ratio\"],\n)\n\nprint(f\"Scenarios compared: {len(comparison.scenarios)}\")\n\nprint(\"\\nTop performers:\")\nfor metric in [\"mean_growth_rate\", \"ruin_probability\"]:\n    ascending = \"ruin\" in metric\n    top = comparison.get_top_performers(metric, n=3, ascending=ascending)\n    print(f\"  {metric.replace('_', ' ').title()}:\")\n    for name, val in top:\n        print(f\"    {name}: {val:.4f}\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cell-7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparison Grid\n\nBar charts for each metric with difference indicators from baseline."
   ],
   "id": "cell-8"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig = comparator.create_comparison_grid(\n    metrics=[\"mean_growth_rate\", \"ruin_probability\", \"mean_final_assets\", \"sharpe_ratio\"],\n    figsize=(16, 10),\n    show_diff=True,\n)\nplt.tight_layout()\nplt.show()"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cell-9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Insight Extraction\n\nThe `InsightExtractor` automatically identifies key findings and generates an executive summary."
   ],
   "id": "cell-10"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "extractor = InsightExtractor()\n\ninsights = extractor.extract_insights(\n    comparison,\n    focus_metrics=[\"mean_growth_rate\", \"ruin_probability\"],\n    threshold_importance=50,\n)\n\nprint(f\"Extracted {len(insights)} insights:\\n\")\nfor i, ins in enumerate(insights[:5], 1):\n    print(f\"{i}. [{ins.category.upper()}] {ins.title}\")\n    print(f\"   {ins.description}\")\n    print(f\"   Importance: {ins.importance:.0f}/100 | Confidence: {ins.confidence:.1%}\\n\")\n\nexecutive_summary = extractor.generate_executive_summary(max_points=5, focus_positive=True)\nprint(\"\\nExecutive Summary:\")\nprint(executive_summary)"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cell-11"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Smart Annotations\n\nThe `SmartAnnotationPlacer` prevents overlapping labels on complex plots. Peak/valley auto-detection annotates extrema."
   ],
   "id": "cell-12"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 9))\n\nnp.random.seed(42)\nx = np.arange(100)\ny1 = np.cumsum(np.random.randn(100) * 0.5) + 50\ny2 = np.cumsum(np.random.randn(100) * 0.3) + 45\ny3 = np.cumsum(np.random.randn(100) * 0.4) + 40\n\nax.plot(x, y1, label=\"Optimized Strategy\", linewidth=2, color=\"#1f77b4\")\nax.plot(x, y2, label=\"Baseline Strategy\", linewidth=2, color=\"#ff7f0e\")\nax.plot(x, y3, label=\"Conservative Strategy\", linewidth=2, color=\"#2ca02c\")\n\nplacer = SmartAnnotationPlacer(ax)\nplacer = auto_annotate_peaks_valleys(\n    ax, x, y1, n_peaks=2, n_valleys=1,\n    peak_color=\"#006400\", valley_color=\"#8B0000\", fontsize=8, placer=placer,\n)\n\nannotations = [\n    {\"text\": \"Optimized outperforms\", \"point\": (60, y1[60]), \"priority\": 85, \"color\": \"#4169E1\"},\n    {\"text\": \"Baseline steady\", \"point\": (35, y2[35]), \"priority\": 75, \"color\": \"#FF8C00\"},\n    {\"text\": \"Conservative lags\", \"point\": (75, y3[75]), \"priority\": 65, \"color\": \"#556B2F\"},\n]\nplacer.add_smart_annotations(annotations, fontsize=9)\n\nax.set_xlabel(\"Time Period\", fontsize=11)\nax.set_ylabel(\"Performance Metric\", fontsize=11)\nax.set_title(\"Smart Annotation Demonstration\", fontsize=13, fontweight=\"bold\", pad=20)\nax.legend(loc=\"lower right\", framealpha=0.95, edgecolor=\"gray\")\nax.grid(True, alpha=0.2, linestyle=\"--\", linewidth=0.5)\nax.set_xlim(-5, 105)\ny_min = min(y1.min(), y2.min(), y3.min())\ny_max = max(y1.max(), y2.max(), y3.max())\npad = (y_max - y_min) * 0.15\nax.set_ylim(y_min - pad, y_max + pad)\n\nplt.tight_layout()\nplt.show()"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cell-13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. A/B Testing Visualization\n\nDirect comparison of two scenarios across all key metrics with improvement indicators."
   ],
   "id": "cell-14"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "scenario_a = \"insurance_optimization_baseline\"\nscenario_b = list(comparison.scenarios)[1] if len(comparison.scenarios) > 1 else scenario_a\nmetrics_to_compare = [\"mean_growth_rate\", \"ruin_probability\", \"mean_final_assets\", \"sharpe_ratio\"]\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\nfor idx, metric in enumerate(metrics_to_compare):\n    ax = axes.flat[idx]\n    val_a = comparison.metrics[metric].get(scenario_a, 0)\n    val_b = comparison.metrics[metric].get(scenario_b, 0)\n\n    bars = ax.bar([\"Baseline\", \"Alternative\"], [val_a, val_b],\n                   color=[\"#1f77b4\", \"#ff7f0e\"], alpha=0.8)\n    for bar, val in zip(bars, [val_a, val_b]):\n        if \"probability\" in metric or \"rate\" in metric:\n            label = f\"{val:.2%}\"\n        elif \"assets\" in metric:\n            label = f\"${val / 1e6:.1f}M\"\n        else:\n            label = f\"{val:.3f}\"\n        ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height(),\n               label, ha=\"center\", va=\"bottom\", fontweight=\"bold\")\n\n    diff = ((val_b - val_a) / val_a * 100) if val_a != 0 else 0\n    worse_is_lower = \"ruin\" in metric or \"drawdown\" in metric\n    improvement = diff < 0 if worse_is_lower else diff > 0\n    color = \"green\" if improvement else \"red\"\n    status = \"Better\" if improvement else \"Worse\"\n    ax.text(0.5, 0.95, f\"{diff:+.1f}% {status}\",\n           transform=ax.transAxes, ha=\"center\", va=\"top\",\n           fontsize=12, fontweight=\"bold\", color=color,\n           bbox=dict(boxstyle=\"round\", facecolor=\"white\", edgecolor=color, alpha=0.8))\n\n    ax.set_title(metric.replace(\"_\", \" \").title())\n    ax.set_ylabel(\"Value\")\n    ax.grid(True, alpha=0.3, axis=\"y\")\n\nfig.suptitle(f\"A/B Test: {scenario_a} vs {scenario_b}\",\n             fontsize=16, fontweight=\"bold\")\nplt.tight_layout()\nplt.show()"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cell-15"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Comparison Report\n\nSave the full comparison (data, plots, and insights) to disk."
   ],
   "id": "cell-16"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n\noutput_dir = \"comparison_reports\"\nos.makedirs(output_dir, exist_ok=True)\n\noutput_files = comparator.export_comparison_report(\n    os.path.join(output_dir, \"scenario_comparison\"),\n    include_plots=True,\n)\n\ninsights_path = os.path.join(output_dir, \"insights.md\")\nextractor.export_insights(insights_path, output_format=\"markdown\")\n\nprint(\"Exported files:\")\nfor key, path in output_files.items():\n    print(f\"  {key}: {path}\")\nprint(f\"  insights: {insights_path}\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cell-17"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n\n- `ScenarioComparator` provides automated ranking, grid plots, and difference tables.\n- `InsightExtractor` generates prioritized findings and executive summaries from raw results.\n- `SmartAnnotationPlacer` prevents label overlaps on complex multi-series plots.\n- A/B testing visualizations make direct scenario comparisons immediately understandable.\n- Export all results (CSV, plots, Markdown) for stakeholder distribution.\n\n## Next Steps\n\n- [../reporting/01_report_generation](../reporting/01_report_generation.ipynb) -- full report generation pipeline\n- [02_executive_dashboards](02_executive_dashboards.ipynb) -- executive visualization suite\n- [../optimization/01_basic_optimization](../optimization/01_basic_optimization.ipynb) -- optimization workflows"
   ],
   "id": "cell-18"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
