{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Risk Metrics Suite\n",
    "\n",
    "## Overview\n",
    "- **What this notebook does:** Demonstrates the comprehensive risk metrics suite for quantifying tail risk -- VaR, TVaR, PML, Expected Shortfall, Economic Capital, bootstrap confidence intervals, coherence testing, scenario comparison, and insurance limit selection.\n",
    "- **Prerequisites:** [core/01_loss_distributions.ipynb](01_loss_distributions.ipynb)\n",
    "- **Estimated runtime:** 1--2 minutes\n",
    "- **Audience:** [Practitioner]\n",
    "\n",
    "## Why Risk Metrics?\n",
    "Insurance decisions hinge on understanding tail risk: the probability and severity of extreme losses. This notebook introduces a toolkit of risk metrics and shows how to use them for setting insurance limits, comparing operational scenarios, and quantifying the uncertainty around risk estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from ergodic_insurance.risk_metrics import RiskMetrics, compare_risk_metrics\n",
    "from ergodic_insurance.loss_distributions import ManufacturingLossGenerator\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SIMULATIONS = 10_000\n",
    "ANNUAL_REVENUE = 50_000_000\n",
    "\n",
    "ATTRITIONAL_PARAMS = {\n",
    "    'base_frequency': 5.0,\n",
    "    'severity_mean': 50_000,\n",
    "    'severity_cv': 0.8,\n",
    "    'revenue_scaling_exponent': 0.3,\n",
    "    'reference_revenue': 50_000_000,\n",
    "}\n",
    "LARGE_PARAMS = {\n",
    "    'base_frequency': 0.5,\n",
    "    'severity_mean': 2_000_000,\n",
    "    'severity_cv': 1.2,\n",
    "    'revenue_scaling_exponent': 0.2,\n",
    "    'reference_revenue': 50_000_000,\n",
    "}\n",
    "CATASTROPHIC_PARAMS = {\n",
    "    'base_frequency': 0.02,\n",
    "    'severity_xm': 10_000_000,\n",
    "    'severity_alpha': 2.5,\n",
    "}\n",
    "\n",
    "CONFIDENCE_LEVELS = [0.90, 0.95, 0.99, 0.995, 0.999]\n",
    "RETURN_PERIODS = [10, 25, 50, 100, 200, 250, 500, 1000]\n",
    "\n",
    "print(\"Risk metrics notebook configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Manufacturing Loss Scenarios\n",
    "\n",
    "Simulate annual aggregate losses for a manufacturer with $50M revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = ManufacturingLossGenerator(\n",
    "    attritional_params=ATTRITIONAL_PARAMS,\n",
    "    large_params=LARGE_PARAMS,\n",
    "    catastrophic_params=CATASTROPHIC_PARAMS,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "annual_losses = []\n",
    "for _ in range(N_SIMULATIONS):\n",
    "    events, year_stats = generator.generate_losses(duration=1.0, revenue=ANNUAL_REVENUE)\n",
    "    annual_losses.append(sum(e.amount for e in events))\n",
    "\n",
    "annual_losses = np.array(annual_losses)\n",
    "\n",
    "print(f\"Generated {N_SIMULATIONS:,} annual loss scenarios\")\n",
    "print(f\"Mean annual loss: ${np.mean(annual_losses):,.0f}\")\n",
    "print(f\"Median annual loss: ${np.median(annual_losses):,.0f}\")\n",
    "print(f\"Max annual loss: ${np.max(annual_losses):,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Core Risk Metrics: VaR, TVaR, and PML\n",
    "\n",
    "- **VaR (Value at Risk):** The loss level exceeded with probability (1 - confidence)\n",
    "- **TVaR (Tail VaR):** The expected loss *given* the loss exceeds VaR (captures tail severity)\n",
    "- **PML (Probable Maximum Loss):** The loss expected to occur once every N years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = RiskMetrics(annual_losses, seed=42)\n",
    "\n",
    "print(\"Value at Risk (VaR) and Tail VaR (TVaR)\")\n",
    "print(\"=\" * 52)\n",
    "print(f\"{'Confidence':<12} {'VaR':>15} {'TVaR':>15} {'TVaR/VaR':>10}\")\n",
    "print(\"-\" * 52)\n",
    "\n",
    "var_results, tvar_results = {}, {}\n",
    "for conf in CONFIDENCE_LEVELS:\n",
    "    var_val = metrics.var(conf)\n",
    "    tvar_val = metrics.tvar(conf)\n",
    "    var_results[conf] = var_val\n",
    "    tvar_results[conf] = tvar_val\n",
    "    print(f\"{conf:>10.1%}  ${var_val:>14,.0f}  ${tvar_val:>14,.0f}  {tvar_val / var_val:>9.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nProbable Maximum Loss (PML)\")\n",
    "print(\"=\" * 42)\n",
    "print(f\"{'Return Period':<15} {'PML':>15} {'Annual Prob':>12}\")\n",
    "print(\"-\" * 42)\n",
    "\n",
    "for period in RETURN_PERIODS:\n",
    "    pml_val = metrics.pml(period)\n",
    "    print(f\"{period:>10}-year  ${pml_val:>14,.0f}  {1 / period:>11.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Additional Metrics\n",
    "\n",
    "Economic Capital, Maximum Drawdown, and Tail Index provide further insight into the loss distribution shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec_999 = metrics.economic_capital(0.999)\n",
    "max_dd = metrics.maximum_drawdown()\n",
    "tail_idx = metrics.tail_index()\n",
    "\n",
    "print(\"Additional Risk Metrics\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Economic Capital (99.9%): ${ec_999:,.0f}\")\n",
    "print(f\"Maximum Drawdown: ${max_dd:,.0f}\")\n",
    "print(f\"Tail Index (Hill estimator): {tail_idx:.2f}\")\n",
    "print(f\"  Interpretation: {'Heavy' if tail_idx < 3 else 'Moderate'} tail\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Loss Distribution and Risk Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = metrics.plot_distribution(\n",
    "    bins=50, show_metrics=True,\n",
    "    confidence_levels=[0.95, 0.99, 0.995],\n",
    ")\n",
    "plt.suptitle('Manufacturing Loss Distribution and Risk Metrics', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Bootstrap Confidence Intervals\n",
    "\n",
    "Risk metric point estimates carry sampling uncertainty. Bootstrap confidence intervals quantify how much the estimates might shift with a different sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"VaR with 95% Bootstrap Confidence Intervals\")\n",
    "print(\"=\" * 57)\n",
    "print(f\"{'Confidence':<12} {'VaR':>15} {'CI Lower':>15} {'CI Upper':>15}\")\n",
    "print(\"-\" * 57)\n",
    "\n",
    "for conf in [0.95, 0.99, 0.995]:\n",
    "    result = metrics.var(conf, bootstrap_ci=True, n_bootstrap=1_000)\n",
    "    ci_lower, ci_upper = result.confidence_interval\n",
    "    print(f\"{conf:>10.1%}  ${result.value:>14,.0f}  ${ci_lower:>14,.0f}  ${ci_upper:>14,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Coherence Testing\n",
    "\n",
    "A *coherent* risk measure satisfies monotonicity, sub-additivity, positive homogeneity, and translation invariance. TVaR is coherent; VaR is not. This verification helps ensure our implementation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_results = metrics.coherence_test()\n",
    "\n",
    "print(\"Coherence Properties of TVaR\")\n",
    "print(\"=\" * 40)\n",
    "for prop, satisfied in coherence_results.items():\n",
    "    status = \"Satisfied\" if satisfied else \"Not satisfied\"\n",
    "    print(f\"  {prop.replace('_', ' ').title()}: {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Scenario Comparison\n",
    "\n",
    "Compare risk metrics across a base case, a high-frequency scenario, and a high-severity scenario to understand how the loss profile changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = {'Base Case': annual_losses}\n",
    "\n",
    "# High frequency scenario\n",
    "gen_high_freq = ManufacturingLossGenerator(\n",
    "    attritional_params={**ATTRITIONAL_PARAMS, 'base_frequency': 8.0},\n",
    "    large_params={**LARGE_PARAMS, 'base_frequency': 0.8},\n",
    "    seed=43,\n",
    ")\n",
    "high_freq_losses = []\n",
    "for _ in range(5_000):\n",
    "    events, _ = gen_high_freq.generate_losses(duration=1.0, revenue=ANNUAL_REVENUE)\n",
    "    high_freq_losses.append(sum(e.amount for e in events))\n",
    "scenarios['High Frequency'] = np.array(high_freq_losses)\n",
    "\n",
    "# High severity scenario\n",
    "gen_high_sev = ManufacturingLossGenerator(\n",
    "    attritional_params={**ATTRITIONAL_PARAMS, 'severity_mean': 75_000},\n",
    "    large_params={**LARGE_PARAMS, 'severity_mean': 4_000_000, 'severity_cv': 1.5},\n",
    "    seed=44,\n",
    ")\n",
    "high_sev_losses = []\n",
    "for _ in range(5_000):\n",
    "    events, _ = gen_high_sev.generate_losses(duration=1.0, revenue=ANNUAL_REVENUE)\n",
    "    high_sev_losses.append(sum(e.amount for e in events))\n",
    "scenarios['High Severity'] = np.array(high_sev_losses)\n",
    "\n",
    "comparison_df = compare_risk_metrics(scenarios, confidence_levels=[0.95, 0.99, 0.995])\n",
    "print(\"Risk Metrics Comparison Across Scenarios\")\n",
    "print(\"=\" * 60)\n",
    "print(comparison_df.round(0).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Return Period Curves and Insurance Limit Selection\n",
    "\n",
    "Return period curves show the expected loss at each return period, directly informing insurance limit selection. Choosing a limit aligned with the 100-year PML covers all but the most extreme events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "return_periods_plot = np.array([2, 5, 10, 25, 50, 100, 200, 500, 1000])\n",
    "for name, losses in scenarios.items():\n",
    "    rm = RiskMetrics(losses)\n",
    "    periods, vals = rm.return_period_curve(return_periods_plot)\n",
    "    ax1.semilogx(periods, vals / 1e6, 'o-', label=name, linewidth=2, markersize=6)\n",
    "\n",
    "ax1.set_xlabel('Return Period (years)')\n",
    "ax1.set_ylabel('Loss Amount ($M)')\n",
    "ax1.set_title('Return Period Curves')\n",
    "ax1.grid(True, alpha=0.3, which='both')\n",
    "ax1.legend()\n",
    "\n",
    "for name, losses in scenarios.items():\n",
    "    sorted_losses = np.sort(losses)[::-1]\n",
    "    exceedance_prob = np.arange(1, len(sorted_losses) + 1) / len(sorted_losses)\n",
    "    ax2.semilogy(sorted_losses / 1e6, exceedance_prob, '-', label=name, linewidth=2)\n",
    "\n",
    "ax2.set_xlabel('Loss Amount ($M)')\n",
    "ax2.set_ylabel('Annual Exceedance Probability')\n",
    "ax2.set_title('Exceedance Probability Curves')\n",
    "ax2.grid(True, alpha=0.3, which='both')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_options = {\n",
    "    'Conservative (VaR 95%)': metrics.var(0.95),\n",
    "    'Standard (VaR 99%)': metrics.var(0.99),\n",
    "    'PML-100': metrics.pml(100),\n",
    "    'PML-250': metrics.pml(250),\n",
    "    'Comprehensive (TVaR 99%)': metrics.tvar(0.99),\n",
    "}\n",
    "\n",
    "print(\"Insurance Limit Selection Analysis\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Limit Option':<25} {'Limit':>15} {'Coverage %':>12} {'Avg Uncovered':>15}\")\n",
    "print(\"-\" * 67)\n",
    "\n",
    "for option, limit in limit_options.items():\n",
    "    covered = np.mean(annual_losses <= limit) * 100\n",
    "    uncovered = annual_losses[annual_losses > limit] - limit\n",
    "    avg_uncovered = np.mean(uncovered) if len(uncovered) > 0 else 0\n",
    "    print(f\"{option:<25} ${limit:>14,.0f} {covered:>11.1f}% ${avg_uncovered:>14,.0f}\")\n",
    "\n",
    "print(\"\\nRecommendation:\")\n",
    "print(f\"  Cost-conscious: VaR(95%) = ${limit_options['Conservative (VaR 95%)']:,.0f}\")\n",
    "print(f\"  Balanced: PML-100 = ${limit_options['PML-100']:,.0f}\")\n",
    "print(f\"  Comprehensive: TVaR(99%) = ${limit_options['Comprehensive (TVaR 99%)']:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- **VaR** is intuitive but ignores tail severity; **TVaR** captures what happens beyond the threshold.\n",
    "- **PML** return-period analysis directly maps to insurance limit decisions.\n",
    "- Bootstrap confidence intervals quantify sampling uncertainty around point estimates.\n",
    "- Coherence testing validates that TVaR behaves as a proper risk measure.\n",
    "- High-severity scenarios shift the tail more than high-frequency scenarios, driving higher insurance limits.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **Study long-term dynamics:** [core/06_long_term_dynamics.ipynb](06_long_term_dynamics.ipynb)\n",
    "- **Explore growth dynamics:** [core/07_growth_dynamics.ipynb](07_growth_dynamics.ipynb)\n",
    "- **Optimize insurance programs:** [optimization/01_retention_optimization.ipynb](../optimization/01_retention_optimization.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
