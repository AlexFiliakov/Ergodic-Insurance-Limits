{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Distribution Analysis\n",
    "\n",
    "## Overview\n",
    "- **What this notebook does:** Explores the three types of manufacturing losses (attritional, large, catastrophic), their distributions, temporal patterns, extreme-value behavior, and correlations.\n",
    "- **Prerequisites:** [getting-started/03_basic_manufacturer.ipynb](../getting-started/03_basic_manufacturer.ipynb)\n",
    "- **Estimated runtime:** 1--2 minutes\n",
    "- **Audience:** [Practitioner]\n",
    "\n",
    "## Why Loss Distributions Matter\n",
    "Insurance optimization depends on accurately modeling the *frequency* and *severity* of losses. This notebook shows how the `ManufacturingLossGenerator` combines three loss types into a realistic aggregate loss distribution, and how to use that distribution to select insurance retention levels."
   ]
  },
  {
   "cell_type": "code",
   "source": "\"\"\"Google Colab setup: mount Drive and install package dependencies.\n\nRun this cell first. If prompted to restart the runtime, do so, then re-run all cells.\nThis cell is a no-op when running locally.\n\"\"\"\nimport sys, os\nif 'google.colab' in sys.modules:\n    from google.colab import drive\n    drive.mount('/content/drive')\n\n    NOTEBOOK_DIR = '/content/drive/My Drive/Colab Notebooks/ei_notebooks/core'\n\n    os.chdir(NOTEBOOK_DIR)\n    if NOTEBOOK_DIR not in sys.path:\n        sys.path.append(NOTEBOOK_DIR)\n\n    !pip install git+https://github.com/AlexFiliakov/Ergodic-Insurance-Limits.git -q 2>&1 | tail -3\n    print('\\nSetup complete. If you see numpy/scipy import errors below,')\n    print('restart the runtime (Runtime > Restart runtime) and re-run all cells.')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "\n",
    "from ergodic_insurance.loss_distributions import ManufacturingLossGenerator\n",
    "from ergodic_insurance.visualization import WSJ_COLORS, format_currency\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Loss parameters for a typical manufacturer with $10M revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REVENUE = 10_000_000\n",
    "N_SIMULATIONS = 10_000\n",
    "\n",
    "ATTRITIONAL_PARAMS = {\n",
    "    'base_frequency': 5.0,\n",
    "    'severity_mean': 50_000,\n",
    "    'severity_cv': 0.8,\n",
    "}\n",
    "LARGE_PARAMS = {\n",
    "    'base_frequency': 0.5,\n",
    "    'severity_mean': 2_000_000,\n",
    "    'severity_cv': 1.2,\n",
    "}\n",
    "CATASTROPHIC_PARAMS = {\n",
    "    'base_frequency': 0.02,\n",
    "    'severity_xm': 10_000_000,\n",
    "    'severity_alpha': 2.5,\n",
    "}\n",
    "\n",
    "generator = ManufacturingLossGenerator(\n",
    "    attritional_params=ATTRITIONAL_PARAMS,\n",
    "    large_params=LARGE_PARAMS,\n",
    "    catastrophic_params=CATASTROPHIC_PARAMS,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(\"Loss generator configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loss Distribution by Type\n",
    "\n",
    "Generate many one-year loss scenarios and break them down by loss type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_losses = []\n",
    "loss_types = []\n",
    "\n",
    "for _ in range(N_SIMULATIONS):\n",
    "    events, _ = generator.generate_losses(duration=1.0, revenue=REVENUE)\n",
    "    for event in events:\n",
    "        all_losses.append(event.amount)\n",
    "        loss_types.append(event.loss_type)\n",
    "\n",
    "df = pd.DataFrame({'amount': all_losses, 'type': loss_types})\n",
    "\n",
    "# Summary statistics by type\n",
    "stats = df.groupby('type')['amount'].agg(['count', 'mean', 'std', 'min', 'max']).round(0)\n",
    "print(f\"Generated {len(all_losses):,} individual loss events across {N_SIMULATIONS:,} simulation years.\")\n",
    "print(f\"\\nSummary by loss type:\")\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=(\n",
    "        'Loss Distribution by Type', 'Empirical CDF',\n",
    "        'Loss Frequency by Type', 'Summary Statistics',\n",
    "    ),\n",
    "    specs=[\n",
    "        [{'type': 'histogram'}, {'type': 'scatter'}],\n",
    "        [{'type': 'bar'}, {'type': 'table'}],\n",
    "    ],\n",
    ")\n",
    "\n",
    "for loss_type in df['type'].unique():\n",
    "    type_data = df[df['type'] == loss_type]['amount']\n",
    "    fig.add_trace(go.Histogram(x=type_data, name=loss_type, opacity=0.7, nbinsx=30), row=1, col=1)\n",
    "\n",
    "sorted_losses = np.sort(all_losses)\n",
    "cdf = np.arange(1, len(sorted_losses) + 1) / len(sorted_losses)\n",
    "fig.add_trace(go.Scatter(x=sorted_losses, y=cdf, mode='lines', name='ECDF',\n",
    "                         line=dict(color=WSJ_COLORS['blue'])), row=1, col=2)\n",
    "\n",
    "freq_data = df.groupby('type').size().reset_index(name='count')\n",
    "fig.add_trace(go.Bar(x=freq_data['type'], y=freq_data['count'],\n",
    "                     marker_color=[WSJ_COLORS['blue'], WSJ_COLORS['orange'], WSJ_COLORS['red']]),\n",
    "              row=2, col=1)\n",
    "\n",
    "fig.add_trace(go.Table(\n",
    "    header=dict(values=['Type', 'Count', 'Mean', 'Std', 'Min', 'Max'], align='left'),\n",
    "    cells=dict(values=[stats.index, stats['count'],\n",
    "                       [f'${x:,.0f}' for x in stats['mean']],\n",
    "                       [f'${x:,.0f}' for x in stats['std']],\n",
    "                       [f'${x:,.0f}' for x in stats['min']],\n",
    "                       [f'${x:,.0f}' for x in stats['max']]], align='left'),\n",
    "), row=2, col=2)\n",
    "\n",
    "fig.update_layout(height=800, title_text=f'Loss Distribution Analysis ({N_SIMULATIONS:,} simulations)')\n",
    "fig.update_xaxes(title_text='Loss Amount', row=1, col=1, tickformat='$.2s')\n",
    "fig.update_xaxes(title_text='Loss Amount', row=1, col=2, tickformat='$.2s', type='log')\n",
    "fig.show()\n",
    "\n",
    "print(f\"\\nAverage annual loss: ${np.mean(all_losses):,.0f}\")\n",
    "print(f\"95th percentile: ${np.percentile(all_losses, 95):,.0f}\")\n",
    "print(f\"99th percentile: ${np.percentile(all_losses, 99):,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Temporal Loss Patterns\n",
    "\n",
    "How do annual losses evolve over time? This helps understand clustering and volatility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_YEARS_TEMPORAL = 20\n",
    "\n",
    "yearly_data = []\n",
    "for year in range(N_YEARS_TEMPORAL):\n",
    "    events, year_stats = generator.generate_losses(duration=1.0, revenue=REVENUE)\n",
    "    yearly_data.append({\n",
    "        'year': year + 1,\n",
    "        'total_loss': year_stats['total_amount'],\n",
    "        'num_events': len(events),\n",
    "        'attritional': sum(e.amount for e in events if e.loss_type == 'attritional'),\n",
    "        'large': sum(e.amount for e in events if e.loss_type == 'large'),\n",
    "        'catastrophic': sum(e.amount for e in events if e.loss_type == 'catastrophic'),\n",
    "    })\n",
    "\n",
    "temporal_df = pd.DataFrame(yearly_data)\n",
    "\n",
    "fig = make_subplots(rows=3, cols=1, subplot_titles=(\n",
    "    'Annual Total Losses', 'Loss Composition by Type', 'Cumulative Losses',\n",
    "), row_heights=[0.35, 0.35, 0.3])\n",
    "\n",
    "mean_loss = temporal_df['total_loss'].mean()\n",
    "fig.add_trace(go.Bar(x=temporal_df['year'], y=temporal_df['total_loss'],\n",
    "                     name='Total Loss', marker_color=WSJ_COLORS['blue']), row=1, col=1)\n",
    "fig.add_hline(y=mean_loss, line_dash='dash', line_color=WSJ_COLORS['red'],\n",
    "              annotation_text=f'Mean: ${mean_loss:,.0f}', row=1, col=1)\n",
    "\n",
    "for col_name, color in [('attritional', WSJ_COLORS['light_blue']),\n",
    "                         ('large', WSJ_COLORS['orange']),\n",
    "                         ('catastrophic', WSJ_COLORS['red'])]:\n",
    "    fig.add_trace(go.Bar(x=temporal_df['year'], y=temporal_df[col_name],\n",
    "                         name=col_name.title(), marker_color=color), row=2, col=1)\n",
    "\n",
    "temporal_df['cumulative'] = temporal_df['total_loss'].cumsum()\n",
    "fig.add_trace(go.Scatter(x=temporal_df['year'], y=temporal_df['cumulative'],\n",
    "                         mode='lines+markers', name='Cumulative',\n",
    "                         line=dict(color=WSJ_COLORS['blue'], width=2)), row=3, col=1)\n",
    "\n",
    "fig.update_layout(height=900, title_text=f'Temporal Loss Pattern Analysis ({N_YEARS_TEMPORAL} Years)',\n",
    "                  barmode='stack')\n",
    "fig.update_yaxes(title_text='Loss Amount', tickformat='$.2s')\n",
    "fig.show()\n",
    "\n",
    "print(f\"\\nMean annual loss: ${temporal_df['total_loss'].mean():,.0f}\")\n",
    "print(f\"CV of annual losses: {temporal_df['total_loss'].std() / temporal_df['total_loss'].mean():.2f}\")\n",
    "print(f\"Years with catastrophic losses: {(temporal_df['catastrophic'] > 0).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extreme Value Analysis\n",
    "\n",
    "Return period analysis answers the question: *how large a loss should we expect once every N years?* This is critical for setting insurance limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EVA_SIMS = 10_000\n",
    "annual_maxima = []\n",
    "annual_totals = []\n",
    "\n",
    "for _ in range(N_EVA_SIMS):\n",
    "    events, year_stats = generator.generate_losses(duration=1.0, revenue=REVENUE)\n",
    "    annual_maxima.append(max((e.amount for e in events), default=0))\n",
    "    annual_totals.append(year_stats['total_amount'])\n",
    "\n",
    "return_periods = [2, 5, 10, 20, 50, 100, 200, 500]\n",
    "\n",
    "print('Return Period Analysis')\n",
    "print('=' * 70)\n",
    "print(f'{\"Return Period\":<15} {\"Max Loss\":<20} {\"Total Loss\":<20}')\n",
    "print('-' * 70)\n",
    "for rp in return_periods:\n",
    "    pct = 100 * (1 - 1 / rp)\n",
    "    max_val = np.percentile(annual_maxima, pct)\n",
    "    total_val = np.percentile(annual_totals, pct)\n",
    "    print(f'{rp:>10}-year  ${max_val:<19,.0f} ${total_val:<19,.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Correlation Between Loss Types\n",
    "\n",
    "Understanding whether attritional and large losses tend to co-occur in the same year is important for aggregate limit selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CORR_YEARS = 100\n",
    "corr_data = {'attritional_total': [], 'large_total': [], 'catastrophic_total': [], 'total_loss': []}\n",
    "\n",
    "for year in range(N_CORR_YEARS):\n",
    "    events, year_stats = generator.generate_losses(duration=1.0, revenue=REVENUE)\n",
    "    corr_data['attritional_total'].append(sum(e.amount for e in events if e.loss_type == 'attritional'))\n",
    "    corr_data['large_total'].append(sum(e.amount for e in events if e.loss_type == 'large'))\n",
    "    corr_data['catastrophic_total'].append(sum(e.amount for e in events if e.loss_type == 'catastrophic'))\n",
    "    corr_data['total_loss'].append(year_stats['total_amount'])\n",
    "\n",
    "corr_df = pd.DataFrame(corr_data)\n",
    "corr_matrix = corr_df.corr()\n",
    "\n",
    "print('Correlation Matrix:')\n",
    "print(corr_matrix.round(3))\n",
    "\n",
    "print(f\"\\nAttritional-Large correlation: {corr_matrix.loc['attritional_total', 'large_total']:.3f}\")\n",
    "print(f\"Years with catastrophic losses: {(corr_df['catastrophic_total'] > 0).sum()} / {N_CORR_YEARS}\")\n",
    "print(f\"\\nAverage contribution to total:\")\n",
    "print(f\"  Attritional:  {100 * corr_df['attritional_total'].sum() / corr_df['total_loss'].sum():.1f}%\")\n",
    "print(f\"  Large:        {100 * corr_df['large_total'].sum() / corr_df['total_loss'].sum():.1f}%\")\n",
    "print(f\"  Catastrophic: {100 * corr_df['catastrophic_total'].sum() / corr_df['total_loss'].sum():.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Optimal Retention Analysis\n",
    "\n",
    "Given the loss distribution, what retention level minimizes total cost (retained losses + insurance premium)? This analysis finds the sweet spot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retention_levels = np.logspace(5, 7, 20)  # $100K to $10M\n",
    "N_RETENTION_SIMS = 1_000\n",
    "results = []\n",
    "\n",
    "for retention in retention_levels:\n",
    "    retained_losses = []\n",
    "    total_losses = []\n",
    "    for _ in range(N_RETENTION_SIMS):\n",
    "        events, year_stats = generator.generate_losses(duration=1.0, revenue=REVENUE)\n",
    "        total = year_stats['total_amount']\n",
    "        total_losses.append(total)\n",
    "        retained_losses.append(min(total, retention))\n",
    "\n",
    "    avg_retained = np.mean(retained_losses)\n",
    "    expected_ceded = np.mean(total_losses) - avg_retained\n",
    "    insurance_premium = expected_ceded * 1.5  # 150% loading\n",
    "\n",
    "    results.append({\n",
    "        'retention': retention,\n",
    "        'avg_retained': avg_retained,\n",
    "        'est_premium': insurance_premium,\n",
    "        'total_cost': avg_retained + insurance_premium,\n",
    "        'volatility_reduction': np.std(retained_losses) / np.std(total_losses),\n",
    "    })\n",
    "\n",
    "ret_df = pd.DataFrame(results)\n",
    "optimal_idx = ret_df['total_cost'].idxmin()\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=('Total Cost vs Retention', 'Volatility Reduction'))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=ret_df['retention'], y=ret_df['total_cost'], mode='lines',\n",
    "                         name='Total Cost', line=dict(color=WSJ_COLORS['blue'], width=2)), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=ret_df['retention'], y=ret_df['avg_retained'], mode='lines',\n",
    "                         name='Retained Loss', line=dict(color=WSJ_COLORS['orange'], width=2, dash='dash')),\n",
    "              row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=ret_df['retention'], y=ret_df['est_premium'], mode='lines',\n",
    "                         name='Insurance Premium', line=dict(color=WSJ_COLORS['green'], width=2, dash='dot')),\n",
    "              row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=ret_df['retention'], y=ret_df['volatility_reduction'] * 100,\n",
    "                         mode='lines+markers', name='Volatility',\n",
    "                         line=dict(color=WSJ_COLORS['red'], width=2)), row=1, col=2)\n",
    "\n",
    "fig.update_layout(height=400, title_text='Optimal Retention Analysis')\n",
    "fig.update_xaxes(title_text='Retention Level', type='log', tickformat='$.2s')\n",
    "fig.update_yaxes(title_text='Cost', tickformat='$.2s', row=1, col=1)\n",
    "fig.update_yaxes(title_text='Volatility (%)', row=1, col=2)\n",
    "fig.show()\n",
    "\n",
    "print(f\"\\nOptimal retention level: ${ret_df.loc[optimal_idx, 'retention']:,.0f}\")\n",
    "print(f\"Total cost at optimum: ${ret_df.loc[optimal_idx, 'total_cost']:,.0f}\")\n",
    "print(f\"Volatility reduction: {ret_df.loc[optimal_idx, 'volatility_reduction'] * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- Manufacturing losses come in three distinct types with very different frequency-severity profiles.\n",
    "- Catastrophic losses are rare but dominate tail risk -- they drive the need for high insurance limits.\n",
    "- Loss types show low correlation, meaning aggregate analysis overstates diversification benefits.\n",
    "- Return-period analysis directly informs insurance limit selection.\n",
    "- An optimal retention level exists that minimizes total cost of risk (retained losses + premium).\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **Design insurance structures:** [core/02_insurance_structures.ipynb](02_insurance_structures.ipynb)\n",
    "- **See how losses affect long-term growth:** [core/03_ergodic_advantage.ipynb](03_ergodic_advantage.ipynb)\n",
    "- **Quantify tail risk:** [core/05_risk_metrics.ipynb](05_risk_metrics.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
